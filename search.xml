<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>K近邻算法</title>
    <url>/posts/1516846897/</url>
    <content><![CDATA[<p>介绍K近邻算法基本原理</p>
<span id="more"></span>
<h1 id="K近邻算法"><a href="#K近邻算法" class="headerlink" title="K近邻算法"></a>K近邻算法</h1><h2 id="一、什么是k近邻算法"><a href="#一、什么是k近邻算法" class="headerlink" title="一、什么是k近邻算法"></a>一、什么是k近邻算法</h2><p>给定一个训练数据集、对新的输入实例，在训练集中找到与该实例<strong>最邻近</strong>的k个实例，<strong>这k个</strong>实例的多数属于某个类，就把该输入实例分为这个类。如下图所示：输入新的❓点判断它属于classA还是classB</p>
<img data-src="https://s2.loli.net/2023/06/05/3AXmkiRC6U9JyhV.png" alt="202306051337912" style="zoom:100%;">

<p>那么问题来了k近邻算法市<strong>最近邻</strong>，那么这个最近邻怎么判断？计算距离！怎么计算距离</p>
<h2 id="二、距离度量"><a href="#二、距离度量" class="headerlink" title="二、距离度量"></a>二、距离度量</h2><h3 id="1、欧几里得距离"><a href="#1、欧几里得距离" class="headerlink" title="1、欧几里得距离"></a>1、欧几里得距离</h3><p>$$</p>
<p>d(x,y)&#x3D;\sqrt{\displaystyle\sum_{i&#x3D;1}^{n}(y_i-x_i)^2}</p>
<p>$$</p>
<h3 id="3、曼哈顿距离"><a href="#3、曼哈顿距离" class="headerlink" title="3、曼哈顿距离"></a>3、曼哈顿距离</h3><p>$$</p>
<p>d(x,y)&#x3D;(\displaystyle\sum_{i&#x3D;0}^{n}{|y_i-x_i|})</p>
<p>$$</p>
<h3 id="4、闵可夫斯基距离"><a href="#4、闵可夫斯基距离" class="headerlink" title="4、闵可夫斯基距离"></a>4、闵可夫斯基距离</h3><p>$$<br>Minkowski Distance&#x3D;(\displaystyle\sum_{i&#x3D;0}^{n}{|y_i-x_i|})^\frac{1}{p}<br>$$</p>
<p>现在我们知道了如何确定距离，但是问题又来了，我们应该怎么去定义我们的K值呢？</p>
<h2 id="三、K值选择"><a href="#三、K值选择" class="headerlink" title="三、K值选择"></a>三、K值选择</h2><p>如果选择较小的 k 值，就相当于用较小的邻域中的训练实例进行预测，“学习”的近似误差(approximation error)会减小，只有与输入实例较近的(相似的)训练实例才会对预测结果起作用。但缺点是“学习”的估计误差(estimation error)会增大，预测结果会对近邻的实例点非常敏感 。如果邻近的实例点恰巧是噪声，预测就会出错。换句话说，k 值的减小就意味着整体模型变得复杂，容易发生过拟合。</p>
<p>如果选择较大的 k 值，就相当于用较大邻域中的训练实例进行预测。其优点是可以减少学习的估计误差，但缺点是学习的近似误差会增大。这时与输入实例较远的(不相似的)训练实例也会对预测起作用，使预测发生错误。k 值的增大就意味着整体的模型变得简单。</p>
<p>如果k &#x3D;N，那么无论输入实例是什么，都将简单地预测它属于在训练实例中最多的类。这时，模型过于简单，完全忽略训练实例中的大量有用信息，是不可取的。在应用中，k 值一般取一个比较小的数值。通常采用<strong>交叉验证法</strong>来选取最优的k值。</p>
<h2 id="四、算法流程-python"><a href="#四、算法流程-python" class="headerlink" title="四、算法流程(python)"></a>四、算法流程(python)</h2><p>第一步：确定KNN算法需要确定的参数：1、输入的待分类变量(训练集)；2、k值；3、计算距离的方法；4、训练集数据；4、训练集标签<br>第二步：计算距离，KNN算法的核心就是计算与每一个变量的距离，而后挑选前K个。<strong>所以我们有时候需要对输入数据维度拓展，使其达到和测试集形状相同。</strong><br>第三步：判断KNN的准确率</p>
<h2 id="五、KNN评价"><a href="#五、KNN评价" class="headerlink" title="五、KNN评价"></a>五、KNN评价</h2><p><strong>优势</strong><br>易于实现：鉴于算法的简单性和准确性，它是新数据科学家将学习的首批分类器之一<br>轻松适应：随着新训练样本的增加，算法会根据任何新数据进行调整，因为所有训练数据都存储在内存中<br>很少的超参数：KNN 只需要 k值和距离度量，与其他机器学习算法相比，所需的超参数很少<br><strong>缺点</strong><br>不能很好地扩展：由于 KNN 是一种惰性算法，因此与其他分类器相比，它占用了更多的内存和数据存储。 从时间和金钱的角度来看，这可能是昂贵的。 更多的内存和存储将增加业务开支，而更多的数据可能需要更长的时间来计算。 虽然已经创建了不同的数据结构（例如 Ball-Tree）来解决计算效率低下的问题，但分类器是否理想可能取决于业务问题<br>维度的诅咒：KNN 算法容易成为维度诅咒的受害者，这意味着它在高维数据输入时表现不佳。 这有时也称为峰值现象，在算法达到最佳特征数量后，额外的特征会增加分类错误的数量，尤其是当样本尺寸较小时<br>容易过拟合：由于”维度的诅咒”，KNN 也更容易过拟合。 虽然利用特征选择和降维技术来防止这种情况发生</p>
<h2 id="六、python代码实现-手写字体识别"><a href="#六、python代码实现-手写字体识别" class="headerlink" title="六、python代码实现(手写字体识别)"></a>六、python代码实现(手写字体识别)</h2><p>需要调用的包(可以尝试利用pytorch去提高计算速度、原理也很简单pytorch可以和numpy几乎无缝衔接)</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> operator</span><br><span class="line"><span class="keyword">from</span> os <span class="keyword">import</span> listdir</span><br></pre></td></tr></table></figure>
<p>核心代码</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">KNN</span>(<span class="params">inx,dataset,labels,k,distances_way</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    inx:输入需要分类的数字</span></span><br><span class="line"><span class="string">    datset:输入样本训练集</span></span><br><span class="line"><span class="string">    labels:标签向量</span></span><br><span class="line"><span class="string">    k:选择最近邻的数目,其中标签数量和矩阵dataset的行数相同</span></span><br><span class="line"><span class="string">    distance:计算距离的方式</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment">#计算距离</span></span><br><span class="line">    datasize_h = dataset.shape[<span class="number">0</span>]</span><br><span class="line">    inx = np.tile(inx,(datasize_h,<span class="number">1</span>)) <span class="comment">#将inx维度拓展成和dataset形状相同的的矩阵</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> distances_way == <span class="built_in">str</span>(<span class="string">&#x27;o&#x27;</span>):<span class="comment">#欧几里得距离</span></span><br><span class="line">        diffmat = inx - dataset</span><br><span class="line">        sq_diffmat = diffmat**<span class="number">2</span></span><br><span class="line">        sq_distances = sq_diffmat.<span class="built_in">sum</span>(axis=<span class="number">1</span>)</span><br><span class="line">        distance = sq_distances**<span class="number">0.5</span></span><br><span class="line">    <span class="keyword">elif</span> distances_way == <span class="built_in">str</span>(<span class="string">&#x27;man&#x27;</span>):<span class="comment">#曼哈顿距离</span></span><br><span class="line">        diffmat =  inx - dataset</span><br><span class="line">        abs_diffmat = <span class="built_in">abs</span>(diffmat)</span><br><span class="line">        distance = abs_diffmat.<span class="built_in">sum</span>(axis=<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">elif</span> distances_way == <span class="built_in">str</span>(<span class="string">&#x27;min&#x27;</span>):<span class="comment">#闵可夫斯基距离</span></span><br><span class="line">        p = <span class="built_in">int</span>(<span class="built_in">input</span>(<span class="string">&#x27;输入p值:&#x27;</span>))</span><br><span class="line">        diffmat = inx - dataset</span><br><span class="line">        sq_diffmat = diffmat**<span class="number">2</span></span><br><span class="line">        sq_distances = sq_diffmat.<span class="built_in">sum</span>(axis=<span class="number">1</span>)</span><br><span class="line">        distance = sq_distances**(<span class="number">1</span>/p)</span><br><span class="line">    distance_sort = distance.argsort() <span class="comment">#按距离有小到大排序</span></span><br><span class="line">    <span class="comment">#return distance_sort</span></span><br><span class="line">    <span class="comment">#将排序得到的距离和我们的标签进行对应起来，利用哈希表</span></span><br><span class="line">    dic = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(k):</span><br><span class="line">        diff_label = labels[distance_sort[i]]</span><br><span class="line">        dic[diff_label] = dic.get(diff_label,<span class="number">0</span>)+<span class="number">1</span></span><br><span class="line">    dic_sort = <span class="built_in">sorted</span>(dic.items(), key=operator.itemgetter(<span class="number">1</span>),reverse=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> dic_sort[<span class="number">0</span>][<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<p>手写字体识别(列子来源于《机器学习实战》自己做了部分改变)</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#将32*32化成1*1024的矩阵，也可以化成32*32的矩阵</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">data_read</span>(<span class="params">path</span>):</span><br><span class="line">    data = <span class="built_in">open</span>(path)</span><br><span class="line">    data_use = np.zeros((<span class="number">1</span>,<span class="number">1024</span>))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">32</span>):</span><br><span class="line">        data_line = data.readline() <span class="comment">#读取每一行</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">32</span>):</span><br><span class="line">            data_use[<span class="number">0</span>,<span class="number">32</span>*i+j] = <span class="built_in">int</span>(data_line[j])</span><br><span class="line">    <span class="keyword">return</span> data_use</span><br><span class="line"></span><br><span class="line"><span class="comment">#将32*32的text文件化成32*32的矩阵</span></span><br><span class="line"><span class="comment"># def data_read(path):</span></span><br><span class="line"><span class="comment">#     data = open(path)</span></span><br><span class="line"><span class="comment">#     data_use = np.zeros([32,32])</span></span><br><span class="line"><span class="comment">#     data_narry = np.array(data)</span></span><br><span class="line"><span class="comment">#     for j in range(len(a)):</span></span><br><span class="line"><span class="comment">#         for i in range(len(a)):</span></span><br><span class="line"><span class="comment">#             data_use[i][j] = a[i][j]</span></span><br><span class="line"><span class="comment">#     return data_use</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">handwritingClassTest</span>():</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    hwLabels:手写数字真实值</span></span><br><span class="line"><span class="string">    m:训练集文件个数 mTset:测试集的文件个数</span></span><br><span class="line"><span class="string">    trainingMat:存储训练集的全部一维化的数据</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    hwLabels = []</span><br><span class="line">    trainingFileList = listdir(<span class="string">&#x27;D:/Github-code/python-gogogo/机器学习/分类问题/K近邻算法/手写数据文件/training_handwriting&#x27;</span>)</span><br><span class="line">    m = <span class="built_in">len</span>(trainingFileList) <span class="comment"># m=1934</span></span><br><span class="line">    trainingMat = np.zeros((m, <span class="number">1024</span>))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(m):</span><br><span class="line">        <span class="comment"># 对文件名进行拆分 ---&gt;开始 只取文件名的第一个字符(对应真实数字)</span></span><br><span class="line">        fileNameStr = trainingFileList[i] <span class="comment"># 得到的都是文件名的字符串</span></span><br><span class="line">        fileStr = fileNameStr.split(<span class="string">&#x27;.&#x27;</span>)[<span class="number">0</span>]</span><br><span class="line">        classNumStr = <span class="built_in">int</span>(fileStr.split(<span class="string">&#x27;_&#x27;</span>)[<span class="number">0</span>])</span><br><span class="line">        hwLabels.append(classNumStr)</span><br><span class="line">        <span class="comment"># 对文件名进行拆分 ---&gt;结束 </span></span><br><span class="line">        trainingMat[i,:] = data_read(<span class="string">&#x27;D:/Github-code/python-gogogo/机器学习/分类问题/K近邻算法/手写数据文件/training_handwriting/%s&#x27;</span>% fileNameStr)</span><br><span class="line">    <span class="comment">#return trainingMat</span></span><br><span class="line"></span><br><span class="line">    testFileList = listdir(<span class="string">&#x27;D:/Github-code/python-gogogo/机器学习/分类问题/K近邻算法/手写数据文件/test_handwriting&#x27;</span>)</span><br><span class="line">    errorCount = <span class="number">0.0</span></span><br><span class="line">    mTest = <span class="built_in">len</span>(testFileList)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(mTest):</span><br><span class="line">        fileNameStr = testFileList[i]</span><br><span class="line">        fileStr = fileNameStr.split(<span class="string">&#x27;.&#x27;</span>)[<span class="number">0</span>]     </span><br><span class="line">        classNumStr = <span class="built_in">int</span>(fileStr.split(<span class="string">&#x27;_&#x27;</span>)[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">        vectorUnderTest = data_read(<span class="string">&#x27;D:/Github-code/python-gogogo/机器学习/分类问题/K近邻算法/手写数据文件/test_handwriting/%s&#x27;</span> % fileNameStr)</span><br><span class="line">        <span class="built_in">print</span>(vectorUnderTest.shape)</span><br><span class="line">        classifierResult = KNN(vectorUnderTest, trainingMat, hwLabels, <span class="number">3</span>,<span class="string">&#x27;o&#x27;</span>)</span><br><span class="line">        <span class="built_in">print</span> (<span class="string">&quot;KNN分类结果: %d, 实际结果: %d&quot;</span> % (classifierResult, classNumStr))</span><br><span class="line">        <span class="keyword">if</span> (classifierResult != classNumStr): errorCount += <span class="number">1.0</span></span><br><span class="line">    <span class="built_in">print</span> (<span class="string">&quot;\n错误数字数量: %d&quot;</span> % errorCount)</span><br><span class="line">    <span class="built_in">print</span> (<span class="string">&quot;\n错误比率: %f&quot;</span> % (errorCount/<span class="built_in">float</span>(mTest)))</span><br></pre></td></tr></table></figure>
<h2 id="七、个人思考"><a href="#七、个人思考" class="headerlink" title="七、个人思考"></a>七、个人思考</h2><p>既然过程中涉及到了pytorch，并且做的是”手写字体识别“，那么的话我们不妨自己尝试使用自己随便拍一张照片，而后去识别自己手写字体(将图片二值化，我暂时想到这样，因为深度学习自己也不是太过了了解)，如果利用KNN算法可能效率没有使用CNN算法效率那么高，其中可能还会涉及到opencv库的使用。<br>评论区大佬有想法不妨踢我哈哈哈哈哈。😀😀文章有什么不足欢迎留言。</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><blockquote>
<p>1、<a href="https://www.ibm.com/cn-zh/topics/knn#:~:text=k%2D%E6%9C%80%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95%EF%BC%8C%E4%B9%9F,%E6%9C%80%E5%B8%B8%E8%A1%A8%E7%A4%BA%E7%9A%84%E6%A0%87%E7%AD%BE%E3%80%82">https://www.ibm.com/cn-zh/topics/knn#:~:text=k%2D%E6%9C%80%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95%EF%BC%8C%E4%B9%9F,%E6%9C%80%E5%B8%B8%E8%A1%A8%E7%A4%BA%E7%9A%84%E6%A0%87%E7%AD%BE%E3%80%82</a><br>2、李航《统计学学习方法第二版》</p>
</blockquote>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>分类算法</tag>
      </tags>
  </entry>
  <entry>
    <title>Attention is all you need阅读笔记</title>
    <url>/posts/44dbd2a1/</url>
    <content><![CDATA[<h1 id="Attention-is-all-you-need"><a href="#Attention-is-all-you-need" class="headerlink" title="Attention is all you need"></a>Attention is all you need</h1><blockquote>
<p><strong>Transformer</strong>模型</p>
</blockquote>
<h2 id="Model-Architecture"><a href="#Model-Architecture" class="headerlink" title="Model Architecture"></a>Model Architecture</h2><p><strong>Transformer</strong>结构上和传统的翻译模型相同，拥有<strong>encoder-decoder structure</strong></p>
<blockquote>
<p><strong>encoder</strong>：将一系列输入的符号表示$(x_1…,x_n)$映射到连续表示$z&#x3D;(z_1….,z_n)$<br><strong>decoder</strong>：将$z$解码出系列输出序列$(y_i,….,y_n)$</p>
</blockquote>
<p>模型结构图：</p>
<div align="center"><img data-src="https://s2.loli.net/2023/09/26/XH1zOVho89r2YKq.png" alt="202309261042951" style="zoom:100%;"></div>

<p>上述结构图中：左侧为<code>encoder</code>结构，右侧为<code>decoder</code>结构<br>1、<code>encoder</code>结构：整体上<code>encoder</code>由两部分构成：<strong>1、Multi-head Attention</strong>；<strong>2、Feed Forwarded</strong>构成。在每一部分里面都是通过<em>残差进行连接</em>。(论文中对此的描述为：$LayerNorm(x+Sublayer(x))$其中$x+Sublayer(x)$就是一个残差连接的操作，而$LayerNorm$就是一个<em>Norm</em>所进行的一个操作。)</p>
<p>2、<code>decoder</code>结构整体与<code>encoder</code>相似，只是多了一个<strong>Masker Multi-Head Attention</strong>结构，这部分结构起到的作用为：因为如果是做序列预测（时间序列、翻译等）一般来说都是不去借助<strong>未来</strong>的数据，也就是说保证我们在预测$i$时候只考虑了$i$之前的信息。</p>
<blockquote>
<p>This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position i can depend only on the known outputs at positions less than i.</p>
</blockquote>
<h2 id="Attention模型"><a href="#Attention模型" class="headerlink" title="Attention模型"></a>Attention模型</h2><div align="center"><img data-src="https://s2.loli.net/2023/09/25/zIMCbZgAOlHLqxN.png" alt="202309251434370" style="zoom:80%;"></div>

<p>在Transformer函数中设计的注意模型：<br>1、Scaled Dot-Product Attention<br>$$<br>Attention(Q,K,V)&#x3D;softmax(\frac{QK^T}{\sqrt{d_k}})V<br>$$<br>QKV理解视频<br><a href="https://www.youtube.com/watch?v=OyFJWRnt_AY">https://www.youtube.com/watch?v=OyFJWRnt_AY</a></p>
<h2 id="补充内容"><a href="#补充内容" class="headerlink" title="补充内容"></a>补充内容</h2><h3 id="1、encoder-decoder"><a href="#1、encoder-decoder" class="headerlink" title="1、encoder-decoder"></a>1、encoder-decoder</h3><p><a href="https://proceedings.neurips.cc/paper_files/paper/2014/hash/09c6c3783b4a70054da74f2538ed47c6-Abstract.html">https://proceedings.neurips.cc/paper_files/paper/2014/hash/09c6c3783b4a70054da74f2538ed47c6-Abstract.html</a></p>
<h3 id="2、norm"><a href="#2、norm" class="headerlink" title="2、norm"></a>2、norm</h3><p><strong>Training state-of-the-art, deep neural networks is computationally expensive. One way to reduce the training time is to normalize the activities of the neurons.</strong></p>
<p>正则化：$x&#x3D;\frac{x-x_{min}}{x_{max}- x_{min}}$将数据都转化到$[0,1]$；以及标准化：$x&#x3D;\frac{x- \mu}{\sigma}$将数据都转化为<strong>0均值同方差</strong>。在深度学习中常见4类<strong>标准化</strong>操作：<br>1、Bath-Norm；2、Layer-Norm；3、Instance-Norm；4、Group-Norm。这4类标准化化唯一区别就在对样本$\mu,\sigma$的计算区别。</p>
<blockquote>
<p>定义如下计算公式：<br>$$<br>\mu_i&#x3D; \frac{1}{m}\sum_{k\in S_i}x_k<br>$$<br>$$<br>\sigma_i&#x3D; \sqrt{\frac{1}{m}\sum_{k\in S_i}(x_k- \mu_i)^2+\epsilon}<br>$$<br>4类标准化区别就在于对于参数$S_i$的定义！！！<br>如：对一组图片定义如下变量：$(N,C,H,W)$分别代表：batch、channel、height、width<br><strong>Bath-norm</strong>：$S_i&#x3D;{k_C&#x3D;i_C}$<br><strong>Layer-norm</strong>：$S_i&#x3D;{k_C&#x3D;i_N}$<br><strong>Instance-norm</strong>：$S_i&#x3D;{k_C&#x3D;i_C,K_N&#x3D;i_N}$<br><strong>Group-norm</strong>：$S_i&#x3D;{k_N&#x3D;i_N, \lfloor \frac{k_C}{C&#x2F;G} \rfloor&#x3D;\lfloor \frac{i_C}{C&#x2F;G} \rfloor}$<br>$G$代表组的数量，$C&#x2F;G$每个组的通道数量</p>
</blockquote>
<p>对于<em>Layer-norm</em>、<em>batch-norm</em>、<em>Instance-norm</em>、<em>Group-norm</em>理解直观的用下面一组图片了解：</p>
<div align="center"><img data-src="https://s2.loli.net/2023/10/02/qtyoT1przb6alLR.png" alt="Group Normalization" style="zoom:100%;"></div>

<blockquote>
<p>图片来源：<a href="http://arxiv.org/abs/1803.08494">http://arxiv.org/abs/1803.08494</a></p>
</blockquote>
<h4 id="2-1-Batch-norm详细原理"><a href="#2-1-Batch-norm详细原理" class="headerlink" title="2.1 Batch-norm详细原理"></a>2.1 Batch-norm详细原理</h4><blockquote>
<p>参考：<a href="https://proceedings.mlr.press/v37/ioffe15.html">https://proceedings.mlr.press/v37/ioffe15.html</a><br>在原始论文中主要是将<em>Batch-norm</em>运用在图算法（CNN等）中，作者得到结论：通过添加<em>Batch-norm</em>可以加快模型的训练速度，模型效果较以前算法效果更加好</p>
</blockquote>
<p><strong>原理</strong>：对每一个<strong>特征</strong>独立的进行归一化</p>
<blockquote>
<p>we will normalize each scalar feature independently, by making it have zero mean and unit variance.</p>
</blockquote>
<p>对于每一层$d$维输入$x&#x3D;(x^{(1)},…,x^{(d)})$通过:<br>$$<br>\widehat{x}^{(k)}&#x3D;\frac{x^{k}- E[x^{k}]}{\sqrt{Var[x^{k}]}}<br>$$<br><strong>问题一</strong>：如果只是简单的对每一层输入进行简单的正则化将会改变该层所表示的内容（原文中提到：对于通过<code>sigmoid函数</code>处理的输入进行正则化将会导致原本应该的非线性转化为线性）</p>
<blockquote>
<p>Note that simply normalizing each input of a layer may change what the layer can represent. For instance, normalizing the inputs of a sigmoid would constrain them to the linear regime of the nonlinearity.</p>
</blockquote>
<p>通过构建$y^{(k)}&#x3D;\gamma^{(k)}\widehat{x}^{(k)}+\beta^{(k)}$其中$\gamma^{(k)}&#x3D;\sqrt{Var[x^{(k)}]}、\beta^{(k)}&#x3D;E[x^{(k)}]$</p>
<p><strong>问题二</strong>：在训练过程中运用的是全部的数据集，因此想要做到随机优化是不可能的，因为通过小批量随机梯度训练，在每一次小梯度过程中都会产生均值以及方差。（<code>Batch Normalizing Transform</code>）</p>
<h4 id="2-2-Layer-norm详细原理"><a href="#2-2-Layer-norm详细原理" class="headerlink" title="2.2 Layer-norm详细原理"></a>2.2 Layer-norm详细原理</h4><blockquote>
<p>参考：<a href="http://arxiv.org/abs/1607.06450">http://arxiv.org/abs/1607.06450</a><br>针对<em>Batch-norm</em>的部分缺点以及其在序列数据上的表现较差，提出通过使用<em>Layer-norm</em>在序列数据（时间序列、文本数据）上进行使用</p>
</blockquote>
<p><strong>原理</strong>：通过从一个训练样例中计算用于归一化的所有神经元输入之和的均值和方差（也就是说直接对某个神经元的所有的输入进行标准化）</p>
<blockquote>
<p>we transpose batch normalization into layer normalization by computing the mean and variance used for normalization from all of the summed inputs to the neurons in a layer on a single training case</p>
</blockquote>
<p>$$<br>\mu^l&#x3D; \frac{1}{H}\sum_{i&#x3D;1}^{H}a_i^l \<br>\sigma^l&#x3D;\sqrt{\frac{1}{H}\sum_{i&#x3D;1}^{H}(a_i^l- \mu^l)^2}<br>$$<br>其中$H$代表隐藏层的数量、$a^l$代表一层神经元的输入、$\mu和\sigma$则分别代表该层神经元的均值以及方差。</p>
<h4 id="2-3-Group-norm"><a href="#2-3-Group-norm" class="headerlink" title="2.3 Group-norm"></a>2.3 Group-norm</h4><blockquote>
<p>参考：<a href="http://arxiv.org/abs/1803.08494">http://arxiv.org/abs/1803.08494</a></p>
</blockquote>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p>1、<a href="http://arxiv.org/abs/1706.03762">http://arxiv.org/abs/1706.03762</a></p>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>基础算法</tag>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title>自动备份Hexo博客源文件</title>
    <url>/posts/b6b5a9dd/</url>
    <content><![CDATA[<p>由于自己经常通过hexo来写自己的blog那么万一那一天树莓派坏了（自己用树莓派做服务器），那么里面所有的东西都需要重新写那太遭罪了，所以去网上找了将hexo进行备份的操作。<br>⭐以下操作在⏱️2023.6.4⏱️可以完美实现.</p>
<span id="more"></span>

<h1 id="hexo备份操作"><a href="#hexo备份操作" class="headerlink" title="hexo备份操作"></a>hexo备份操作</h1><h2 id="hexo需要备份什么东西？"><a href="#hexo需要备份什么东西？" class="headerlink" title="hexo需要备份什么东西？"></a>hexo需要备份什么东西？</h2><p>我们在<strong>新电脑</strong>通过hexo重新搭建博客的时候（跳过hexo安装）<code>hexo init</code>那么在目标目录下会生成一部分文件，但是我们不可能重新在<strong>新电脑</strong>里面把之前的东西在写一遍，此时需要将<strong>旧电脑</strong>写的东西迁移到新电脑。此时分为两种情况：</p>
<h3 id="1、旧电脑还活着"><a href="#1、旧电脑还活着" class="headerlink" title="1、旧电脑还活着"></a>1、旧电脑还活着</h3><p>我们只需要把<strong>旧电脑</strong>上博客目录下的 <code>source、themes、_config.yml</code>覆盖到 blog 的那个文件夹即可，只要hexo环境没问题，hexo的一系列操作都没问题（在第一次 <code>hexo d</code>可能会有些慢，耐心等待）。同样，在新电脑上只要把环境搭建好，只要把两个文件夹一个配置文件覆盖过去，都没问题。包括在备份时也只需要把这两个文件夹一个配置文件备份好，其他的都不用管了。</p>
<h3 id="2、旧电脑死了"><a href="#2、旧电脑死了" class="headerlink" title="2、旧电脑死了"></a>2、旧电脑死了</h3><p>此种情况就需要我们重新写blog了，但是我们可以避免这种情况！！！操作如下：<br><strong>第一步</strong>，我们先在github上创建一个仓库作为我们的博客存储。<br><strong>第二步</strong>，我们需要在<strong>旧电脑</strong>上执行如下命令：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git init</span><br><span class="line">git remote add origin git@github.com:yourname/hexo.git</span><br><span class="line">git pull origin master</span><br></pre></td></tr></table></figure>

<p>正如我们上面<strong>第一种情况</strong>我们不需要对所有的文件进行操作，那么的话我们在<strong>根目录</strong>下新建文件 <code>.gitignore </code>在此文件下输入我们不需要备份的文件，我的配置如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">.DS_Store</span><br><span class="line">Thumbs.db</span><br><span class="line">db.json</span><br><span class="line">*.log</span><br><span class="line">node_modules/</span><br><span class="line">public/</span><br><span class="line">.deploy*/</span><br></pre></td></tr></table></figure>

<p><strong>第三步</strong>：执行如下命令：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git add .</span><br><span class="line">git commit -m &quot;备份hexo源码文件&quot;</span><br><span class="line">git push origin master</span><br></pre></td></tr></table></figure>

<p><strong>测试</strong>：<br><code>&lt;img src=&quot;https://s2.loli.net/2023/06/04/WSO4JgRDtdGmsiE.png&quot; alt=&quot;备份操作1&quot; style=&quot;zoom:100%;&quot;/&gt;</code><br>不过！！！！万一哪一天忘记 <code>git push </code>怎么办？</p>
<h3 id="备份方法二"><a href="#备份方法二" class="headerlink" title="备份方法二"></a>备份方法二</h3><p>我们需要自动备份怎么办？要实现这个自动备份功能，需要依赖NodeJs的一个shelljs模块,该模块重新包装了child_process,调用系统命令更加的方便，该模块需要安装后使用。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">npm install --save shelljs</span><br></pre></td></tr></table></figure>

<p><strong>编写自动化脚本</strong></p>
<blockquote>
<p>注意⭐：如下操作只是帮助你减少上面操作，也就是执行 <code> hexo d</code>的时候自动帮你把 <code>git push</code>执行，所以我们还是需在github建立仓库！！！<br>shelljs模块安装完成后，在Hexo根目录的scripts文件夹下新建一个js文件，文件名随意取(我的文件名为:auto.js)。如果没有scripts目录，请新建一个。然后在脚本中，写入以下内容：</p>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">require(&#x27;shelljs/global&#x27;);</span><br><span class="line">try &#123;</span><br><span class="line">    hexo.on(&#x27;deployAfter&#x27;, function() &#123;//当deploy完成后执行备份</span><br><span class="line">        run();</span><br><span class="line">    &#125;);</span><br><span class="line"></span><br><span class="line">&#125; catch (e) &#123;</span><br><span class="line">    console.log(&quot;产生了一个错误啊&lt;(￣3￣)&gt; !，错误详情为：&quot; + e.toString());</span><br><span class="line">&#125;</span><br><span class="line">function run() &#123;</span><br><span class="line">    if (!which(&#x27;git&#x27;)) &#123;</span><br><span class="line">        echo(&#x27;Sorry, this script requires git&#x27;);</span><br><span class="line">        exit(1);</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">        echo(&quot;======================Auto Backup Begin===========================&quot;);</span><br><span class="line">        cd(&#x27;/home/pi/blog&#x27;);    //此处修改为Hexo根目录路径</span><br><span class="line">        if (exec(&#x27;git add --all&#x27;).code !== 0) &#123;</span><br><span class="line">            echo(&#x27;Error: Git add failed&#x27;);</span><br><span class="line">            exit(1);</span><br><span class="line">        &#125;</span><br><span class="line">        if (exec(&#x27;git commit -am &quot;blog auto backup script\&#x27;s commit&quot;&#x27;).code !== 0) &#123;</span><br><span class="line">            echo(&#x27;Error: Git commit failed&#x27;);</span><br><span class="line">            exit(1);</span><br><span class="line">        &#125;</span><br><span class="line">        // 下面if内容，如果你的Git远程仓库名称不为origin的话，还需要修改push命令，修改成自己的远程仓库名和相应的分支名</span><br><span class="line">        if (exec(&#x27;git push origin main&#x27;).code !== 0) &#123;</span><br><span class="line">            echo(&#x27;Error: Git push failed&#x27;);</span><br><span class="line">            exit(1);</span><br><span class="line">        &#125;</span><br><span class="line">        echo(&quot;==================Auto Backup Complete============================&quot;)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>上述有两处需要更改，已经标注</li>
</ul>
<p>测试：&#96;&#96; heox d&#96;&#96;&#96;<br>结果如下：<br><code>&lt;img src=&quot;https://s2.loli.net/2023/06/04/tjhMsOceK6olzAy.png&quot; alt=&quot;202306042002321&quot; style=&quot;zoom:100%;&quot;/&gt;</code></p>
]]></content>
      <categories>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>hexo备份</tag>
      </tags>
  </entry>
  <entry>
    <title>深度学习——pytorch学习</title>
    <url>/posts/1867073817/</url>
    <content><![CDATA[<h1 id="Pytorch学习"><a href="#Pytorch学习" class="headerlink" title="Pytorch学习"></a>Pytorch学习</h1><span id="more"></span>
<p>[toc]</p>
<h2 id="pytorch基本操作"><a href="#pytorch基本操作" class="headerlink" title="pytorch基本操作"></a>pytorch基本操作</h2><p>pytorch使用前首先了解一个一个概念：张量。个人觉得没必要去了解他到底是一种什么东西，可以将其与我们numpy中数组一起进行了解</p>
<blockquote>
<p>张量可以看作是⼀个多维数组。标量可以看作是0维张量，向量可以看作1维张量，矩阵可以看作是⼆维张量</p>
</blockquote>
<p>那么我们后续也都是围绕张量进行展开了解</p>
<h3 id="数据操作"><a href="#数据操作" class="headerlink" title="数据操作"></a>数据操作</h3><h4 id="创建张量"><a href="#创建张量" class="headerlink" title="创建张量"></a>创建张量</h4><p><img data-src="/pytorch%E5%AD%A6%E4%B9%A0/1680163291680.png" alt="1680163291680"></p>
<blockquote>
<p>注意使用pytorch就不得不了解一大特点：利用GPU计算。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&gt;&gt;a = torch.ones(<span class="number">5</span>,<span class="number">3</span>,device=<span class="string">&#x27;cuda&#x27;</span>,dtype=torch.float64)<span class="comment">#设置类型，gpu</span></span><br><span class="line">&gt;&gt;a</span><br><span class="line">tensor([[<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>]], device=<span class="string">&#x27;cuda:0&#x27;</span>, dtype=torch.float64)</span><br><span class="line">&gt;&gt;<span class="built_in">print</span>(a.size(),a.shape)</span><br><span class="line">torch.Size([<span class="number">5</span>, <span class="number">3</span>]) torch.Size([<span class="number">5</span>, <span class="number">3</span>])</span><br></pre></td></tr></table></figure>

<h4 id="操作"><a href="#操作" class="headerlink" title="操作"></a>操作</h4><ul>
<li><strong>加法运算</strong></li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(x+y)</span><br><span class="line"><span class="built_in">print</span>(torch.add(x,y))</span><br><span class="line"><span class="built_in">print</span>(y.add(x))</span><br><span class="line">-&gt;tensor([[<span class="number">0.9841</span>, <span class="number">1.1514</span>, <span class="number">1.0982</span>],</span><br><span class="line">        [<span class="number">0.9974</span>, <span class="number">0.4662</span>, <span class="number">1.5250</span>],</span><br><span class="line">        [<span class="number">0.5120</span>, <span class="number">0.3424</span>, <span class="number">1.6973</span>],</span><br><span class="line">        [<span class="number">1.1808</span>, <span class="number">1.1172</span>, <span class="number">1.6786</span>],</span><br><span class="line">        [<span class="number">1.9113</span>, <span class="number">1.3679</span>, <span class="number">1.3846</span>]])</span><br><span class="line">三个结果都一样</span><br></pre></td></tr></table></figure>

<ul>
<li><p><strong>索引</strong><br>索引出来的结果与原数据共享内存，也即修改⼀个，另⼀个会跟着修改。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">b = x[<span class="number">0</span>:<span class="number">1</span>]</span><br><span class="line"><span class="built_in">print</span>(b)</span><br><span class="line">-&gt;tensor([[<span class="number">0.6847</span>, <span class="number">0.9743</span>, <span class="number">0.9259</span>]])</span><br><span class="line">b +=<span class="number">2</span></span><br><span class="line"><span class="built_in">print</span>(x[<span class="number">0</span>,:])</span><br><span class="line">-&gt;tensor([<span class="number">2.6847</span>, <span class="number">2.9743</span>, <span class="number">2.9259</span>])</span><br></pre></td></tr></table></figure>

<p><img data-src="/pytorch%E5%AD%A6%E4%B9%A0/1680164219080.png" alt="1680164219080"></p>
</li>
<li><p><strong>改变形状</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">c = x.view(<span class="number">15</span>)</span><br><span class="line">c</span><br><span class="line">-&gt;tensor([<span class="number">2.6847</span>, <span class="number">2.9743</span>, <span class="number">2.9259</span>, <span class="number">0.7470</span>, <span class="number">0.2349</span>, <span class="number">0.6642</span>, <span class="number">0.3026</span>, <span class="number">0.1884</span>, <span class="number">0.9414</span>,</span><br><span class="line">        <span class="number">0.4747</span>, <span class="number">0.8469</span>, <span class="number">0.7788</span>, <span class="number">0.9762</span>, <span class="number">0.4373</span>, <span class="number">0.6214</span>])</span><br><span class="line">c +=<span class="number">1</span></span><br><span class="line">x</span><br><span class="line">-&gt;tensor([[<span class="number">3.6847</span>, <span class="number">3.9743</span>, <span class="number">3.9259</span>],</span><br><span class="line">        [<span class="number">1.7470</span>, <span class="number">1.2349</span>, <span class="number">1.6642</span>],</span><br><span class="line">        [<span class="number">1.3026</span>, <span class="number">1.1884</span>, <span class="number">1.9414</span>],</span><br><span class="line">        [<span class="number">1.4747</span>, <span class="number">1.8469</span>, <span class="number">1.7788</span>],</span><br><span class="line">        [<span class="number">1.9762</span>, <span class="number">1.4373</span>, <span class="number">1.6214</span>]])</span><br></pre></td></tr></table></figure>

<p>利用view()函数返回的新tensor与源tensor共享内存（其实是同⼀个tensor），也即更改其中的⼀个，另<br>外⼀个也会跟着改变。所以如果我们想返回⼀个真正新的副本（即不共享内存）该怎么办呢？Pytorch还提供了⼀个reshape()可以改变形状，但是此函数并不能保证返回的是其拷⻉，所以不推荐使⽤。推荐先<br>⽤ clone 创造⼀个副本。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x_clone = x.clone()</span><br><span class="line">x_clone +=<span class="number">1</span></span><br><span class="line"><span class="built_in">print</span>(x_clone, <span class="string">&#x27;\n&#x27;</span>, x)</span><br><span class="line">-&gt;tensor([[<span class="number">4.6847</span>, <span class="number">4.9743</span>, <span class="number">4.9259</span>],</span><br><span class="line">        [<span class="number">2.7470</span>, <span class="number">2.2349</span>, <span class="number">2.6642</span>],</span><br><span class="line">        [<span class="number">2.3026</span>, <span class="number">2.1884</span>, <span class="number">2.9414</span>],</span><br><span class="line">        [<span class="number">2.4747</span>, <span class="number">2.8469</span>, <span class="number">2.7788</span>],</span><br><span class="line">        [<span class="number">2.9762</span>, <span class="number">2.4373</span>, <span class="number">2.6214</span>]]) </span><br><span class="line"> tensor([[<span class="number">3.6847</span>, <span class="number">3.9743</span>, <span class="number">3.9259</span>],</span><br><span class="line">        [<span class="number">1.7470</span>, <span class="number">1.2349</span>, <span class="number">1.6642</span>],</span><br><span class="line">        [<span class="number">1.3026</span>, <span class="number">1.1884</span>, <span class="number">1.9414</span>],</span><br><span class="line">        [<span class="number">1.4747</span>, <span class="number">1.8469</span>, <span class="number">1.7788</span>],</span><br><span class="line">        [<span class="number">1.9762</span>, <span class="number">1.4373</span>, <span class="number">1.6214</span>]])</span><br></pre></td></tr></table></figure></li>
<li><p><strong>线性代数</strong></p>
<p><img data-src="/pytorch%E5%AD%A6%E4%B9%A0/1680164657109.png" alt="1680164657109"></p>
<blockquote>
<p>官方文档：<a href="https://pytorch.org/docs/stable/tensors.html">torch.Tensor — PyTorch 2.0 documentation</a></p>
</blockquote>
</li>
<li><p><strong>广播机制</strong><br>线性代数告诉我们，要是两个矩阵形状不同是不可以直接相加的，但是pytorch的广播机制可以实现此类计算。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = torch.arange(<span class="number">1</span>, <span class="number">3</span>).view(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line"><span class="built_in">print</span>(x)</span><br><span class="line">y = torch.arange(<span class="number">1</span>, <span class="number">4</span>).view(<span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(y)</span><br><span class="line"><span class="built_in">print</span>(x + y)</span><br><span class="line">-&gt;tensor([[<span class="number">1</span>, <span class="number">2</span>]])</span><br><span class="line">tensor([[<span class="number">1</span>],</span><br><span class="line">        [<span class="number">2</span>],</span><br><span class="line">        [<span class="number">3</span>]])</span><br><span class="line">tensor([[<span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">        [<span class="number">3</span>, <span class="number">4</span>],</span><br><span class="line">        [<span class="number">4</span>, <span class="number">5</span>]])</span><br></pre></td></tr></table></figure>

<p>计算过程中，由于x，y的形状是不相同的，通过广播机制，会将x进行拓展，也就是将x变成3x2，y变成3x2而后再进行计算。</p>
</li>
</ul>
<h3 id="tensor和numpy相互转换"><a href="#tensor和numpy相互转换" class="headerlink" title="tensor和numpy相互转换"></a>tensor和numpy相互转换</h3><p>我们很容易⽤ numpy() 和 from_numpy() 将 Tensor 和NumPy中的数组相互转换。但是需要注意的⼀<br>点是： <strong>这两个函数所产⽣的的 Tensor 和NumPy中的数组共享相同的内存（所以他们之间的转换很<br>快），改变其中⼀个时另⼀个也会改变！！！</strong></p>
<blockquote>
<p>还有⼀个常⽤的将NumPy中的array转换成 Tensor 的⽅法就是 torch.tensor() , 需要注意的<br>是，此⽅法总是会进⾏数据拷⻉（就会消耗更多的时间和空间），所以返回的 Tensor 和原来的数<br>据不再共享内</p>
</blockquote>
<ul>
<li>numpy()<br>将tensor转换为numpy形式</li>
<li>torch.from_numpy()<br>将numpy转换为tensor</li>
</ul>
<h2 id="自动求梯度"><a href="#自动求梯度" class="headerlink" title="自动求梯度"></a>自动求梯度</h2><blockquote>
<p><a href="https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html#sphx-glr-beginner-blitz-autograd-tutorial-py">A Gentle Introduction to torch.autograd — PyTorch Tutorials 2.0.0+cu117 documentation</a></p>
</blockquote>
<p>在生成tensor时候我们添加属性 (requires_grad&#x3D;True )，它将开始追踪(track)在其上的所有操作（这样就可以利⽤链式法则进⾏梯度传播了）。完成计算后，可以调⽤.backward() 来完成所有梯度计算。此Tensor的梯度将累积到.grad 属性中。</p>
<p>如果不想要被继续追踪，可以调⽤ .detach() 将其从追踪记录中分离出来，这样就可以防⽌将来的计算被追踪，这样梯度就传不过去了。此外，还可以⽤ with torch.no_grad() 将不想被追踪的操作代码块包裹起来，这种⽅法在评估模型的时候很常⽤，因为在评估模型时，我们并不需要计算可训练参数（requires_grad&#x3D;True）的梯度。</p>
<blockquote>
<p>通俗的来讲就是我们添加 requires_grad 可以记录在张量上所进行的所有的操作</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = torch.ones(<span class="number">2</span>,<span class="number">2</span>,requires_grad=<span class="literal">True</span>)</span><br><span class="line">y = x+ <span class="number">2</span></span><br><span class="line">z = y*y*<span class="number">3</span></span><br><span class="line">p = z.mean()</span><br><span class="line"><span class="built_in">print</span>(p)</span><br><span class="line">-&gt;tensor(<span class="number">27.</span>, grad_fn=&lt;MeanBackward0&gt;)</span><br><span class="line">p.backward()</span><br><span class="line"><span class="built_in">print</span>(x.grad)</span><br><span class="line">-&gt;tensor([[<span class="number">4.5000</span>, <span class="number">4.5000</span>],</span><br><span class="line">        [<span class="number">4.5000</span>, <span class="number">4.5000</span>]])</span><br></pre></td></tr></table></figure>

<p>计算结果怎么来的呢？首先我们知道p所进行的计算如下</p>
<p>$$<br>p&#x3D;\frac{1}{4}\sum3(x_i+2)^2<br>$$</p>
<p>$$<br>\dfrac{\partial p}{\partial x}|_{x_i&#x3D;1}&#x3D;\frac{9}{2}&#x3D;4.5<br>$$</p>
<p>不过在此处需要一个<strong>注意事项</strong>，我们的p必须是一个标量，也就是说p必须是一个数字，不然结果会报错。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = torch.tensor([<span class="number">2.</span>, <span class="number">1.</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line">y = torch.tensor([[<span class="number">1.</span>, <span class="number">2.</span>], [<span class="number">3.</span>, <span class="number">4.</span>]], requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">z = torch.mm(x.view(<span class="number">1</span>, <span class="number">2</span>), y)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;z:<span class="subst">&#123;z&#125;</span>&quot;</span>)</span><br><span class="line">z.backward(torch.Tensor([[<span class="number">1.</span>, <span class="number">0</span>]]), retain_graph=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;x.grad: <span class="subst">&#123;x.grad&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;y.grad: <span class="subst">&#123;y.grad&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">--&gt;z:tensor([[<span class="number">5.</span>, <span class="number">8.</span>]], grad_fn=&lt;MmBackward&gt;)</span><br><span class="line">x.grad: tensor([[<span class="number">1.</span>, <span class="number">3.</span>]])</span><br><span class="line">y.grad: tensor([[<span class="number">2.</span>, <span class="number">0.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">0.</span>]])</span><br></pre></td></tr></table></figure>

<p>算法解释如下：<br><img data-src="/pytorch%E5%AD%A6%E4%B9%A0/1680176521877.png" alt="1680176521877"><br>我们在对z进行求梯度的时候，指定了参数[1,0]也就是相对于进行了加权操作</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://www.w3cschool.cn/article/9034837.html">https://www.w3cschool.cn/article/9034837.html</a></p>
<p><a href="https://blog.csdn.net/qq_39208832/article/details/117415229">https://blog.csdn.net/qq_39208832/article/details/117415229</a></p>
<p><a href="https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html#sphx-glr-beginner-blitz-autograd-tutorial-py">https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html#sphx-glr-beginner-blitz-autograd-tutorial-py</a></p>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>Pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title>pytorch实现CNN</title>
    <url>/posts/3220359105/</url>
    <content><![CDATA[<h1 id="pytorch实现CNN"><a href="#pytorch实现CNN" class="headerlink" title="pytorch实现CNN"></a>pytorch实现CNN</h1><span id="more"></span>
<p>nn.Conv2d<br>torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride&#x3D;1, padding&#x3D;0, dilation&#x3D;1, groups&#x3D;1, bias&#x3D;True, padding_mode&#x3D;’zeros’, device&#x3D;None, dtype&#x3D;None)<br><strong>参数解释</strong><br>1、in_channels (int) – 输入图像中的通道数<br>2、out_channels (int) – 卷积产生的通道数<br>3、kernel_size (int or tuple) – 卷积核的大小<br>4、stride (int or tuple, optional) – 卷积的步幅，默认值：1<br>5、padding (int, tuple or str, optional) – 添加到输入的所有四个边的填充，默认值：0<br>6、padding_mode (str, optional) – ‘zeros’, ‘reflect’, ‘replicate’ or ‘circular’，默认: ‘zeros’<br>6、dilation (int or tuple, optional) – 内核元素之间的间距，默认值：1<br>7、groups (int, optional) – 从输入通道到输出通道的阻塞连接数，默认值：1<br>8、bias (bool, optional) – 如果是 “真”，则在输出中增加一个可学习的偏置，默认值： 真  </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> collections</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line">torch.set_printoptions(edgeitems=<span class="number">2</span>)</span><br><span class="line">torch.manual_seed(<span class="number">123</span>)</span><br><span class="line">class_names = [<span class="string">&#x27;airplane&#x27;</span>,<span class="string">&#x27;automobile&#x27;</span>,<span class="string">&#x27;bird&#x27;</span>,<span class="string">&#x27;cat&#x27;</span>,<span class="string">&#x27;deer&#x27;</span>,<span class="string">&#x27;dog&#x27;</span>,<span class="string">&#x27;frog&#x27;</span>,<span class="string">&#x27;horse&#x27;</span>,<span class="string">&#x27;ship&#x27;</span>,<span class="string">&#x27;truck&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment">#下载数据</span></span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets, transforms</span><br><span class="line">data_path = <span class="string">&#x27;E:/CIFAR-10/&#x27;</span> </span><br><span class="line">cifar10 = datasets.CIFAR10(</span><br><span class="line">    data_path, train=<span class="literal">True</span>, download=<span class="literal">True</span>,</span><br><span class="line">    transform=transforms.Compose([</span><br><span class="line">        transforms.ToTensor(),</span><br><span class="line">        transforms.Normalize((<span class="number">0.4915</span>, <span class="number">0.4823</span>, <span class="number">0.4468</span>),</span><br><span class="line">                             (<span class="number">0.2470</span>, <span class="number">0.2435</span>, <span class="number">0.2616</span>))</span><br><span class="line">    ]))</span><br><span class="line">cifar10_val = datasets.CIFAR10(</span><br><span class="line">    data_path, train=<span class="literal">False</span>, download=<span class="literal">True</span>,</span><br><span class="line">    transform=transforms.Compose([</span><br><span class="line">        transforms.ToTensor(),</span><br><span class="line">        transforms.Normalize((<span class="number">0.4915</span>, <span class="number">0.4823</span>, <span class="number">0.4468</span>),</span><br><span class="line">                             (<span class="number">0.2470</span>, <span class="number">0.2435</span>, <span class="number">0.2616</span>))</span><br><span class="line">    ]))</span><br><span class="line"><span class="comment">#识别鸟和飞机</span></span><br><span class="line">label_map = &#123;<span class="number">0</span>: <span class="number">0</span>, <span class="number">2</span>: <span class="number">1</span>&#125;</span><br><span class="line">class_names = [<span class="string">&#x27;airplane&#x27;</span>, <span class="string">&#x27;bird&#x27;</span>]</span><br><span class="line">cifar2 = [(img, label_map[label]) <span class="keyword">for</span> img, label <span class="keyword">in</span> cifar10 <span class="keyword">if</span> label <span class="keyword">in</span> [<span class="number">0</span>, <span class="number">2</span>]]</span><br><span class="line">cifar2_val = [(img, label_map[label]) <span class="keyword">for</span> img, label <span class="keyword">in</span> cifar10_val <span class="keyword">if</span> label <span class="keyword">in</span> [<span class="number">0</span>, <span class="number">2</span>]]</span><br><span class="line"><span class="comment">#定义神经网络模型</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Net</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">3</span>, <span class="number">16</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.act1 = nn.Tanh()</span><br><span class="line">        self.pool1 = nn.MaxPool2d(<span class="number">2</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">16</span>, <span class="number">8</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.act2 = nn.Tanh()</span><br><span class="line">        self.pool2 = nn.MaxPool2d(<span class="number">2</span>)</span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">8</span> * <span class="number">8</span> * <span class="number">8</span>, <span class="number">32</span>)</span><br><span class="line">        self.act3 = nn.Tanh()</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">32</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        out = self.pool1(self.act1(self.conv1(x)))</span><br><span class="line">        out = self.pool2(self.act2(self.conv2(out)))</span><br><span class="line">        out = out.view(-<span class="number">1</span>, <span class="number">8</span> * <span class="number">8</span> * <span class="number">8</span>)</span><br><span class="line">        out = self.act3(self.fc1(out))</span><br><span class="line">        out = self.fc2(out)</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> datetime  </span><br><span class="line"><span class="comment">#定义损失函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">training_loop</span>(<span class="params">n_epochs, optimizer, model, loss_fn, train_loader</span>):</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, n_epochs + <span class="number">1</span>): </span><br><span class="line">        loss_train = <span class="number">0.0</span></span><br><span class="line">        <span class="keyword">for</span> imgs, labels <span class="keyword">in</span> train_loader:</span><br><span class="line">            <span class="comment">#利用GPU计算</span></span><br><span class="line">            imgs = imgs.to(<span class="string">&quot;cuda:0&quot;</span>)</span><br><span class="line">            labels = labels.to(<span class="string">&quot;cuda:0&quot;</span>)</span><br><span class="line">            <span class="comment">#计算损失函数</span></span><br><span class="line">            outputs = model(imgs)</span><br><span class="line">            loss = loss_fn(outputs, labels)</span><br><span class="line">            <span class="comment">#随机梯度下降更新参数</span></span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            loss.backward()</span><br><span class="line">            optimizer.step()</span><br><span class="line">            loss_train += loss.item()</span><br><span class="line">        <span class="keyword">if</span> epoch == <span class="number">1</span> <span class="keyword">or</span> epoch % <span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;&#123;&#125; Epoch &#123;&#125;, 训练损失&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(datetime.datetime.now(), epoch,</span><br><span class="line">                loss_train / <span class="built_in">len</span>(train_loader))) </span><br></pre></td></tr></table></figure>

<p>运行</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_loader = torch.utils.data.DataLoader(cifar2, batch_size=<span class="number">64</span>,</span><br><span class="line">                                           shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">model = Net()</span><br><span class="line">model = model.to(<span class="string">&quot;cuda:0&quot;</span>)</span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=<span class="number">1e-2</span>)</span><br><span class="line">loss_fn = nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line">training_loop(</span><br><span class="line">    n_epochs = <span class="number">100</span>,</span><br><span class="line">    optimizer = optimizer,</span><br><span class="line">    model = model,</span><br><span class="line">    loss_fn = loss_fn,</span><br><span class="line">    train_loader = train_loader,</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>CNN</tag>
      </tags>
  </entry>
  <entry>
    <title>决策树算法</title>
    <url>/posts/4012468743/</url>
    <content><![CDATA[<p><strong>决策树</strong>是一个预测模型；他代表的是对象属性与对象值之间的一种映射关系。树中每个节点表示某个对象，而每个分叉路径则代表某个可能的属性值，而每个叶节点则对应从根节点到该叶节点所经历的路径所表示的对象的值。决策树仅有单一输出，若欲有复数输出，可以建立独立的决策树以处理不同输出。 数据挖掘中决策树是一种经常要用到的技术，可以用于分析数据，同样也可以用来作预测，从数据产生决策树的机器学习技术叫做 <strong>决策树学习</strong> ,通俗说就是<strong>决策树</strong></p>
<span id="more"></span>

<h2 id="一、什么是决策树算法"><a href="#一、什么是决策树算法" class="headerlink" title="一、什么是决策树算法"></a>一、什么是决策树算法</h2><p><strong>决策树</strong>是一个预测模型；他代表的是对象属性与对象值之间的一种映射关系。树中每个节点表示某个对象，而每个分叉路径则代表某个可能的属性值，而每个叶节点则对应从根节点到该叶节点所经历的路径所表示的对象的值。决策树仅有单一输出，若欲有复数输出，可以建立独立的决策树以处理不同输出。 数据挖掘中决策树是一种经常要用到的技术，可以用于分析数据，同样也可以用来作预测，从数据产生决策树的机器学习技术叫做 <strong>决策树学习</strong> ,通俗说就是<strong>决策树</strong></p>
<blockquote>
<p>维基百科：<a href="https://zh.wikipedia.org/zh-cn/%E5%86%B3%E7%AD%96%E6%A0%91#%E7%AE%80%E4%BB%8B">https://zh.wikipedia.org/zh-cn/%E5%86%B3%E7%AD%96%E6%A0%91#%E7%AE%80%E4%BB%8B</a></p>
</blockquote>
<p>不懂？那我们引用<strong>周志华老师西瓜书</strong>上的例子，立马就能有一个大概了解。</p>
<blockquote>
<p>比如你带你表妹现在要去西瓜摊买西瓜，而作为卖西瓜老手的你总是能够一眼挑选出那个最好吃最甜的西瓜，而表妹总是选的不尽人意，表妹突发奇想向你请教怎么选出一个心满意足的西瓜。你说：得价钱！不对不对咱们在谈买西瓜，你说咱们啊，先看“它是什么颜色?”，如果是“青绿色”，则我们再看 “它的根蒂是什么形态?”，如果是“蜷缩 ”，我们再判断“它敲起来是什么声音?”，最后，我们得出最终决策：这个瓜很润，呸呸呸，是很甜！</p>
</blockquote>
<p>我相信你现在应该有一个大概了解了，不就是选择一个目的（我们需要进行的分类的标签），然后根据一系列的特征从而满足我们的目的，以后我们就借用这个特征去挑选“好瓜”。但是！先泼一盆凉水给你，我们怎么开始第一步呢？这还不简单，直接选择”颜色“呀！但是我们为什么不从”根茎“下手呢？下面就是我们要将的如何进行划分，也就是划分标准。</p>
<h2 id="二、划分标准"><a href="#二、划分标准" class="headerlink" title="二、划分标准"></a>二、划分标准</h2><h3 id="2-1-信息增益-ID3决策树算法划分标准"><a href="#2-1-信息增益-ID3决策树算法划分标准" class="headerlink" title="2.1 信息增益(ID3决策树算法划分标准)"></a>2.1 信息增益(ID3决策树算法划分标准)</h3><p>必须先了解信息熵这个概念，<strong>信息熵</strong>，维基百科上的定义：是接收的每条消息中包含的信息的平均量。这里，“消息”代表来自分布或数据流中的事件、样本或特征。（熵最好理解为不确定性的量度而不是确定性的量度，因为越随机的信源的熵越大）来自信源的另一个特征是样本的概率分布。这里的想法是，比较不可能发生的事情，当它发生了，会提供更多的信息。由于一些其他的原因，把信息（熵）定义为概率分布的对数的相反数是有道理的。事件的概率分布和每个事件的信息量构成了一个随机变量，这个随机变量的均值（即期望）就是这个分布产生的信息量的平均值（即熵）。熵的概念最早起源于物理学，用于度量一个热力学系统的无序程度。在信息论里面，熵是对不确定性的测量。但是在信息世界，<strong>熵越高，则能传输越多的信息，熵越低，则意味着传输的信息越少</strong>。还是不懂？那你不妨记住：<strong>信息熵是对信息量的一种衡量</strong></p>
<blockquote>
<p>维基百科：<a href="https://zh.wikipedia.org/zh-cn/%E7%86%B5_(%E4%BF%A1%E6%81%AF%E8%AE%BA)">https://zh.wikipedia.org/zh-cn/%E7%86%B5_(%E4%BF%A1%E6%81%AF%E8%AE%BA)</a><br>Shannon,C.E.(1948).A Mathematical Theory of Communication. Bell System Technical Journal,27(3),379–423.doi:10.1002&#x2F;j.1538-7305.1948.tb01338.x</p>
</blockquote>
<p>一般地，划分数据集的大原则是：<strong>将无序的数据变得更加有序</strong>。在划分数据集之前之后信息发生的变化称为信息增益，知道如何计算信息增益，我们就可以计算每个特征值划分数据集获得的信息增益，<strong>获得信息增益最高的特征就是最好的选择</strong>。也就是说我们可用信息增益来进行决策树的划分属性选择，他们公式如下：</p>
<p>$$<br>信息熵：Ent(D)&#x3D;-\displaystyle\sum_{k&#x3D;1}^{|y|}p_klog_2p_k \取负数：保证信息熵&gt;0<br>$$</p>
<p>其中$Ent(D)$的值越小，则消息熵越小</p>
<p>$$<br>信息增益Gain(D,a)&#x3D;Ent(D)-\sum_{v&#x3D;1}^{V}\frac{|D^v|}{|D|}Ent(D^v) \V:离散属性a的可能取值的个数<br>$$</p>
<p>怎么使用？再次借用周志华老师书上例子，我们来区分好瓜坏瓜。</p>
<blockquote>
<img data-src="https://s2.loli.net/2023/06/04/YiKdrHCL4758yJI.png" alt="202306041908136" style="zoom:75%;">

<ul>
<li><strong>注释</strong>：本文都是采用的ID3决策树算法</li>
</ul>
<p>因为我们的目的是区分好瓜坏瓜所以先计算其信息熵：</p>
<p>$$<br>Ent(D)&#x3D;-\sum_{k&#x3D;1}^{2}p_klog_2p_k&#x3D;-(\frac{8}{17}log_2(\frac{8}{17})+\frac{9}{17}log_2(\frac{9}{17}))&#x3D;0.998<br>$$</p>
<p>同理可得假设我们以色泽进行分类，那么色泽有三种可能 {青绿、乌黑、浅白}，我们再计算每种色泽下所对应好坏瓜的概率（$p_1:好，p_2:坏$）：青绿：$p_1&#x3D;0.5,p_2&#x3D;0.5$；乌黑：$p_1&#x3D;\frac{4}{6}，p_2&#x3D;\frac{2}{6}$；浅白：$p_1&#x3D;\frac{1}{5},p_2&#x3D;\frac{4}{5}$；这样一来我们可以计算各种信息增益：</p>
<p>$$<br>Ent(青绿)&#x3D;1，Ent(乌黑)&#x3D;0.918，Ent(浅白)&#x3D;0.722<br>$$</p>
<p>然后计算信息增益:</p>
<p>$$<br>Gain(D,色泽)&#x3D;Ent(D)-\sum_{v&#x3D;1}^{V}\frac{|D^v|}{|D|}Ent(D^v)&#x3D;0.998-(\frac{6}{17}\times1+\frac{6}{17}\times0.918+\frac{5}{17}\times0.722)&#x3D;0.109<br>$$</p>
<p>$$<br>同理可得其他的信息增益：\Gain(D,根蒂)&#x3D;0.143;\Gain(D,敲声)&#x3D;0.141;\Gain(D,纹理)&#x3D;0.381;\Gain(D,脐部)&#x3D;0.289;\Gain(D,触感)&#x3D;0.006;<br>$$</p>
<p>纹理的信息增益最大，所以我们取纹理作为我们的划分标准，同理从纹理出发再取计算其他属性，并且得到信息增益，以此类推只到所有标准都划分完毕。</p>
</blockquote>
<h3 id="2-2-基尼指数-CART决策树算法划分标准"><a href="#2-2-基尼指数-CART决策树算法划分标准" class="headerlink" title="2.2 基尼指数(CART决策树算法划分标准)"></a>2.2 基尼指数(CART决策树算法划分标准)</h3><h2 id="三、评价"><a href="#三、评价" class="headerlink" title="三、评价"></a>三、评价</h2><p><strong>决策树的优点</strong><br>1、决策树易于理解和实现.人们在通过解释后都有能力去理解决策树所表达的意义<br>2、对于决策树，数据的准备往往是简单或者是不必要的.其他的技术往往要求先把数据一般化，比如去掉多余的或者空白的属性<br>3、能够同时处理数据型和常规型属性。其他的技术往往要求数据属性的单一<br>4、是一个白盒模型如果给定一个观察的模型，那么根据所产生的决策树很容易推出相应的逻辑表达式<br>5、易于通过静态测试来对模型进行评测。表示有可能测量该模型的可信度<br>6、在相对短的时间内能够对大型数据源做出可行且效果良好的结果<br><strong>决策树的缺点</strong><br>对于那些各类别样本数量不一致的数据，在决策树当中信息增益的结果偏向于那些具有更多数值的特征</p>
<p>文章如若有错误，欢迎大佬们批评指正。💪💪</p>
<h2 id="四、代码"><a href="#四、代码" class="headerlink" title="四、代码"></a>四、代码</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#计算信息熵</span></span><br><span class="line"><span class="keyword">from</span> math <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">ent</span>(<span class="params">data</span>):</span><br><span class="line">    data_length = <span class="built_in">len</span>(data)</span><br><span class="line">    dic = &#123;&#125;</span><br><span class="line">    <span class="comment">#统计个数</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> data:</span><br><span class="line">        data_leable = i[-<span class="number">1</span>]</span><br><span class="line">        <span class="keyword">if</span> data_leable <span class="keyword">not</span> <span class="keyword">in</span> dic:</span><br><span class="line">            dic[data_leable] = <span class="number">0</span></span><br><span class="line">        dic[data_leable] +=<span class="number">1</span></span><br><span class="line">   <span class="comment">#return dic</span></span><br><span class="line">    <span class="comment">#计算信息熵</span></span><br><span class="line">    ent_number = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> dic:</span><br><span class="line">        num1 = <span class="built_in">float</span>(dic[j])/data_length</span><br><span class="line">        ent_number = ent_number - num1*log(num1,<span class="number">2</span>)</span><br><span class="line">    <span class="keyword">return</span> ent_number</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#划分数据集合</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">data_split</span>(<span class="params">data, axis, value</span>):</span><br><span class="line">    data_list = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> data:</span><br><span class="line">        <span class="keyword">if</span> i[axis] == value:</span><br><span class="line">            feat = i[:axis]</span><br><span class="line">            feat.extend(i[axis+<span class="number">1</span>:])</span><br><span class="line">            data_list.append(feat)</span><br><span class="line">    <span class="keyword">return</span> data_list</span><br><span class="line"></span><br><span class="line"><span class="comment">#extend与append函数区别：a.extend(b):将b的值全部加到a中；a.append(b):将b列表加到a中</span></span><br><span class="line"><span class="comment"># a = [1,2]</span></span><br><span class="line"><span class="comment"># b = [3,4]</span></span><br><span class="line"><span class="comment"># c = [3,4]</span></span><br><span class="line"><span class="comment"># a.append(b)</span></span><br><span class="line"><span class="comment"># c.extend(b)</span></span><br><span class="line"><span class="comment"># print(a,c)</span></span><br><span class="line"><span class="comment"># [1, 2, [3, 4]] [3, 4, 3, 4]</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#计算信增益,并且挑选最佳特征</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">gain_chose</span>(<span class="params">data</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    data_ent:信息熵</span></span><br><span class="line"><span class="string">    feat_list:每一种标签的全部数据</span></span><br><span class="line"><span class="string">    feat_value:每一种标签下的取值</span></span><br><span class="line"><span class="string">    data_gain:信息增益</span></span><br><span class="line"><span class="string">    ent_number:计算各自列下的信息熵</span></span><br><span class="line"><span class="string">    prop:计算概率</span></span><br><span class="line"><span class="string">    best_feature:最佳划分特征</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    data_base_length = <span class="built_in">len</span>(data[<span class="number">0</span>])-<span class="number">1</span></span><br><span class="line">    ent_base = ent(data)</span><br><span class="line">    gain_max = -<span class="number">1</span> <span class="comment">#取任何小于0的值都可以</span></span><br><span class="line">    <span class="comment">#第一个for循环得到每一列，第二个for循环则是对每一列中取值</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(data_base_length):</span><br><span class="line">        feat_list = [a[i] <span class="keyword">for</span> a <span class="keyword">in</span> data]</span><br><span class="line">        feat_value = <span class="built_in">set</span>(feat_list)</span><br><span class="line">        ent_number = <span class="number">0.0</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> feat_value:</span><br><span class="line">            data_split_use = data_split(data, i, j)</span><br><span class="line">            prop = <span class="built_in">len</span>(data_split_use)/<span class="built_in">float</span>(<span class="built_in">len</span>(data_split_use))</span><br><span class="line">            ent_number = ent_number + prop * ent(data_split_use)</span><br><span class="line">        data_gain = ent_base - ent_number </span><br><span class="line">        <span class="keyword">if</span> data_gain &gt; gain_max:</span><br><span class="line">            gain_max = data_gain</span><br><span class="line">            best_feature = i</span><br><span class="line">    <span class="keyword">return</span> best_feature</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#绘制决策树</span></span><br><span class="line"><span class="keyword">import</span> operator</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">majorityCnt</span>(<span class="params">classList</span>):</span><br><span class="line">    classCount=&#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> vote <span class="keyword">in</span> classList:</span><br><span class="line">        <span class="keyword">if</span> vote <span class="keyword">not</span> <span class="keyword">in</span> classCount.keys(): classCount[vote] = <span class="number">0</span></span><br><span class="line">        classCount[vote] += <span class="number">1</span></span><br><span class="line">    sortedClassCount = <span class="built_in">sorted</span>(classCount.iteritems(), key=operator.itemgetter(<span class="number">1</span>), reverse=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> sortedClassCount[<span class="number">0</span>][<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">createTree</span>(<span class="params">dataSet,labels</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    classList:存储dataset中的标签</span></span><br><span class="line"><span class="string">    bestFeatLabel:根节点</span></span><br><span class="line"><span class="string">    myTree:存储树结构</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    classList = [example[-<span class="number">1</span>] <span class="keyword">for</span> example <span class="keyword">in</span> dataSet]</span><br><span class="line">    <span class="comment"># 如果全是一个特征，直接返回</span></span><br><span class="line">    <span class="keyword">if</span> classList.count(classList[<span class="number">0</span>]) == <span class="built_in">len</span>(classList): </span><br><span class="line">        <span class="keyword">return</span> classList[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># 如果数组长度为1，则</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(dataSet[<span class="number">0</span>]) == <span class="number">1</span>: </span><br><span class="line">        <span class="keyword">return</span> majorityCnt(classList)</span><br><span class="line"></span><br><span class="line">    bestFeat = gain_chose(dataSet)</span><br><span class="line">    bestFeatLabel = labels[bestFeat]     <span class="comment"># 挑选出根节点</span></span><br><span class="line">    myTree = &#123;bestFeatLabel:&#123;&#125;&#125;</span><br><span class="line">    <span class="keyword">del</span>(labels[bestFeat]) <span class="comment"># 删除以免再次选到</span></span><br><span class="line"></span><br><span class="line">    featValues = [example[bestFeat] <span class="keyword">for</span> example <span class="keyword">in</span> dataSet]</span><br><span class="line">    uniqueVals = <span class="built_in">set</span>(featValues)</span><br><span class="line">    <span class="keyword">for</span> value <span class="keyword">in</span> uniqueVals:</span><br><span class="line">        subLabels = labels[:] </span><br><span class="line">        myTree[bestFeatLabel][value] = createTree(data_split(dataSet, bestFeat, value),subLabels)</span><br><span class="line">    <span class="keyword">return</span> myTree   </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="五、决策树算法流程"><a href="#五、决策树算法流程" class="headerlink" title="五、决策树算法流程"></a>五、决策树算法流程</h2><p>第一步：计算信息熵：利用<strong>哈希表</strong>得到分类标签的对应数量关系，而后感觉对应的数量关系计算信息熵。<br>第二步：计算信息增益并且挑选出最佳特征：分为两个步骤。第一步：划分数据集合，第二步计算</p>
<ul>
<li>第一步：以西瓜例子说明，我们在挑选好 ‘yes’or’no’ 之后，我们要回过头观察前面特征的集合（色泽：a），而后在色泽：a中分别计算满足’yes’or’no’的信息熵。依次类推分别得到纹理等特征的信息熵。</li>
<li>第二步：在我们前一步所得到的信息熵中计算信息增益</li>
<li>第三步：挑选信息增益最大的值<br>第三步：挑选好最佳特征之后开始绘制决策树</li>
</ul>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><blockquote>
<p>周志华 《西瓜书》</p>
<p>《机器学习实战》</p>
</blockquote>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>评价标准</tag>
      </tags>
  </entry>
  <entry>
    <title>初识深度学习</title>
    <url>/posts/3065358813/</url>
    <content><![CDATA[<h1 id="初识深度学习"><a href="#初识深度学习" class="headerlink" title="初识深度学习"></a>初识深度学习</h1><span id="more"></span>
<hr>
<p><strong>注：</strong></p>
<p>1、本文为个人学习笔记，所以内容大多比较简练不做过多解释</p>
<p>2、文章参考书籍：1、《python深度学习》[美] 弗朗索瓦·肖莱　著 张亮 译；2、《深度学习》(花书)</p>
<p>3、部分内容自己阅读参考文献进行补充</p>
<p>4、大部分代码都是<strong>基于pytorch</strong>进行编写&lt;先学原理后学代码&gt;</p>
<p>n、后续用到什么就继续继续补充</p>
<hr>
<p>[toc]</p>
<h2 id="一、初识神经网络"><a href="#一、初识神经网络" class="headerlink" title="一、初识神经网络"></a>一、初识神经网络</h2><h3 id="1-1-神经网络的工作原理"><a href="#1-1-神经网络的工作原理" class="headerlink" title="1.1 神经网络的工作原理"></a>1.1 神经网络的工作原理</h3><p><img data-src="https://s2.loli.net/2023/06/04/wCQ1ymusatMEjkX.png" alt="1680779540890"></p>
<ul>
<li>优化器：随机梯度下降、牛顿法</li>
<li>层：可以取类比多层感知机模型中的隐藏层</li>
<li>数据变换：激活函数</li>
</ul>
<p>进一步解释见：2.2.3 基于梯度优化</p>
<h3 id="1-2-神经网络基础知识"><a href="#1-2-神经网络基础知识" class="headerlink" title="1.2 神经网络基础知识"></a>1.2 神经网络基础知识</h3><h4 id="1-2-1-张量"><a href="#1-2-1-张量" class="headerlink" title="1.2.1 张量"></a>1.2.1 张量</h4><p>关于张量的了解其实没必要过多去解释他到底是个什么样子的，python编程过程中可以直接将其与numpy的矩阵一起了解(<strong>下以pytorch为例</strong>)</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">a = torch.ones(<span class="number">3</span>,<span class="number">2</span>)</span><br><span class="line">a</span><br><span class="line">&gt;tensor([[<span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">1.</span>]])</span><br></pre></td></tr></table></figure>

<p>但是张量的一些基本性质还是需要了解：</p>
<ul>
<li><strong>轴的个数（阶）</strong>。例如，3D 张量有 3 个轴，矩阵有 2 个轴。这在 Numpy 等 Python 库中也叫张量的 ndim。</li>
<li><strong>形状</strong>。这是一个整数元组，表示张量沿每个轴的维度大小（元素个数）。例如，上述列子就是一个二维的3x2</li>
<li><strong>数据类型</strong>。这是张量中所包含数据的类型，例如，张量的类型可以是 float32、uint8、float64 等。在极少数情况下，你可能会遇到字符（char）张量。注意，Numpy（以及大多数其他库）中不存在字符串张量，因为张量存储在预先分配的连续内存段中，而字符串的长度是可变的，无法用这种方式存储。</li>
</ul>
<blockquote>
<p><strong>pytorch的具体语法，后续继续补充</strong></p>
</blockquote>
<h5 id="1-2-1-1-时间序列数据"><a href="#1-2-1-1-时间序列数据" class="headerlink" title="1.2.1.1 时间序列数据"></a>1.2.1.1 时间序列数据</h5><p>以股票交易为例：每时每刻都存在交易，并且每时每刻对于不同的股票也都存在买入、卖出。那么我们可以借助3D张量进行表示：</p>
<img data-src="https://s2.loli.net/2023/06/04/d3XRzVQlGiwokWy.png" alt="202306041904592" style="zoom:100%;">

<ul>
<li><strong>时间步长</strong>：可以假设是1分钟内进行交易</li>
<li><strong>特征</strong>：某一个股票进行的交易</li>
<li><strong>样本</strong>：不同的层可以表示是不同股票：茅台。。。。。</li>
</ul>
<h5 id="1-2-1-2-图像数据"><a href="#1-2-1-2-图像数据" class="headerlink" title="1.2.1.2 图像数据"></a>1.2.1.2 图像数据</h5><p>一张<strong>彩色</strong>图片可以由3部分构成：高度、宽度、颜色深度（灰度图像只有一个颜色通道可以取消颜色深度），那么图片也是3D张量：</p>
<img data-src="https://s2.loli.net/2023/06/04/6ojruqShLPxmCdO.png" alt="202306041905909" style="zoom:100%;">

<blockquote>
<p>假设图片为<strong>彩色</strong>，大小为256x256。那么我们张量可以如此表示：（n, 256, 256, 3)。n：图片数目；3：彩色图片一般3种颜色（光的3原色）</p>
</blockquote>
<blockquote>
<p><a href="https://baike.baidu.com/item/%E4%B8%89%E5%8E%9F%E8%89%B2%E5%8E%9F%E7%90%86/6969780">三原色原理_百度百科 (baidu.com)</a></p>
</blockquote>
<p>$$<br>\textcolor{red}{注意}\text{:PyTorch 模块要求张量排列为 C×H×W（分别表示通道、高度和宽度）而tensorflow则是：H×WxC}<br>$$</p>
<h5 id="1-2-1-3-视频数据"><a href="#1-2-1-3-视频数据" class="headerlink" title="1.2.1.3 视频数据"></a>1.2.1.3 视频数据</h5><p>视频是一帧一帧所构成的，也就是说我们看到的视频都是一张一张的图片，也就是说一个视频是一个4D张量（不同视频构成5D张量）</p>
<h4 id="1-2-2-张量的运算"><a href="#1-2-2-张量的运算" class="headerlink" title="1.2.2 张量的运算"></a>1.2.2 张量的运算</h4><blockquote>
<p>后续补充！</p>
<p><a href="https://www.w3cschool.cn/pytorch/pytorch-5ubt3bby.html">1、PyTorch是什么？_w3cschool</a></p>
</blockquote>
<h4 id="1-2-3-基于梯度优化"><a href="#1-2-3-基于梯度优化" class="headerlink" title="1.2.3 基于梯度优化"></a>1.2.3 基于梯度优化</h4><p>在此引用神经网络工作原理图：</p>
<img data-src="https://s2.loli.net/2023/06/04/wCQ1ymusatMEjkX.png" alt="202306041900967" style="zoom:100%;">


<p>在多层感知机模型中有3类常用的激活函数：Sigmoid、Relu、Tannh。假设卷积神经网络只有2层：$f(x)&#x3D;f^{(1)}(f^{(2)}(x,w,b))$此处我们的$f^{(2)}$就是我们的激活函数，而$w,b$就是我们的权重。一开始，这些权重矩阵取较小的<strong>随机值</strong>，这一步叫作<strong>随机初始化</strong>，他们肯定不会得到任何有用的表示。虽然得到的表示是没有意义的，但这是一个起点。下一步则是根据反馈信号逐渐调节这些权重。这个逐渐调节的过程叫作<strong>训练</strong>，也就是机器学习中的学习。</p>
<p>上述过程发生在一个训练循环（training loop）内，其具体过程如下。必要时一直重复这些步骤。</p>
<ul>
<li>(1) 抽取训练样本 x 和对应目标 y 组成的数据批量。</li>
<li>(2) 在 x 上运行网络［这一步叫作<strong>前向传播</strong>］，得到预测值 y_pred。</li>
<li>(3) 计算网络在这批数据上的<em>损失</em>，用于衡量 y_pred 和 y 之间的距离。</li>
<li>(4) 更新网络的所有权重，使网络在这批数据上的损失略微下降</li>
</ul>
<blockquote>
<p><em>关于权重深度学习有何不同</em></p>
<p>先前的机器学习技术（浅层学习）仅包含将输入数据变换到一两个连续的表示空间，通常使用简单的变换，比如高维非线性投影（SVM）或决策树。但这些技术通常无法得到复杂问题所需要的精确表示。因此，人们必须竭尽全力让初始输入数据更适合用这些方法处理，也必须手动为数据设计好的表示层。这叫作<strong>特征工程</strong>，深度学习完全将这个步骤自动化。</p>
</blockquote>
<p>上述步骤复杂在于更新权重！我们更新权重的目的：使得损失函数最小。我们使用得方法是：<strong>随机梯度下降：</strong></p>
<ul>
<li>(1) 抽取训练样本 x 和对应目标 y 组成的数据批量。</li>
<li>(2) 在 x 上运行网络，得到预测值 y_pred。</li>
<li>(3) 计算网络在这批数据上的损失，用于衡量 y_pred 和 y 之间的距离。</li>
<li>(4) 计算损失相对于网络参数的梯度［一次<strong>反向传播</strong>］。</li>
<li>(5) 将参数沿着梯度的反方向移动一点，比如 W -&#x3D; step * gradient，从而使这批数据上的损失减小一点。</li>
</ul>
<p>前向传播和反向传播：</p>
<ul>
<li>前向传播：输入提供的初始信息x，然后传播给每一层得隐藏单元，最终产出输出y</li>
<li>反向传播：允许来自代价函数的信息通过网络向后流动，以便计算梯度</li>
</ul>
<blockquote>
<p>设存在模型：$\widehat{y}&#x3D;wx+b$，设损失函数为：$J&#x3D;\frac{1}{2}(y-\widehat{y})^2$。</p>
<p><strong>前向传播</strong>：我们直接输入$x$最后返回y</p>
<p><strong>反向传播</strong>：我们在初始化参数：$w,b$时候所得到$\widehat{y}$损失函数值比较大。那么根据梯度下降我们需要对$w，b$进行更新于是就有：梯度：$\nabla_{w}J &#x3D;\dfrac{\partial J}{\partial \widehat{y}}\dfrac{\partial \widehat{y}}{\partial {w}}$，所以：$w&#x3D;w-\epsilon\nabla_{w}J$，我们计算梯度的过程就是反向传播过程</p>
</blockquote>
<h2 id="二、机器学习补充"><a href="#二、机器学习补充" class="headerlink" title="二、机器学习补充"></a>二、机器学习补充</h2><h3 id="2-1-机器学习模型评估"><a href="#2-1-机器学习模型评估" class="headerlink" title="2.1 机器学习模型评估"></a>2.1 机器学习模型评估</h3><p>机器学习的目的是得到可以<strong>泛化的模型</strong>，即在前所未见的数据上表现很好。在机器学习过程中我们经常需要对数据进行切分：划分训练集、测试集。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split </span><br><span class="line">x_train, x_test, y_train, y_test= train_test_split(x,y,test_size=<span class="number">0.2</span>) <span class="comment">#选取20%的数据作为测试数据</span></span><br></pre></td></tr></table></figure>

<blockquote>
<p>切分有两种：</p>
<p>1、简单的留出验证（上述代码）</p>
<p>2、<strong>K 折验证</strong></p>
<p>将数据划分为大小相同的 K 个分区。对于每个分区 i，在剩余的 K-1 个分区上训练模型，然后在分区 i 上评估模型。最终分数等于 K 个分数的平均值。</p>
<p><img data-src="/%E5%88%9D%E8%AF%86%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/1680867838870.png" alt="1680867838870"></p>
</blockquote>
<p>我们最终的目的是我们设计的模型在测试集上、训练集上的测试效果都满足我们的要求。在此过程中涉及两个问题：1、<strong>欠拟合</strong>：模型不能再训练集上或者足够低的误差；2、<strong>过拟合</strong>：训练误差和测试误差之间差距过大。</p>
<h3 id="2-2-正则化"><a href="#2-2-正则化" class="headerlink" title="2.2 正则化"></a>2.2 正则化</h3><p>为了防止模型从训练数据中学到错误或无关紧要的模式，<strong>最优解决方法是获取更多的训练数据</strong>。模型的训练数据越多，泛化能力自然也越好。如果无法获取更多数据，次优解决方法是<strong>调节模型允许存储的信息量</strong>，或对模型允许存储的信息加以约束。如果一个网络只能记住几个模式，那么优化过程会迫使模型集中学习最重要的模式，这样更可能得到良好的泛化。这种降低过拟合的方法叫作正则化。</p>
<h3 id="2-2-1-添加权重正则化"><a href="#2-2-1-添加权重正则化" class="headerlink" title="2.2.1 添加权重正则化"></a>2.2.1 添加权重正则化</h3><p>L2正则化：</p>
<p>$$<br>\widehat{J}(w,x,y)&#x3D;J(w,x,y)+\frac{\alpha}{2}||w||^2<br>$$</p>
<p>L1正则化：</p>
<p>$$<br>\widehat{J}(w,x,y)&#x3D;J(w,x,y)+\alpha||w||<br>$$</p>
<p>上述函数：$J$：损失函数，$||w||$：我们添加正则化项。在此过程中我们只对权重做惩罚而不对偏置做惩罚。</p>
<h2 id="2-3-机器学习流程"><a href="#2-3-机器学习流程" class="headerlink" title="2.3 机器学习流程"></a>2.3 机器学习流程</h2><p>1、定义问题，收集数据集</p>
<p>2、选择衡量成功的指标</p>
<blockquote>
<p>选择合适评价指标：RMSE、准确率、召回率等</p>
</blockquote>
<p>3、确定评估方法</p>
<blockquote>
<p>验证集、K折交叉验证等</p>
</blockquote>
<p>4、准备数据</p>
<p>5、建立模型并优化</p>
<blockquote>
<p>正则化、调节超参数</p>
</blockquote>
<h2 id="三、推荐文献"><a href="#三、推荐文献" class="headerlink" title="三、推荐文献"></a>三、推荐文献</h2><p>1、LECUN Y, BENGIO Y, HINTON G. Deep learning[J&#x2F;OL]. Nature, 2015, 521(7553): 436-444. <a href="https://doi.org/10.1038/nature14539">https://doi.org/10.1038/nature14539</a>.</p>
<p>2、RUMELHART D E, HINTON G E, WILLIAMS R J. Learning representations by back-propagating errors[J&#x2F;OL]. Nature, 1986, 323(6088): 533-536. <a href="https://doi.org/10.1038/323533a0">https://doi.org/10.1038/323533a0</a>.</p>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>分类算法</tag>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title>图像识别算法--VGG16</title>
    <url>/posts/6d8b26d3/</url>
    <content><![CDATA[<p><strong>前言</strong>：人类科技就是不断烧开水（发电）、丢石头（航天等）。深度学习就是一个不断<strong>解方程</strong>的过程（参数量格外大的方程）</p>
<p><strong>本文内容</strong>：</p>
<p>1、介绍VGG16基本原理</p>
<p>2、VGG16 pytorch复现</p>
<span id="more"></span>

<h1 id="图像识别算法–VGG16"><a href="#图像识别算法–VGG16" class="headerlink" title="图像识别算法–VGG16"></a>图像识别算法–VGG16</h1><p>[TOC]</p>
<h1 id="1、参考文献"><a href="#1、参考文献" class="headerlink" title="1、参考文献"></a>1、参考文献</h1><p>VGG16：[1]SIMONYAN K, ZISSERMAN A. Very Deep Convolutional Networks for Large-Scale Image Recognition[M&#x2F;OL]. arXiv, 2015[2023-04-07]. <a href="http://arxiv.org/abs/1409.1556">http://arxiv.org/abs/1409.1556</a>.</p>
<p>Dropout：[2]SRIVASTAVA N, HINTON G, KRIZHEVSKY A, 等. Dropout: A Simple Way to Prevent Neural Networks from Overﬁtting[J].</p>
<h1 id="2、VGG16理论"><a href="#2、VGG16理论" class="headerlink" title="2、VGG16理论"></a>2、VGG16理论</h1><h2 id="2-1-VGG16-优点"><a href="#2-1-VGG16-优点" class="headerlink" title="2.1 VGG16 优点"></a>2.1 VGG16 优点</h2><p>1、使用3x3的卷积核而非7x7的卷积核</p>
<blockquote>
<p>First, we incorporate three non-linear rectification layers instead of a single one, which makes the decision function more discriminative. Second, we decrease the number of parameters.</p>
</blockquote>
<p>也就是说VGG16一方面减少了参数（相对于7x7），另外一方面通过3非线性层，更加具有非线性表达能力</p>
<h2 id="2-2-VGG16网络结构图"><a href="#2-2-VGG16网络结构图" class="headerlink" title="2.2 VGG16网络结构图"></a>2.2 VGG16网络结构图</h2><p>VGG设计的神经网络结构图：</p>
<img data-src="https://s2.loli.net/2023/05/22/TMopYLiGVAXEZ4a.png" alt="image-20230521224103927" style="zoom:100%;">

<blockquote>
<p>D：dropout</p>
</blockquote>
<p><em>图片变化过程</em>：</p>
<p>1：输入我们的川建国（224x224x3）—&gt;224x224x64—&gt;112x112x64</p>
<p>2：112x112x64—&gt;112x112x128—&gt;56x56x128</p>
<p>3：56x56x128—&gt;56x56x256—&gt;56x56x256—&gt;28x28x256</p>
<p>4：28x28x256—&gt;28x28x512—&gt;28x28x512—&gt;14x14x512</p>
<p>5：14x14x512—&gt;14x14x512—&gt;14x14x512—&gt;7x7x512</p>
<blockquote>
<p>变化过程第一个数字代表输入，最后一个数字代表这一层的输出，下一层的输入</p>
</blockquote>
<p>全连接层：</p>
<p>1、输入：7x7x512（25088），输出：4096</p>
<p>2、输入：4096，输出4096</p>
<p>3、输入：4096，输出1000 (<strong>因为进行的是1000个分类</strong>）</p>
<p>在参考文献1中作者同时比较了下面几种不同结构（VGG11、VGG16与VGG19）：</p>
<img data-src="https://s2.loli.net/2023/06/04/tDAGlFpbTnv7wXP.png" alt="image-20230521225247719" style="zoom: 50%;">

<hr>
<p><strong>建议</strong>：</p>
<blockquote>
<p>we have found that our conceptually much simpler scheme already provides a speedup of 3.75 times on an off-the-shelf 4-GPU system, as compared to using a single GPU. On a system equipped with four NVIDIA Titan Black GPUs, training a single net took 2–3 weeks depending on the architecture.</p>
<p>我们发现，与使用单个GPU相比，我们在概念上更简单的方案已经在现成的4 - GPU系统上提供了3.75倍的加速比。在搭载4个NVIDIA Titan Black GPU的系统上，根据架构的不同，训练单个网络需要2 ~ 3周。</p>
</blockquote>
<p>访问链接：<a href="https://www.image-net.org/challenges/LSVRC/2012/index.php">https://www.image-net.org/challenges/LSVRC/2012/index.php</a></p>
<p>如果想复现VGG16，直接使用论文作者数据是不要切合实际的：1、数据过大；2、没有这么高的电脑配置。</p>
<p>推荐使用数据集：<a href="https://download.pytorch.org/tutorial/hymenoptera_data.zip">https://download.pytorch.org/tutorial/hymenoptera_data.zip</a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">url = <span class="string">&quot;https://download.pytorch.org/tutorial/hymenoptera_data.zip&quot;</span></span><br><span class="line">save_path = os.path.join(data_dir, <span class="string">&quot;hymenoptera_data.zip&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(save_path):</span><br><span class="line">    urllib.request.urlretrieve(url, save_path)</span><br><span class="line">    <span class="built_in">zip</span> = zipfile.ZipFile(save_path)</span><br><span class="line">    <span class="built_in">zip</span>.extractall(data_dir)</span><br><span class="line">    <span class="built_in">zip</span>.close()</span><br><span class="line">    os.remove(save_path)</span><br></pre></td></tr></table></figure>

<p>pytorch官方数据，主要是实现蜜蜂和蚂蚁分类，不过在使用前必须对图片进行处理，因为他提供的图片并非都是<strong>224x224x3</strong>，所以需要对图片进行转换。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">图片预处理：</span></span><br><span class="line"><span class="string">1、图片裁剪</span></span><br><span class="line"><span class="string">2、标准化</span></span><br><span class="line"><span class="string">3、图片旋转</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ImageTransform</span>():</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, resize, mean, std</span>):</span><br><span class="line">        self.data_transform = &#123;</span><br><span class="line">            <span class="string">&#x27;train&#x27;</span>: transforms.Compose([</span><br><span class="line">                transforms.RandomResizedCrop(resize, scale=(<span class="number">0.5</span>, <span class="number">1.0</span>)), </span><br><span class="line">                <span class="comment">#scale在调整大小之前，指定裁剪的随机区域的下限和上限。规模是相对于原始图像的面积来定义的</span></span><br><span class="line">                transforms.RandomHorizontalFlip(), <span class="comment">#以给定的概率随机水平翻转给定的图像</span></span><br><span class="line"></span><br><span class="line">                transforms.ToTensor(), <span class="comment">#将图片转化为张量</span></span><br><span class="line">                transforms.Normalize(mean, std) <span class="comment">#将图片进行正则化</span></span><br><span class="line">            ]),</span><br><span class="line">            <span class="string">&#x27;val&#x27;</span>: transforms.Compose([</span><br><span class="line">                transforms.Resize(resize), <span class="comment">#改变尺寸</span></span><br><span class="line">                transforms.CenterCrop(resize), <span class="comment">#中心裁剪图像</span></span><br><span class="line"></span><br><span class="line">                transforms.ToTensor(), </span><br><span class="line">                transforms.Normalize(mean, std)</span><br><span class="line">            ])</span><br><span class="line">        &#125;</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__call__</span>(<span class="params">self, img, phase=<span class="string">&#x27;train&#x27;</span></span>):</span><br><span class="line">        <span class="keyword">return</span> self.data_transform[phase](img)</span><br></pre></td></tr></table></figure>

<p>上述代码涉及到一个理论：在卷积神经网络中（VGG也是一种卷积神经网络），在对于训练集数据不足的时候，可以尝试对图片进行旋转等操作来补充训练集数据。比如我们川建国，我旋转他就相当于又增加了一个训练集数据。</p>
<p><strong>如果实验室电脑配置不够</strong>：建议直接租算力（如果只是轻微使用深度学习+实验室没钱）</p>
<p>推荐网站：<a href="https://www.autodl.com/home">AutoDL-品质GPU租用平台-租GPU就上AutoDL</a>，学生认证价格也还ok。网站提供GPU（部分）：</p>
<img data-src="https://s2.loli.net/2023/05/22/MkONHI4BzLD8XAp.png" alt="image-20230521231718698" style="zoom:50%;">

<hr>
<p>多尺度评价的实验结果：</p>
<img data-src="https://s2.loli.net/2023/05/22/jlU6khWQNdmXTLO.png" alt="image-20230521230626642" style="zoom: 80%;">

<p>作者操作过程中还使用了：1、$L_{2}$范数；2、设置0.5的dropout</p>
<h2 id="2-2-1-复现代码"><a href="#2-2-1-复现代码" class="headerlink" title="2.2.1 复现代码"></a>2.2.1 复现代码</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#定义训练网络 VGG-16</span></span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">vgg16</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="comment">#开始定义网络结构</span></span><br><span class="line">        self.conv1 = torch.nn.Conv2d(<span class="number">3</span>, <span class="number">64</span>, <span class="number">3</span>, padding=(<span class="number">1</span>,<span class="number">1</span>))</span><br><span class="line">        self.conv2 = torch.nn.Conv2d(<span class="number">64</span>, <span class="number">64</span>, <span class="number">3</span>, padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        self.pool1 = torch.nn.MaxPool2d((<span class="number">2</span>, <span class="number">2</span>), padding=(<span class="number">1</span>, <span class="number">1</span>)) <span class="comment">#64x112x112</span></span><br><span class="line"></span><br><span class="line">        self.conv3 = torch.nn.Conv2d(<span class="number">64</span>, <span class="number">128</span>,<span class="number">3</span>,padding=(<span class="number">1</span>,<span class="number">1</span>))</span><br><span class="line">        self.conv4 = torch.nn.Conv2d(<span class="number">128</span>, <span class="number">128</span>, <span class="number">3</span>, padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        self.pool2 = torch.nn.MaxPool2d((<span class="number">2</span>, <span class="number">2</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">        self.conv5 = torch.nn.Conv2d(<span class="number">128</span>, <span class="number">256</span>,<span class="number">3</span>, padding=(<span class="number">1</span>,<span class="number">1</span>))</span><br><span class="line">        self.conv6 = torch.nn.Conv2d(<span class="number">256</span>, <span class="number">256</span>,<span class="number">3</span>, padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        self.conv7 = torch.nn.Conv2d(<span class="number">256</span>, <span class="number">256</span>,<span class="number">3</span>, padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        self.pool3 = torch.nn.MaxPool2d((<span class="number">2</span>,<span class="number">2</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))       </span><br><span class="line">        </span><br><span class="line">        self.conv8 = torch.nn.Conv2d(<span class="number">256</span>, <span class="number">512</span>,<span class="number">3</span>, padding=(<span class="number">1</span>,<span class="number">1</span>))</span><br><span class="line">        self.conv9 = torch.nn.Conv2d(<span class="number">512</span>, <span class="number">512</span>,<span class="number">3</span>, padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        self.conv10 = torch.nn.Conv2d(<span class="number">512</span>, <span class="number">512</span>,<span class="number">3</span>, padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        self.pool4 = torch.nn.MaxPool2d((<span class="number">2</span>,<span class="number">2</span>),padding=(<span class="number">1</span>, <span class="number">1</span>))         </span><br><span class="line"></span><br><span class="line">        self.conv11 = torch.nn.Conv2d(<span class="number">512</span>, <span class="number">512</span>,<span class="number">3</span>)</span><br><span class="line">        self.conv12 = torch.nn.Conv2d(<span class="number">512</span>, <span class="number">512</span>,<span class="number">3</span>, padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        self.conv13 = torch.nn.Conv2d(<span class="number">512</span>, <span class="number">512</span>,<span class="number">3</span>, padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        self.pool5 = torch.nn.MaxPool2d((<span class="number">2</span>,<span class="number">2</span>),padding=(<span class="number">1</span>, <span class="number">1</span>)) </span><br><span class="line"></span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">512</span>*<span class="number">7</span>*<span class="number">7</span>, <span class="number">4096</span>)</span><br><span class="line">        self.dropout1 = nn.Dropout(<span class="number">0.5</span>)</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">4096</span>, <span class="number">4096</span>)</span><br><span class="line">        self.dropout2 = nn.Dropout(<span class="number">0.5</span>)</span><br><span class="line">        self.fc3 = nn.Linear(<span class="number">4096</span>, <span class="number">2</span>)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        insize = x.size(<span class="number">0</span>)</span><br><span class="line">        out = F.relu(self.conv1(x))</span><br><span class="line">        out = self.pool1(F.relu(self.conv2(out)))</span><br><span class="line"></span><br><span class="line">        out = F.relu(self.conv3(out))</span><br><span class="line">        out = self.pool2(F.relu(self.conv4(out)))</span><br><span class="line"></span><br><span class="line">        out = F.relu(self.conv5(out))</span><br><span class="line">        out = F.relu(self.conv6(out))</span><br><span class="line">        out = self.pool3(F.relu(self.conv7(out)))</span><br><span class="line"></span><br><span class="line">        out = F.relu(self.conv8(out))</span><br><span class="line">        out = F.relu(self.conv9(out))</span><br><span class="line">        out = self.pool4(F.relu(self.conv10(out)))</span><br><span class="line"></span><br><span class="line">        out = F.relu(self.conv11(out))</span><br><span class="line">        out = F.relu(self.conv12(out))</span><br><span class="line">        out = self.pool5(F.relu(self.conv13(out)))</span><br><span class="line"></span><br><span class="line">        out = out.view(insize, -<span class="number">1</span>) <span class="comment">#这里对于不同数据处理会有不一样，-1位于后面相当于直接将数据进行平铺--&gt;1*n；</span></span><br><span class="line">        <span class="comment"># -1位于前面则---&gt;n*1</span></span><br><span class="line">        out = self.dropout1(self.act1(self.fc1(out)))</span><br><span class="line">        out = self.dropout2(self.act1(self.fc2(out)))</span><br><span class="line">        out = self.fc3(out)</span><br><span class="line">        out = F.log_softmax(out, dim=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line">device = torch.device(<span class="string">&#x27;cuda:0&#x27;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&#x27;CPU&#x27;</span>)</span><br><span class="line">vgg = vgg16()</span><br><span class="line">x = torch.rand(size=(<span class="number">4</span>, <span class="number">3</span>, <span class="number">224</span>, <span class="number">224</span>)) <span class="comment">#相当于4张224x224的图片，所以旋转out.view(insize, -1)</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">x = torch.rand(size=(3, 224, 224)) out.view(224*224*3, -1)</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line">vgg(x)</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>文献笔记</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>图像识别</tag>
      </tags>
  </entry>
  <entry>
    <title>支持向量机</title>
    <url>/posts/671832206/</url>
    <content><![CDATA[<p>介绍支持向量机基本原理</p>
<span id="more"></span>
<p>[toc]</p>
<h1 id="支持向量机-SVM"><a href="#支持向量机-SVM" class="headerlink" title="支持向量机(SVM)"></a>支持向量机(SVM)</h1><h2 id="一、什么是支持向量机"><a href="#一、什么是支持向量机" class="headerlink" title="一、什么是支持向量机"></a>一、什么是支持向量机</h2><p>支持向量机在高纬或无限维空间中构造<strong>超平面</strong>或超平面集合，其可以用于分类、回归或其他任务。直观来说，分类边界距离最近的训练资料点越远越好，因为这样可以缩小分类器的<strong>泛化误差</strong>。</p>
<blockquote>
<p><a href="https://zh.wikipedia.org/zh-cn/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA">https://zh.wikipedia.org/zh-cn/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA</a></p>
</blockquote>
<p>里面涉及到几个概念：1、超平面；2、泛化误差；3、支持向量</p>
<ul>
<li>什么是超平面呢？</li>
</ul>
<p>假如咱们是在用支持向量机来处理二分类问题吧。咱们设想假如在一个直角坐标系里面存在两个不同类别：黑点和紫点。现在我们需要将他们进行分离，你会怎么做？你或许会说：这还不简单直接画一条线不久完事了吗？看来你明白什么是<strong>超平面</strong>，好的我们看下图：</p>
<p><img data-src="/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/1678697314457.png" alt="1678697314457"></p>
<p>那么到底是蓝色好呢？还是黄色好呢？这就是我们要提的第二个概念——泛化误差</p>
<ul>
<li>什么是泛化误差</li>
</ul>
<p>仔细看图可能会绝对蓝色最佳呀，为什么呢？因为箭头标记的点他是距离最短的呀！听起来似乎很有道理，<strong>但是</strong>，我们一般实验都是取部分样本进行测试，由于训练集的局限性或噪声的因素，训练集外的样本可能比图中的训练样本更接近两个类的分隔界，这将使许多划分超平面出现错误，而黄色的超平面受影响最小。这就是我们要考虑的——泛化误差！直观理解就是：在新加入样本点的时候超平面依旧可以很好的划分数据点。</p>
<ul>
<li>什么是支持向量</li>
</ul>
<p>我们将超平面分别加减常数C这样的话我们的超平面就会发生移动但是移动多少呢？当我们移动到接触样本数据点，而这些样本数据点就决定了我们的<strong>间隔距离</strong>（卖个关子），他们就叫<strong>支持向量</strong></p>
<p>说了那么多，怎么求这个超平面方程呢？(<strong>可能会涉及许多的数学知识，想直接了解代码直接跳过这部分</strong>)</p>
<h2 id="二、数学理论"><a href="#二、数学理论" class="headerlink" title="二、数学理论"></a>二、数学理论</h2><p>机器学习算法和数学知识离不开关系，我尽可能的简化数学知识，当然如果只想要代码也可以，但是得知道如何<strong>调参</strong></p>
<blockquote>
<p>支持向量机的有效性取决于核函数、核参数和软间隔参数 <em>C</em> 的选择</p>
</blockquote>
<h3 id="2-1-数学预备知识"><a href="#2-1-数学预备知识" class="headerlink" title="2.1 数学预备知识"></a>2.1 数学预备知识</h3><ul>
<li>两直线距离</li>
</ul>
<p>$$<br>设存在两平行直线：A_0x+B_0y+b_0&#x3D;0；A_0x+B_0y+b_1&#x3D;0；则他们之间的距离为:\d&#x3D;\frac{|b_1-b_0|}{\sqrt{A_0^2+B_0^2}}<br>$$</p>
<ul>
<li>二次规划问题<br>形如：</li>
</ul>
<p>$$<br>min;;c^Tx+\frac{1}{2}x^TQx\subject;to:Dx\geqslant d\Ax&#x3D;b<br>$$</p>
<p>如果Q≥0时，那么此时就是凸优化问题</p>
<ul>
<li>拉格朗日乘子法</li>
</ul>
<p>拉格朗日乘子法是一种将约束优化问题转化为无约束优化问题的方法，我们现在要求解如下优化（最小化）问题：</p>
<p>$$<br>minf(x);;s.t.;g(x)&#x3D;0<br>$$</p>
<p>也就是求解f(x)在约束条件g(x)&#x3D;0的条件下的最小值的问题。那么我们可以引入拉格朗日函数：</p>
<p>$$<br>L(x,\alpha)&#x3D;f(x)+\alpha g(x);;; \alpha 记作拉格朗日乘子<br>$$</p>
<blockquote>
<p><a href="https://zhuanlan.zhihu.com/p/440297403">形象理解拉格朗日乘子法 - 知乎 (zhihu.com)</a></p>
</blockquote>
<ul>
<li>KKT约束条件</li>
</ul>
<blockquote>
<p><a href="https://zhuanlan.zhihu.com/p/38163970">Karush-Kuhn-Tucker (KKT)条件 - 知乎 (zhihu.com)</a></p>
</blockquote>
<h3 id="2-2-线性可分支持向量机"><a href="#2-2-线性可分支持向量机" class="headerlink" title="2.2 线性可分支持向量机"></a>2.2 线性可分支持向量机</h3><ul>
<li>什么是线性可分？</li>
</ul>
<p>不说概念，直观理解就是：咱们可以直接通过一条直线或者一个平面去划分数据集，那这就叫线性可分支持向量机！比如说：咱们有训练数据集：$T&#x3D;[(x_1,y_1),(x_2,y_2)]$现在我们需要找到一条直线划分他们，我想你会立马想到$ax+by+c&#x3D;0$这个方程。很好那么我们推广到$n$个点的情况。</p>
<p>给定训练数据集:</p>
<p>$$<br>T&#x3D;[(x_1,y_1)……(x_n,y_n)]<br>$$</p>
<p>这种情况下怎么办呢？同样的方法我们假设超平面方程为：</p>
<p>$$<br>w^Tx+b&#x3D;0\其中w，b都是模型参数,注意此时w就不是一个数字而是一个向量！<br>$$</p>
<p>那么的话咱们方程也有了，接下来问题就转化成求解$w,b$参数具体的值的问题了。再次回顾我们2元的点到直线距离公式：$d&#x3D;\frac{|ax+by+c|}{\sqrt{a^2+b^2}}$同样的道理我们推到到多维情况：</p>
<p>$$<br>d&#x3D;\frac{|w^Tx+b|}{||w||}<br>$$</p>
<p>不要忘记我们的出发点是什么：二分类！也就是说我们要将我们的点划分开来，这样的话是不是在超平面的某一侧是a类另外一侧则是b类，还记得我们前面提到的<strong>间隔距离</strong>吗？我们设：</p>
<p>$$<br>\begin{cases}<br>    w^Tx_i+b\geqslant1,y_i&#x3D;1\<br>    w^Tx_i+b\leqslant1,y_i&#x3D;-1<br>\end{cases}\\text{这样的话两个超平面的距离就是}r&#x3D;\frac{2}{||w||}<br>$$</p>
<blockquote>
<p>超平面距离就可以类比两平行直线之间距离</p>
</blockquote>
<p>这样的话我们就是找到最大“间隔”也就是说计算：</p>
<p>$$<br>max\frac{2}{||w||}简化计算记作min\frac{1}{2}||w||^2\<br>subject;to;y_i(w^Tx_i+b)\geqslant1,;i&#x3D;1,2….n<br>$$</p>
<p>利用拉格朗日乘子法进行求解：</p>
<p>$$<br>公式1;;L(w,b,\alpha)&#x3D;\frac{1}{2}||w||^2+\sum_{i&#x3D;1}^{m}\alpha_i(1-y_i(w^Tx_i+b))\||w||^2&#x3D;w^Tw<br>$$</p>
<p>此时我们对w，b分别计算偏导并且令其为0得到:</p>
<p>$$<br>公式2;;w&#x3D;\sum_{i&#x3D;1}^{m}\alpha_iy_ix_i;;\sum_{i&#x3D;1}^{m}\alpha_iy_i&#x3D;0<br>$$</p>
<p>将公式2代入公式1得到对偶问题：</p>
<p>$$<br>公式3;;max\sum_{i&#x3D;1}^{m}\alpha_i-\frac{1}{2}\sum_{i&#x3D;1}^{m}\sum_{j&#x3D;1}^{m}\alpha_iy_ix^T_i\alpha_jy_jx_j<br>$$</p>
<blockquote>
<p>计算步骤如下：</p>
<p><img data-src="/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/1678763077705.png" alt="1678763077705"></p>
</blockquote>
<p>上述过程需要满足KKT条件：</p>
<p>$$<br>\begin{cases}<br>    \alpha_i\ge0\y_if(x_i)-1\ge0\\alpha_i(y_if(x_i)-1)&#x3D;0<br>\end{cases}<br>$$</p>
<h3 id="2-3-核函数"><a href="#2-3-核函数" class="headerlink" title="2.3 核函数"></a>2.3 核函数</h3><p>前面我们讲诉了线性可分，那么可能就会有人有疑问：那么会不会存在线性不可分的问题呢？我们先看下图：</p>
<p><img data-src="/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/1678789201324.png" alt="1678789201324"></p>
<p>同样都是黑点紫点但是无论你如何画直线都不可能找到这么一条直线能够完美的将两者进行区分，那么是不是支持向量机就不起作用了？但是，从图很容易看到我们可以绘制一个圆圈将两者分开，也就是说存在一种方法将他们划分开来。但是我们知道支持向量机算法是找到一个<strong>超平面</strong>去划分数据集合，显然圆是不符合我们要求的，那怎么办呢？就是我么今天要讲的主角——核函数</p>
<blockquote>
<p><em><strong>核函数</strong></em>：将样本点从低纬映射到高维的特征空间。</p>
</blockquote>
<p><img data-src="/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/1678789607922.png" alt="1678789607922"></p>
<p>从上图很容易知道开始在2维空间我们找不到一条直线划分，但是映射到3维空间时，此时就可以找到一个超平面把他们划分开来。那么此时我们线性可分模型就转化成：</p>
<p>$$<br>f(x)&#x3D;w^T\phi(x)+b;;\phi:代表x的映射<br>$$</p>
<p>对偶问题转化为：</p>
<p>$$<br>max\sum_{i&#x3D;1}^{m}\alpha_i-\frac{1}{2}\sum_{i&#x3D;1}^{m}\sum_{j&#x3D;1}^{m}\alpha_iy_i\phi(x_i)^T\alpha_jy_j\phi(x_j)\\text{我们令}k&#x3D;\phi(x_i)^T\phi(x_j)即我们的核函数<br>$$</p>
<p>几种常用的核函数：</p>
<p><img data-src="/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/1678790610325.png" alt="1678790610325"></p>
<h3 id="2-4-软间隔、正则化"><a href="#2-4-软间隔、正则化" class="headerlink" title="2.4 软间隔、正则化"></a>2.4 软间隔、正则化</h3><p>什么？间隔还有软硬之分？此软硬非我们所认为的软硬！我们前面所设想的都是：我们绘制的超平面可以<strong>完全</strong>将我们的数据点进行划分，但是会不会存在这么一种情况——无论你如何绘制超平面总是会有部分点会错分呢？见下图：</p>
<p><img data-src="/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%5C1679056981202.png" alt="1679056981202"></p>
<p>总是有小黑跑到小紫这边，同时也总是有小紫跑到小黑这边。那么软间隔就是<strong>允许支持向量机在一些样本上出错</strong></p>
<h2 id="三、Python代码"><a href="#三、Python代码" class="headerlink" title="三、Python代码"></a>三、Python代码</h2><h2 id="四、评价"><a href="#四、评价" class="headerlink" title="四、评价"></a>四、评价</h2><p><strong>优点</strong></p>
<ul>
<li>可以解决高维问题，即大型特征空间；</li>
<li>解决小样本下机器学习问题；</li>
<li>能够处理非线性特征的相互作用；</li>
<li>无局部极小值问题；（相对于神经网络等算法）</li>
<li>无需依赖整个数据；</li>
<li>泛化能力比较强；</li>
</ul>
<p><strong>缺点</strong></p>
<ul>
<li>当观测样本很多时，效率并不是很高；</li>
<li>对非线性问题没有通用解决方案，有时候很难找到一个合适的核函数；</li>
<li>对于核函数的高维映射解释力不强，尤其是径向基函数；</li>
<li>常规SVM只支持二分类；</li>
<li><strong>对缺失数据敏感；</strong></li>
</ul>
<h2 id="五、算法流程"><a href="#五、算法流程" class="headerlink" title="五、算法流程"></a>五、算法流程</h2><h2 id="七、参考"><a href="#七、参考" class="headerlink" title="七、参考"></a>七、参考</h2><blockquote>
<p><a href="https://zhuanlan.zhihu.com/p/440297403">https://zhuanlan.zhihu.com/p/440297403</a></p>
<p>李航《统计学学习方法(第二版)》</p>
<p>《西瓜书》</p>
</blockquote>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>支持向量机</tag>
      </tags>
  </entry>
  <entry>
    <title>数据预处理（特征选择和数据降维度）原理以及python实现</title>
    <url>/posts/e92b12e5/</url>
    <content><![CDATA[<center>数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限而已</center>

<p>本文主要内容包括如下几点：</p>
<p>一、特征选择算法</p>
<p>具体介绍了：传统的特征选择算法（Wrapper methods、Filter methods、Embedded methods），以及：Laplacian Score拉普拉斯分数、Fisher Score等</p>
<p>二、数据降维算法</p>
<span id="more"></span>

<h1 id="数据预处理（🐕特征选择和🐉数据降维度）原理以及python实现"><a href="#数据预处理（🐕特征选择和🐉数据降维度）原理以及python实现" class="headerlink" title="数据预处理（🐕特征选择和🐉数据降维度）原理以及python实现"></a>数据预处理（🐕特征选择和🐉数据降维度）原理以及python实现</h1><p>[toc]</p>
<h2 id="一、特征选择（feature-selection）"><a href="#一、特征选择（feature-selection）" class="headerlink" title="一、特征选择（feature selection）"></a>一、特征选择（feature selection）</h2><p><strong>为什么要进行特征选择？</strong></p>
<pre class="mermaid">graph LR
B[input]-->A[ML]-->C[output]</pre>



<p>观察上述流程图，假设$input$的数据特征为：{$t_1…t_n$}我们直接将输入投入到$ML$里面而后得到输出。但是问题来了，我们的特征$t_1…t_n$对于我们的$output$都有作用吗？</p>
<blockquote>
<p>例子：定义我们的我们的$ML$为：男孩女孩判断模型，$input$的特征有：长&#x2F;短头发、有&#x2F;无喉结、邻居孩子男&#x2F;女等，$output$为：男孩 or 女孩。</p>
<p>我相信很容易判断特征：<strong>邻居孩子男&#x2F;女</strong>在对于我们模型效果上是不起作用的，那么的话在开始$input$时不添加该特征。</p>
</blockquote>
<p>回归正题。<strong>特征选择（feature selection）</strong>作为一种数据预处理策略，在各种数据挖掘和机器学习问题准备数据（尤其是高维数据）方面是有效且高效的，<strong>特征选择</strong>的目标包括构建更简单、更易于理解的模型、提高数据挖掘性能以及准备干净、可理解的数据<a href="https://doi.org/10.1145/3136625">^1</a>。</p>
<p>那么常用特征选择算法是什么呢？</p>
<hr>
<h2 id="1-1-传统特征选择算法"><a href="#1-1-传统特征选择算法" class="headerlink" title="1.1 传统特征选择算法"></a>1.1 传统特征选择算法</h2><h3 id="1-1-1-Wrapper-method（包装法）"><a href="#1-1-1-Wrapper-method（包装法）" class="headerlink" title="1.1.1 Wrapper method（包装法）"></a>1.1.1 Wrapper method（包装法）</h3><blockquote>
<p>Wrapper methods rely on the predictive performance of a predefined learning algorithm to evaluate the quality of selected features.</p>
<p>wrapper方法依赖<strong>预先定义的学习算法的预测性能</strong>来评估所挑选特征的质量</p>
</blockquote>
<p><strong>算法步骤</strong>：</p>
<pre class="mermaid">graph LR
A(挑选特征子集)-->B(评估挑选的特征)-->A</pre>
<blockquote>
<p>挑选特征子集，评估挑选特征直到合适为止</p>
</blockquote>
<p>那么在Wrapper methods中对于特征子集的挑选就显得格外重要了，但是如若特征搜索空间为$d$那么就行搜索次数为$2^d$，也就是说使用Wrapper methods很容易陷入NP-hard problem。两类参见的特征筛选方法：</p>
<blockquote>
<p>定义<strong>全部特征</strong>为：$Y&#x3D;{y_1…y_d}$，定义<strong>输出特征</strong>：$X_k&#x3D;{x_j|j&#x3D;1,2…k;x_j\in Y}$</p>
</blockquote>
<h4 id="1、Sequential-selection-algorithms（SFS）-3"><a href="#1、Sequential-selection-algorithms（SFS）-3" class="headerlink" title="1、Sequential selection algorithms（SFS）^3"></a>1、<strong>Sequential selection algorithms</strong>（SFS）<a href="https://rasbt.github.io/mlxtend/user_guide/feature_selection/SequentialFeatureSelector/">^3</a></h4><ul>
<li>1、SFS</li>
</ul>
<p>$$<br>x^{+}&#x3D;arg\ max\ J(X_k+x),\ x\in Y- X_k\<br>X_k+1&#x3D;X_k+x^+\<br>k&#x3D;k+1<br>$$<br>在空的特征子集中添加特征$x$使得目标函数$J$最大</p>
<ul>
<li>2、Sequential Backward Selection（SBS）</li>
</ul>
<p>$$<br>x^-&#x3D;arg\ max\ J(X_k-x),\ x\in X_k\<br>X_{k-1}&#x3D;X_k-x^-\<br>k&#x3D;k-1\<br>$$</p>
<p>与SFS相类似，只不过SBS是从完整的特征开始，而后逐渐减少特征。 </p>
<ul>
<li>3、Sequential Floating Forward Selection （SFFS）</li>
</ul>
<p>2、<strong>Heuristic Search Algorithms</strong></p>
<p>算法步骤：首先初始化：$X_0&#x3D;\phi$，$k&#x3D;0$</p>
<p>step1:<br>$$<br>x^{+}&#x3D;arg\ max\ J(X_k+x),\ x\in Y- X_k\<br>X_k+1&#x3D;X_k+x^+\<br>k&#x3D;k+1\<br>go \ to \ step2<br>$$<br>在空的特征子集中添加特征$x$使得目标函数$J$最大</p>
<p>step2:<br>$$<br>x^-&#x3D;arg\ max\ J(X_k-x),\ x\in X_k\<br>if\ J(X_k-x)&gt;J(X_k)\<br>X_{k-1}&#x3D;X_k-x^-\<br>k&#x3D;k-1\<br>go\ to\ step1<br>$$</p>
<p>SFFS较SFS多一个回溯操作（step2）</p>
<p>循环step1和step2只到达到目标$k$值停止。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">!pip install mlxtend </span><br><span class="line"><span class="keyword">from</span> mlxtend.feature_selection <span class="keyword">import</span> SequentialFeatureSelector <span class="keyword">as</span> SFS</span><br><span class="line">sfs = SFS(knn, <span class="comment">#选择模型</span></span><br><span class="line">          k_features=<span class="number">3</span>, <span class="comment">#选择筛选出的特征 </span></span><br><span class="line">          forward=<span class="literal">True</span>, </span><br><span class="line">          floating=<span class="literal">False</span>, </span><br><span class="line">          scoring=<span class="string">&#x27;accuracy&#x27;</span>, <span class="comment">#评价标准</span></span><br><span class="line">          cv=<span class="number">4</span>, <span class="comment">#交叉验证</span></span><br><span class="line">          n_jobs=-<span class="number">1</span> <span class="comment">#用于并行评估不同特征子集的CPU数量，-1代表全部)</span></span><br><span class="line">sfs = sfs.fit(X, y)</span><br><span class="line"><span class="built_in">print</span>(sbfs.k_feature_idx_) <span class="comment">#返回选择的特征的序号 k_feature_names_返回特征名称</span></span><br><span class="line"><span class="built_in">print</span>(sbfs.k_score_) <span class="comment">#返回评分</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">forward与floating参数选择：</span></span><br><span class="line"><span class="string">1、SFS算法True False</span></span><br><span class="line"><span class="string">2、SBS算法False False</span></span><br><span class="line"><span class="string">3、SFFS算法 True True</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>

<h4 id="2、Heuristic-search-algorithms-启发式搜索算法"><a href="#2、Heuristic-search-algorithms-启发式搜索算法" class="headerlink" title="2、Heuristic search algorithms 启发式搜索算法"></a>2、Heuristic search algorithms 启发式搜索算法</h4><p>Genetic Algorithm（GA）遗传算法</p>
<h4 id="Wrapper-methods缺点"><a href="#Wrapper-methods缺点" class="headerlink" title="Wrapper methods缺点"></a>Wrapper methods缺点</h4><p>1、计算代价高</p>
<p>2、容易过拟合</p>
<h3 id="1-1-2-Filter-methods（过滤法）"><a href="#1-1-2-Filter-methods（过滤法）" class="headerlink" title="1.1.2 Filter methods（过滤法）"></a>1.1.2 Filter methods（过滤法）</h3><blockquote>
<p>Filter methods use variable ranking techniques as the principle criteria for variable selection by ordering</p>
<p>Filter methods使用变量<strong>排序方法</strong>作为按顺序选择变量的原则标准</p>
</blockquote>
<p>也就是说Fileter methods首先通过<em>排序</em>得到特征的相关性，而后进行筛选<a href="%5B10.1016/j.compeleceng.2013.11.024%5D(https://doi.org/10.1016/j.compeleceng.2013.11.024)">^2</a>  !</p>
<p><strong>算法步骤</strong>：</p>
<pre class="mermaid">graph LR
A[特征重要性进行排序]-->B[将低特征滤出]</pre>

<p><strong>排序方法（ranking methods）</strong></p>
<p>特征重要性计算有：</p>
<p>1、方差分析（ANOVA）：基于统计方法来计算特征与目标变量之间的相关程度，通过计算特征的F值来评估特征的重要性。</p>
<p><strong>2、互信息（Mutual Information）</strong>：计算特征与目标变量之间的<strong>信息增益</strong>或互信息，衡量特征与目标变量之间的依赖关系。</p>
<blockquote>
<p>如：通过信息增益进行评价</p>
<p>首先计算香农熵：<br>$$<br>H(Y)&#x3D;-\sum p(y)log(p(y))<br>$$</p>
<p>条件熵计算：<br>$$<br>H(Y|X)&#x3D;-\sum_{x} \sum_{y}p(x,y)log(p(y|x))<br>$$</p>
<p>而后计算：<br>$$<br>I(Y,X)&#x3D;H(Y)-H(Y|X)<br>$$<br>通过计算（3）就可以得到：如果$X$与$Y$之间相互独立那么将会趋近0</p>
</blockquote>
<p><strong>3、相关系数（Correlation Coefficient）</strong>：计算特征和目标变量之间的线性相关性，常用的方法有Pearson相关系数和Spearman相关系数。</p>
<blockquote>
<p>$$<br>R(i)&#x3D;\frac{cov(x_i,Y)}{\sqrt{var(x_i)*var(Y)}}<br>$$</p>
<p>其中$Y$作为输出变量（output）而$x_i$则为我们的input</p>
</blockquote>
<p>4、卡方检验（Chi-square Test）：针对分类问题，对特征和目标变量之间的关联程度进行检验，判断特征的重要性。</p>
<p>5、方差选择（Variance Threshold）：通过计算特征的方差来判断其重要性，方差较低的特征可能对目标变量的预测作用较小</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> VarianceThreshold</span><br><span class="line">X = [[<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>], [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>], [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>]]</span><br><span class="line">sel = VarianceThreshold(threshold=(<span class="number">.8</span> * (<span class="number">1</span> - <span class="number">.8</span>))) <span class="comment">#threshold指定特征选择的方法</span></span><br><span class="line">sel.fit_transform(X)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">array([[0, 1],</span></span><br><span class="line"><span class="string">       [1, 0],</span></span><br><span class="line"><span class="string">       [0, 0],</span></span><br><span class="line"><span class="string">       [1, 1],</span></span><br><span class="line"><span class="string">       [1, 0],</span></span><br><span class="line"><span class="string">       [1, 1]])</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span><span class="string">&quot;</span></span><br></pre></td></tr></table></figure>

<blockquote>
<p>参数说明：</p>
<p><a href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.VarianceThreshold.html#sklearn.feature_selection.VarianceThreshold">sklearn-filter methods</a></p>
</blockquote>
<h3 id="1-1-3-Embedded-methods（嵌入法）"><a href="#1-1-3-Embedded-methods（嵌入法）" class="headerlink" title="1.1.3 Embedded methods（嵌入法）"></a>1.1.3 Embedded methods（嵌入法）</h3><blockquote>
<p>Embedded methods is a tradeoff between filter and wrapper methods that embed the feature selection into model learning.</p>
<p>Embedded方法将Wrapped和filter方法进行权衡，将特征选择嵌入到模型学习中去</p>
</blockquote>
<h2 id="1-2-其它特征选择方法"><a href="#1-2-其它特征选择方法" class="headerlink" title="1.2 其它特征选择方法"></a>1.2 其它特征选择方法</h2><h3 id="1-2-1-Laplacian-Score-拉普拉斯分数-5"><a href="#1-2-1-Laplacian-Score-拉普拉斯分数-5" class="headerlink" title="1.2.1 Laplacian Score 拉普拉斯分数^5"></a>1.2.1 Laplacian Score 拉普拉斯分数<a href="https://proceedings.neurips.cc/paper_files/paper/2005/file/b5b03f06271f8917685d14cea7c6c50a-Paper.pdf">^5</a></h3><blockquote>
<p>HE X, CAI D, NIYOGI P. Laplacian Score for Feature Selection[C&#x2F;OL]&#x2F;&#x2F;Advances in Neural Information Processing Systems: 卷 18. MIT Press, 2005[2023-07-07].</p>
</blockquote>
<p><strong>原文描述如下</strong>：</p>
<img data-src="https://s2.loli.net/2023/07/07/PbUp2IOas8mFKdJ.png" alt="拉普拉斯计算步骤" style="zoom:80%;">

<p><strong>解释如下</strong>：</p>
<blockquote>
<p>$f_{ri}$代表第$r$个特征的第$i$个样本；</p>
</blockquote>
<ol>
<li><p><strong>构建图(Graph)<strong>：首先，需要构建一个图$G$，其中节点（$m$个节点）代表样本数据的特征，边代表特征之间的相似度或关联关系。常用的方法是使用</strong>K近邻</strong>来构建图，即对每个特征找到K个最近的邻居，并将它们连接起来。</p>
</li>
<li><p>计算<strong>亲和矩阵(Affinity Matrix)<strong>：在构建了图之后，需要计算特征之间的相似度或关联性。</strong>节点</strong>之间彼此连接时通过使用：$S(i,j)&#x3D;e^{-\frac{||x_i-x_j||^2}{t}}$，其中$x_i$为$x_j$的$p$个最相邻的节点，$t$为常数。如果节点之间不相互连接那么$S(i,j)&#x3D;0$</p>
<blockquote>
<p>$S_{ij}$较大，说明节点比较“接近”</p>
</blockquote>
</li>
<li><p>计算<strong>拉普拉斯矩阵(Laplacian Matrix)<strong>：首先定义</strong>对角矩阵D（diagonal matrix D</strong>）:$D(i,j)&#x3D;\sum_{j&#x3D;1}^{n}S(i,j)$。那么拉普拉斯矩阵为：$L&#x3D;D-S$。</p>
</li>
<li><p>计算每个特征的<strong>拉普拉斯分数（Laplacian Score）</strong>：</p>
</li>
</ol>
<p>$$<br>L_r&#x3D;\frac{\widetilde{f}<em>{r}^{T}L\widetilde{f_r}}{\widetilde{f}</em>{r}^{T}D\widetilde{f}_r}<br>$$</p>
<p>其中：$\widetilde{f_r}&#x3D;f_r-\frac{f_r^TD1}{1^TD1}1$、$1&#x3D;[1,…,1]^T$、$f_r&#x3D;[f_{r1},…,f_{rm}]^T$、$D&#x3D;diag(S1)$</p>
<ol start="5">
<li>选择k个特征的任务可以通过选择具有<strong>最小拉普拉斯分数</strong>的前k个特征来解决</li>
</ol>
<p><strong>代码如下</strong><a href="https://github.com/jundongl/scikit-feature">^6</a>：</p>
<details><ummary>详细代码
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> scipy.sparse <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics.pairwise <span class="keyword">import</span> pairwise_distances</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">lap_score</span>(<span class="params">X, **kwargs</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    This function implements the laplacian score feature selection, steps are as follows:</span></span><br><span class="line"><span class="string">    1. Construct the affinity matrix W if it is not specified</span></span><br><span class="line"><span class="string">    2. For the r-th feature, we define fr = X(:,r), D = diag(W*ones), ones = [1,...,1]&#x27;, L = D - W</span></span><br><span class="line"><span class="string">    3. Let fr_hat = fr - (fr&#x27;*D*ones)*ones/(ones&#x27;*D*ones)</span></span><br><span class="line"><span class="string">    4. Laplacian score for the r-th feature is score = (fr_hat&#x27;*L*fr_hat)/(fr_hat&#x27;*D*fr_hat)</span></span><br><span class="line"><span class="string">    Input</span></span><br><span class="line"><span class="string">    -----</span></span><br><span class="line"><span class="string">    X: &#123;numpy array&#125;, shape (n_samples, n_features)</span></span><br><span class="line"><span class="string">        input data</span></span><br><span class="line"><span class="string">    kwargs: &#123;dictionary&#125;</span></span><br><span class="line"><span class="string">        W: &#123;sparse matrix&#125;, shape (n_samples, n_samples)</span></span><br><span class="line"><span class="string">            input affinity matrix</span></span><br><span class="line"><span class="string">    Output</span></span><br><span class="line"><span class="string">    ------</span></span><br><span class="line"><span class="string">    score: &#123;numpy array&#125;, shape (n_features,)</span></span><br><span class="line"><span class="string">        laplacian score for each feature</span></span><br><span class="line"><span class="string">    Reference</span></span><br><span class="line"><span class="string">    ---------</span></span><br><span class="line"><span class="string">    He, Xiaofei et al. &quot;Laplacian Score for Feature Selection.&quot; NIPS 2005.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># if &#x27;W&#x27; is not specified, use the default W</span></span><br><span class="line">    <span class="keyword">if</span> <span class="string">&#x27;W&#x27;</span> <span class="keyword">not</span> <span class="keyword">in</span> kwargs.keys():</span><br><span class="line">        W = construct_W(X)</span><br><span class="line">    <span class="comment"># construct the affinity matrix W</span></span><br><span class="line">    W = kwargs[<span class="string">&#x27;W&#x27;</span>]</span><br><span class="line">    <span class="comment"># build the diagonal D matrix from affinity matrix W</span></span><br><span class="line">    D = np.array(W.<span class="built_in">sum</span>(axis=<span class="number">1</span>))</span><br><span class="line">    L = W</span><br><span class="line">    tmp = np.dot(np.transpose(D), X)</span><br><span class="line">    D = diags(np.transpose(D), [<span class="number">0</span>])</span><br><span class="line">    Xt = np.transpose(X)</span><br><span class="line">    t1 = np.transpose(np.dot(Xt, D.todense()))</span><br><span class="line">    t2 = np.transpose(np.dot(Xt, L.todense()))</span><br><span class="line">    <span class="comment"># compute the numerator of Lr</span></span><br><span class="line">    D_prime = np.<span class="built_in">sum</span>(np.multiply(t1, X), <span class="number">0</span>) - np.multiply(tmp, tmp)/D.<span class="built_in">sum</span>()</span><br><span class="line">    <span class="comment"># compute the denominator of Lr</span></span><br><span class="line">    L_prime = np.<span class="built_in">sum</span>(np.multiply(t2, X), <span class="number">0</span>) - np.multiply(tmp, tmp)/D.<span class="built_in">sum</span>()</span><br><span class="line">    <span class="comment"># avoid the denominator of Lr to be 0</span></span><br><span class="line">    D_prime[D_prime &lt; <span class="number">1e-12</span>] = <span class="number">10000</span></span><br><span class="line">    <span class="comment"># compute laplacian score for all features</span></span><br><span class="line">    score = <span class="number">1</span> - np.array(np.multiply(L_prime, <span class="number">1</span>/D_prime))[<span class="number">0</span>, :]</span><br><span class="line">    <span class="keyword">return</span> np.transpose(score)</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">feature_ranking</span>(<span class="params">score</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Rank features in ascending order according to their laplacian scores, the smaller the laplacian score is, the more</span></span><br><span class="line"><span class="string">    important the feature is</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    idx = np.argsort(score, <span class="number">0</span>)</span><br><span class="line">    <span class="keyword">return</span> idx</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">construct_W</span>(<span class="params">X, **kwargs</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Construct the affinity matrix W through different ways</span></span><br><span class="line"><span class="string">    Notes</span></span><br><span class="line"><span class="string">    -----</span></span><br><span class="line"><span class="string">    if kwargs is null, use the default parameter settings;</span></span><br><span class="line"><span class="string">    if kwargs is not null, construct the affinity matrix according to parameters in kwargs</span></span><br><span class="line"><span class="string">    Input</span></span><br><span class="line"><span class="string">    -----</span></span><br><span class="line"><span class="string">    X: &#123;numpy array&#125;, shape (n_samples, n_features)</span></span><br><span class="line"><span class="string">        input data</span></span><br><span class="line"><span class="string">    kwargs: &#123;dictionary&#125;</span></span><br><span class="line"><span class="string">        parameters to construct different affinity matrix W:</span></span><br><span class="line"><span class="string">        y: &#123;numpy array&#125;, shape (n_samples, 1)</span></span><br><span class="line"><span class="string">            the true label information needed under the &#x27;supervised&#x27; neighbor mode</span></span><br><span class="line"><span class="string">        metric: &#123;string&#125;</span></span><br><span class="line"><span class="string">            choices for different distance measures</span></span><br><span class="line"><span class="string">            &#x27;euclidean&#x27; - use euclidean distance</span></span><br><span class="line"><span class="string">            &#x27;cosine&#x27; - use cosine distance (default)</span></span><br><span class="line"><span class="string">        neighbor_mode: &#123;string&#125;</span></span><br><span class="line"><span class="string">            indicates how to construct the graph</span></span><br><span class="line"><span class="string">            &#x27;knn&#x27; - put an edge between two nodes if and only if they are among the</span></span><br><span class="line"><span class="string">                    k nearest neighbors of each other (default)</span></span><br><span class="line"><span class="string">            &#x27;supervised&#x27; - put an edge between two nodes if they belong to same class</span></span><br><span class="line"><span class="string">                    and they are among the k nearest neighbors of each other</span></span><br><span class="line"><span class="string">        weight_mode: &#123;string&#125;</span></span><br><span class="line"><span class="string">            indicates how to assign weights for each edge in the graph</span></span><br><span class="line"><span class="string">            &#x27;binary&#x27; - 0-1 weighting, every edge receives weight of 1 (default)</span></span><br><span class="line"><span class="string">            &#x27;heat_kernel&#x27; - if nodes i and j are connected, put weight W_ij = exp(-norm(x_i - x_j)/2t^2)</span></span><br><span class="line"><span class="string">                            this weight mode can only be used under &#x27;euclidean&#x27; metric and you are required</span></span><br><span class="line"><span class="string">                            to provide the parameter t</span></span><br><span class="line"><span class="string">            &#x27;cosine&#x27; - if nodes i and j are connected, put weight cosine(x_i,x_j).</span></span><br><span class="line"><span class="string">                        this weight mode can only be used under &#x27;cosine&#x27; metric</span></span><br><span class="line"><span class="string">        k: &#123;int&#125;</span></span><br><span class="line"><span class="string">            choices for the number of neighbors (default k = 5)</span></span><br><span class="line"><span class="string">        t: &#123;float&#125;</span></span><br><span class="line"><span class="string">            parameter for the &#x27;heat_kernel&#x27; weight_mode</span></span><br><span class="line"><span class="string">        fisher_score: &#123;boolean&#125;</span></span><br><span class="line"><span class="string">            indicates whether to build the affinity matrix in a fisher score way, in which W_ij = 1/n_l if yi = yj = l;</span></span><br><span class="line"><span class="string">            otherwise W_ij = 0 (default fisher_score = false)</span></span><br><span class="line"><span class="string">        reliefF: &#123;boolean&#125;</span></span><br><span class="line"><span class="string">            indicates whether to build the affinity matrix in a reliefF way, NH(x) and NM(x,y) denotes a set of</span></span><br><span class="line"><span class="string">            k nearest points to x with the same class as x, and a different class (the class y), respectively.</span></span><br><span class="line"><span class="string">            W_ij = 1 if i = j; W_ij = 1/k if x_j \in NH(x_i); W_ij = -1/(c-1)k if x_j \in NM(x_i, y) (default reliefF = false)</span></span><br><span class="line"><span class="string">    Output</span></span><br><span class="line"><span class="string">    ------</span></span><br><span class="line"><span class="string">    W: &#123;sparse matrix&#125;, shape (n_samples, n_samples)</span></span><br><span class="line"><span class="string">        output affinity matrix W</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># default metric is &#x27;cosine&#x27;</span></span><br><span class="line">    <span class="keyword">if</span> <span class="string">&#x27;metric&#x27;</span> <span class="keyword">not</span> <span class="keyword">in</span> kwargs.keys():</span><br><span class="line">        kwargs[<span class="string">&#x27;metric&#x27;</span>] = <span class="string">&#x27;cosine&#x27;</span></span><br><span class="line">    <span class="comment"># default neighbor mode is &#x27;knn&#x27; and default neighbor size is 5</span></span><br><span class="line">    <span class="keyword">if</span> <span class="string">&#x27;neighbor_mode&#x27;</span> <span class="keyword">not</span> <span class="keyword">in</span> kwargs.keys():</span><br><span class="line">        kwargs[<span class="string">&#x27;neighbor_mode&#x27;</span>] = <span class="string">&#x27;knn&#x27;</span></span><br><span class="line">    <span class="keyword">if</span> kwargs[<span class="string">&#x27;neighbor_mode&#x27;</span>] == <span class="string">&#x27;knn&#x27;</span> <span class="keyword">and</span> <span class="string">&#x27;k&#x27;</span> <span class="keyword">not</span> <span class="keyword">in</span> kwargs.keys():</span><br><span class="line">        kwargs[<span class="string">&#x27;k&#x27;</span>] = <span class="number">5</span></span><br><span class="line">    <span class="keyword">if</span> kwargs[<span class="string">&#x27;neighbor_mode&#x27;</span>] == <span class="string">&#x27;supervised&#x27;</span> <span class="keyword">and</span> <span class="string">&#x27;k&#x27;</span> <span class="keyword">not</span> <span class="keyword">in</span> kwargs.keys():</span><br><span class="line">        kwargs[<span class="string">&#x27;k&#x27;</span>] = <span class="number">5</span></span><br><span class="line">    <span class="keyword">if</span> kwargs[<span class="string">&#x27;neighbor_mode&#x27;</span>] == <span class="string">&#x27;supervised&#x27;</span> <span class="keyword">and</span> <span class="string">&#x27;y&#x27;</span> <span class="keyword">not</span> <span class="keyword">in</span> kwargs.keys():</span><br><span class="line">        <span class="built_in">print</span> (<span class="string">&#x27;Warning: label is required in the supervised neighborMode!!!&#x27;</span>)</span><br><span class="line">        exit(<span class="number">0</span>)</span><br><span class="line">    <span class="comment"># default weight mode is &#x27;binary&#x27;, default t in heat kernel mode is 1</span></span><br><span class="line">    <span class="keyword">if</span> <span class="string">&#x27;weight_mode&#x27;</span> <span class="keyword">not</span> <span class="keyword">in</span> kwargs.keys():</span><br><span class="line">        kwargs[<span class="string">&#x27;weight_mode&#x27;</span>] = <span class="string">&#x27;binary&#x27;</span></span><br><span class="line">    <span class="keyword">if</span> kwargs[<span class="string">&#x27;weight_mode&#x27;</span>] == <span class="string">&#x27;heat_kernel&#x27;</span>:</span><br><span class="line">        <span class="keyword">if</span> kwargs[<span class="string">&#x27;metric&#x27;</span>] != <span class="string">&#x27;euclidean&#x27;</span>:</span><br><span class="line">            kwargs[<span class="string">&#x27;metric&#x27;</span>] = <span class="string">&#x27;euclidean&#x27;</span></span><br><span class="line">        <span class="keyword">if</span> <span class="string">&#x27;t&#x27;</span> <span class="keyword">not</span> <span class="keyword">in</span> kwargs.keys():</span><br><span class="line">            kwargs[<span class="string">&#x27;t&#x27;</span>] = <span class="number">1</span></span><br><span class="line">    <span class="keyword">elif</span> kwargs[<span class="string">&#x27;weight_mode&#x27;</span>] == <span class="string">&#x27;cosine&#x27;</span>:</span><br><span class="line">        <span class="keyword">if</span> kwargs[<span class="string">&#x27;metric&#x27;</span>] != <span class="string">&#x27;cosine&#x27;</span>:</span><br><span class="line">            kwargs[<span class="string">&#x27;metric&#x27;</span>] = <span class="string">&#x27;cosine&#x27;</span></span><br><span class="line">    <span class="comment"># default fisher_score and reliefF mode are &#x27;false&#x27;</span></span><br><span class="line">    <span class="keyword">if</span> <span class="string">&#x27;fisher_score&#x27;</span> <span class="keyword">not</span> <span class="keyword">in</span> kwargs.keys():</span><br><span class="line">        kwargs[<span class="string">&#x27;fisher_score&#x27;</span>] = <span class="literal">False</span></span><br><span class="line">    <span class="keyword">if</span> <span class="string">&#x27;reliefF&#x27;</span> <span class="keyword">not</span> <span class="keyword">in</span> kwargs.keys():</span><br><span class="line">        kwargs[<span class="string">&#x27;reliefF&#x27;</span>] = <span class="literal">False</span></span><br><span class="line">    n_samples, n_features = np.shape(X)</span><br><span class="line">    <span class="comment"># choose &#x27;knn&#x27; neighbor mode</span></span><br><span class="line">    <span class="keyword">if</span> kwargs[<span class="string">&#x27;neighbor_mode&#x27;</span>] == <span class="string">&#x27;knn&#x27;</span>:</span><br><span class="line">        k = kwargs[<span class="string">&#x27;k&#x27;</span>]</span><br><span class="line">        <span class="keyword">if</span> kwargs[<span class="string">&#x27;weight_mode&#x27;</span>] == <span class="string">&#x27;binary&#x27;</span>:</span><br><span class="line">            <span class="keyword">if</span> kwargs[<span class="string">&#x27;metric&#x27;</span>] == <span class="string">&#x27;euclidean&#x27;</span>:</span><br><span class="line">                <span class="comment"># compute pairwise euclidean distances</span></span><br><span class="line">                D = pairwise_distances(X)</span><br><span class="line">                D **= <span class="number">2</span></span><br><span class="line">                <span class="comment"># sort the distance matrix D in ascending order</span></span><br><span class="line">                dump = np.sort(D, axis=<span class="number">1</span>)</span><br><span class="line">                idx = np.argsort(D, axis=<span class="number">1</span>)</span><br><span class="line">                <span class="comment"># choose the k-nearest neighbors for each instance</span></span><br><span class="line">                idx_new = idx[:, <span class="number">0</span>:k+<span class="number">1</span>]</span><br><span class="line">                G = np.zeros((n_samples*(k+<span class="number">1</span>), <span class="number">3</span>))</span><br><span class="line">                G[:, <span class="number">0</span>] = np.tile(np.arange(n_samples), (k+<span class="number">1</span>, <span class="number">1</span>)).reshape(-<span class="number">1</span>)</span><br><span class="line">                G[:, <span class="number">1</span>] = np.ravel(idx_new, order=<span class="string">&#x27;F&#x27;</span>)</span><br><span class="line">                G[:, <span class="number">2</span>] = <span class="number">1</span></span><br><span class="line">                <span class="comment"># build the sparse affinity matrix W</span></span><br><span class="line">                W = csc_matrix((G[:, <span class="number">2</span>], (G[:, <span class="number">0</span>], G[:, <span class="number">1</span>])), shape=(n_samples, n_samples))</span><br><span class="line">                bigger = np.transpose(W) &gt; W</span><br><span class="line">                W = W - W.multiply(bigger) + np.transpose(W).multiply(bigger)</span><br><span class="line">                <span class="keyword">return</span> W</span><br><span class="line">            <span class="keyword">elif</span> kwargs[<span class="string">&#x27;metric&#x27;</span>] == <span class="string">&#x27;cosine&#x27;</span>:</span><br><span class="line">                <span class="comment"># normalize the data first</span></span><br><span class="line">                X_normalized = np.power(np.<span class="built_in">sum</span>(X*X, axis=<span class="number">1</span>), <span class="number">0.5</span>)</span><br><span class="line">                <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n_samples):</span><br><span class="line">                    X[i, :] = X[i, :]/<span class="built_in">max</span>(<span class="number">1e-12</span>, X_normalized[i])</span><br><span class="line">                <span class="comment"># compute pairwise cosine distances</span></span><br><span class="line">                D_cosine = np.dot(X, np.transpose(X))</span><br><span class="line">                <span class="comment"># sort the distance matrix D in descending order</span></span><br><span class="line">                dump = np.sort(-D_cosine, axis=<span class="number">1</span>)</span><br><span class="line">                idx = np.argsort(-D_cosine, axis=<span class="number">1</span>)</span><br><span class="line">                idx_new = idx[:, <span class="number">0</span>:k+<span class="number">1</span>]</span><br><span class="line">                G = np.zeros((n_samples*(k+<span class="number">1</span>), <span class="number">3</span>))</span><br><span class="line">                G[:, <span class="number">0</span>] = np.tile(np.arange(n_samples), (k+<span class="number">1</span>, <span class="number">1</span>)).reshape(-<span class="number">1</span>)</span><br><span class="line">                G[:, <span class="number">1</span>] = np.ravel(idx_new, order=<span class="string">&#x27;F&#x27;</span>)</span><br><span class="line">                G[:, <span class="number">2</span>] = <span class="number">1</span></span><br><span class="line">                <span class="comment"># build the sparse affinity matrix W</span></span><br><span class="line">                W = csc_matrix((G[:, <span class="number">2</span>], (G[:, <span class="number">0</span>], G[:, <span class="number">1</span>])), shape=(n_samples, n_samples))</span><br><span class="line">                bigger = np.transpose(W) &gt; W</span><br><span class="line">                W = W - W.multiply(bigger) + np.transpose(W).multiply(bigger)</span><br><span class="line">                <span class="keyword">return</span> W</span><br><span class="line">        <span class="keyword">elif</span> kwargs[<span class="string">&#x27;weight_mode&#x27;</span>] == <span class="string">&#x27;heat_kernel&#x27;</span>:</span><br><span class="line">            t = kwargs[<span class="string">&#x27;t&#x27;</span>]</span><br><span class="line">            <span class="comment"># compute pairwise euclidean distances</span></span><br><span class="line">            D = pairwise_distances(X)</span><br><span class="line">            D **= <span class="number">2</span></span><br><span class="line">            <span class="comment"># sort the distance matrix D in ascending order</span></span><br><span class="line">            dump = np.sort(D, axis=<span class="number">1</span>)</span><br><span class="line">            idx = np.argsort(D, axis=<span class="number">1</span>)</span><br><span class="line">            idx_new = idx[:, <span class="number">0</span>:k+<span class="number">1</span>]</span><br><span class="line">            dump_new = dump[:, <span class="number">0</span>:k+<span class="number">1</span>]</span><br><span class="line">            <span class="comment"># compute the pairwise heat kernel distances</span></span><br><span class="line">            dump_heat_kernel = np.exp(-dump_new/(<span class="number">2</span>*t*t))</span><br><span class="line">            G = np.zeros((n_samples*(k+<span class="number">1</span>), <span class="number">3</span>))</span><br><span class="line">            G[:, <span class="number">0</span>] = np.tile(np.arange(n_samples), (k+<span class="number">1</span>, <span class="number">1</span>)).reshape(-<span class="number">1</span>)</span><br><span class="line">            G[:, <span class="number">1</span>] = np.ravel(idx_new, order=<span class="string">&#x27;F&#x27;</span>)</span><br><span class="line">            G[:, <span class="number">2</span>] = np.ravel(dump_heat_kernel, order=<span class="string">&#x27;F&#x27;</span>)</span><br><span class="line">            <span class="comment"># build the sparse affinity matrix W</span></span><br><span class="line">            W = csc_matrix((G[:, <span class="number">2</span>], (G[:, <span class="number">0</span>], G[:, <span class="number">1</span>])), shape=(n_samples, n_samples))</span><br><span class="line">            bigger = np.transpose(W) &gt; W</span><br><span class="line">            W = W - W.multiply(bigger) + np.transpose(W).multiply(bigger)</span><br><span class="line">            <span class="keyword">return</span> W</span><br><span class="line">        <span class="keyword">elif</span> kwargs[<span class="string">&#x27;weight_mode&#x27;</span>] == <span class="string">&#x27;cosine&#x27;</span>:</span><br><span class="line">            <span class="comment"># normalize the data first</span></span><br><span class="line">            X_normalized = np.power(np.<span class="built_in">sum</span>(X*X, axis=<span class="number">1</span>), <span class="number">0.5</span>)</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n_samples):</span><br><span class="line">                    X[i, :] = X[i, :]/<span class="built_in">max</span>(<span class="number">1e-12</span>, X_normalized[i])</span><br><span class="line">            <span class="comment"># compute pairwise cosine distances</span></span><br><span class="line">            D_cosine = np.dot(X, np.transpose(X))</span><br><span class="line">            <span class="comment"># sort the distance matrix D in ascending order</span></span><br><span class="line">            dump = np.sort(-D_cosine, axis=<span class="number">1</span>)</span><br><span class="line">            idx = np.argsort(-D_cosine, axis=<span class="number">1</span>)</span><br><span class="line">            idx_new = idx[:, <span class="number">0</span>:k+<span class="number">1</span>]</span><br><span class="line">            dump_new = -dump[:, <span class="number">0</span>:k+<span class="number">1</span>]</span><br><span class="line">            G = np.zeros((n_samples*(k+<span class="number">1</span>), <span class="number">3</span>))</span><br><span class="line">            G[:, <span class="number">0</span>] = np.tile(np.arange(n_samples), (k+<span class="number">1</span>, <span class="number">1</span>)).reshape(-<span class="number">1</span>)</span><br><span class="line">            G[:, <span class="number">1</span>] = np.ravel(idx_new, order=<span class="string">&#x27;F&#x27;</span>)</span><br><span class="line">            G[:, <span class="number">2</span>] = np.ravel(dump_new, order=<span class="string">&#x27;F&#x27;</span>)</span><br><span class="line">            <span class="comment"># build the sparse affinity matrix W</span></span><br><span class="line">            W = csc_matrix((G[:, <span class="number">2</span>], (G[:, <span class="number">0</span>], G[:, <span class="number">1</span>])), shape=(n_samples, n_samples))</span><br><span class="line">            bigger = np.transpose(W) &gt; W</span><br><span class="line">            W = W - W.multiply(bigger) + np.transpose(W).multiply(bigger)</span><br><span class="line">            <span class="keyword">return</span> W</span><br><span class="line">    <span class="comment"># choose supervised neighborMode</span></span><br><span class="line">    <span class="keyword">elif</span> kwargs[<span class="string">&#x27;neighbor_mode&#x27;</span>] == <span class="string">&#x27;supervised&#x27;</span>:</span><br><span class="line">        k = kwargs[<span class="string">&#x27;k&#x27;</span>]</span><br><span class="line">        <span class="comment"># get true labels and the number of classes</span></span><br><span class="line">        y = kwargs[<span class="string">&#x27;y&#x27;</span>]</span><br><span class="line">        label = np.unique(y)</span><br><span class="line">        n_classes = np.unique(y).size</span><br><span class="line">        <span class="comment"># construct the weight matrix W in a fisherScore way, W_ij = 1/n_l if yi = yj = l, otherwise W_ij = 0</span></span><br><span class="line">        <span class="keyword">if</span> kwargs[<span class="string">&#x27;fisher_score&#x27;</span>] <span class="keyword">is</span> <span class="literal">True</span>:</span><br><span class="line">            W = lil_matrix((n_samples, n_samples))</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n_classes):</span><br><span class="line">                class_idx = (y == label[i])</span><br><span class="line">                class_idx_all = (class_idx[:, np.newaxis] &amp; class_idx[np.newaxis, :])</span><br><span class="line">                W[class_idx_all] = <span class="number">1.0</span>/np.<span class="built_in">sum</span>(np.<span class="built_in">sum</span>(class_idx))</span><br><span class="line">            <span class="keyword">return</span> W</span><br><span class="line">        <span class="comment"># construct the weight matrix W in a reliefF way, NH(x) and NM(x,y) denotes a set of k nearest</span></span><br><span class="line">        <span class="comment"># points to x with the same class as x, a different class (the class y), respectively. W_ij = 1 if i = j;</span></span><br><span class="line">        <span class="comment"># W_ij = 1/k if x_j \in NH(x_i); W_ij = -1/(c-1)k if x_j \in NM(x_i, y)</span></span><br><span class="line">        <span class="keyword">if</span> kwargs[<span class="string">&#x27;reliefF&#x27;</span>] <span class="keyword">is</span> <span class="literal">True</span>:</span><br><span class="line">            <span class="comment"># when xj in NH(xi)</span></span><br><span class="line">            G = np.zeros((n_samples*(k+<span class="number">1</span>), <span class="number">3</span>))</span><br><span class="line">            id_now = <span class="number">0</span></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n_classes):</span><br><span class="line">                class_idx = np.column_stack(np.where(y == label[i]))[:, <span class="number">0</span>]</span><br><span class="line">                D = pairwise_distances(X[class_idx, :])</span><br><span class="line">                D **= <span class="number">2</span></span><br><span class="line">                idx = np.argsort(D, axis=<span class="number">1</span>)</span><br><span class="line">                idx_new = idx[:, <span class="number">0</span>:k+<span class="number">1</span>]</span><br><span class="line">                n_smp_class = (class_idx[idx_new[:]]).size</span><br><span class="line">                <span class="keyword">if</span> <span class="built_in">len</span>(class_idx) &lt;= k:</span><br><span class="line">                    k = <span class="built_in">len</span>(class_idx) - <span class="number">1</span></span><br><span class="line">                G[id_now:n_smp_class+id_now, <span class="number">0</span>] = np.tile(class_idx, (k+<span class="number">1</span>, <span class="number">1</span>)).reshape(-<span class="number">1</span>)</span><br><span class="line">                G[id_now:n_smp_class+id_now, <span class="number">1</span>] = np.ravel(class_idx[idx_new[:]], order=<span class="string">&#x27;F&#x27;</span>)</span><br><span class="line">                G[id_now:n_smp_class+id_now, <span class="number">2</span>] = <span class="number">1.0</span>/k</span><br><span class="line">                id_now += n_smp_class</span><br><span class="line">            W1 = csc_matrix((G[:, <span class="number">2</span>], (G[:, <span class="number">0</span>], G[:, <span class="number">1</span>])), shape=(n_samples, n_samples))</span><br><span class="line">            <span class="comment"># when i = j, W_ij = 1</span></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n_samples):</span><br><span class="line">                W1[i, i] = <span class="number">1</span></span><br><span class="line">            <span class="comment"># when x_j in NM(x_i, y)</span></span><br><span class="line">            G = np.zeros((n_samples*k*(n_classes - <span class="number">1</span>), <span class="number">3</span>))</span><br><span class="line">            id_now = <span class="number">0</span></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n_classes):</span><br><span class="line">                class_idx1 = np.column_stack(np.where(y == label[i]))[:, <span class="number">0</span>]</span><br><span class="line">                X1 = X[class_idx1, :]</span><br><span class="line">                <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(n_classes):</span><br><span class="line">                    <span class="keyword">if</span> label[j] != label[i]:</span><br><span class="line">                        class_idx2 = np.column_stack(np.where(y == label[j]))[:, <span class="number">0</span>]</span><br><span class="line">                        X2 = X[class_idx2, :]</span><br><span class="line">                        D = pairwise_distances(X1, X2)</span><br><span class="line">                        idx = np.argsort(D, axis=<span class="number">1</span>)</span><br><span class="line">                        idx_new = idx[:, <span class="number">0</span>:k]</span><br><span class="line">                        n_smp_class = <span class="built_in">len</span>(class_idx1)*k</span><br><span class="line">                        G[id_now:n_smp_class+id_now, <span class="number">0</span>] = np.tile(class_idx1, (k, <span class="number">1</span>)).reshape(-<span class="number">1</span>)</span><br><span class="line">                        G[id_now:n_smp_class+id_now, <span class="number">1</span>] = np.ravel(class_idx2[idx_new[:]], order=<span class="string">&#x27;F&#x27;</span>)</span><br><span class="line">                        G[id_now:n_smp_class+id_now, <span class="number">2</span>] = -<span class="number">1.0</span>/((n_classes-<span class="number">1</span>)*k)</span><br><span class="line">                        id_now += n_smp_class</span><br><span class="line">            W2 = csc_matrix((G[:, <span class="number">2</span>], (G[:, <span class="number">0</span>], G[:, <span class="number">1</span>])), shape=(n_samples, n_samples))</span><br><span class="line">            bigger = np.transpose(W2) &gt; W2</span><br><span class="line">            W2 = W2 - W2.multiply(bigger) + np.transpose(W2).multiply(bigger)</span><br><span class="line">            W = W1 + W2</span><br><span class="line">            <span class="keyword">return</span> W</span><br><span class="line">        <span class="keyword">if</span> kwargs[<span class="string">&#x27;weight_mode&#x27;</span>] == <span class="string">&#x27;binary&#x27;</span>:</span><br><span class="line">            <span class="keyword">if</span> kwargs[<span class="string">&#x27;metric&#x27;</span>] == <span class="string">&#x27;euclidean&#x27;</span>:</span><br><span class="line">                G = np.zeros((n_samples*(k+<span class="number">1</span>), <span class="number">3</span>))</span><br><span class="line">                id_now = <span class="number">0</span></span><br><span class="line">                <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n_classes):</span><br><span class="line">                    class_idx = np.column_stack(np.where(y == label[i]))[:, <span class="number">0</span>]</span><br><span class="line">                    <span class="comment"># compute pairwise euclidean distances for instances in class i</span></span><br><span class="line">                    D = pairwise_distances(X[class_idx, :])</span><br><span class="line">                    D **= <span class="number">2</span></span><br><span class="line">                    <span class="comment"># sort the distance matrix D in ascending order for instances in class i</span></span><br><span class="line">                    idx = np.argsort(D, axis=<span class="number">1</span>)</span><br><span class="line">                    idx_new = idx[:, <span class="number">0</span>:k+<span class="number">1</span>]</span><br><span class="line">                    n_smp_class = <span class="built_in">len</span>(class_idx)*(k+<span class="number">1</span>)</span><br><span class="line">                    G[id_now:n_smp_class+id_now, <span class="number">0</span>] = np.tile(class_idx, (k+<span class="number">1</span>, <span class="number">1</span>)).reshape(-<span class="number">1</span>)</span><br><span class="line">                    G[id_now:n_smp_class+id_now, <span class="number">1</span>] = np.ravel(class_idx[idx_new[:]], order=<span class="string">&#x27;F&#x27;</span>)</span><br><span class="line">                    G[id_now:n_smp_class+id_now, <span class="number">2</span>] = <span class="number">1</span></span><br><span class="line">                    id_now += n_smp_class</span><br><span class="line">                <span class="comment"># build the sparse affinity matrix W</span></span><br><span class="line">                W = csc_matrix((G[:, <span class="number">2</span>], (G[:, <span class="number">0</span>], G[:, <span class="number">1</span>])), shape=(n_samples, n_samples))</span><br><span class="line">                bigger = np.transpose(W) &gt; W</span><br><span class="line">                W = W - W.multiply(bigger) + np.transpose(W).multiply(bigger)</span><br><span class="line">                <span class="keyword">return</span> W</span><br><span class="line">            <span class="keyword">if</span> kwargs[<span class="string">&#x27;metric&#x27;</span>] == <span class="string">&#x27;cosine&#x27;</span>:</span><br><span class="line">                <span class="comment"># normalize the data first</span></span><br><span class="line">                X_normalized = np.power(np.<span class="built_in">sum</span>(X*X, axis=<span class="number">1</span>), <span class="number">0.5</span>)</span><br><span class="line">                <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n_samples):</span><br><span class="line">                    X[i, :] = X[i, :]/<span class="built_in">max</span>(<span class="number">1e-12</span>, X_normalized[i])</span><br><span class="line">                G = np.zeros((n_samples*(k+<span class="number">1</span>), <span class="number">3</span>))</span><br><span class="line">                id_now = <span class="number">0</span></span><br><span class="line">                <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n_classes):</span><br><span class="line">                    class_idx = np.column_stack(np.where(y == label[i]))[:, <span class="number">0</span>]</span><br><span class="line">                    <span class="comment"># compute pairwise cosine distances for instances in class i</span></span><br><span class="line">                    D_cosine = np.dot(X[class_idx, :], np.transpose(X[class_idx, :]))</span><br><span class="line">                    <span class="comment"># sort the distance matrix D in descending order for instances in class i</span></span><br><span class="line">                    idx = np.argsort(-D_cosine, axis=<span class="number">1</span>)</span><br><span class="line">                    idx_new = idx[:, <span class="number">0</span>:k+<span class="number">1</span>]</span><br><span class="line">                    n_smp_class = <span class="built_in">len</span>(class_idx)*(k+<span class="number">1</span>)</span><br><span class="line">                    G[id_now:n_smp_class+id_now, <span class="number">0</span>] = np.tile(class_idx, (k+<span class="number">1</span>, <span class="number">1</span>)).reshape(-<span class="number">1</span>)</span><br><span class="line">                    G[id_now:n_smp_class+id_now, <span class="number">1</span>] = np.ravel(class_idx[idx_new[:]], order=<span class="string">&#x27;F&#x27;</span>)</span><br><span class="line">                    G[id_now:n_smp_class+id_now, <span class="number">2</span>] = <span class="number">1</span></span><br><span class="line">                    id_now += n_smp_class</span><br><span class="line">                <span class="comment"># build the sparse affinity matrix W</span></span><br><span class="line">                W = csc_matrix((G[:, <span class="number">2</span>], (G[:, <span class="number">0</span>], G[:, <span class="number">1</span>])), shape=(n_samples, n_samples))</span><br><span class="line">                bigger = np.transpose(W) &gt; W</span><br><span class="line">                W = W - W.multiply(bigger) + np.transpose(W).multiply(bigger)</span><br><span class="line">                <span class="keyword">return</span> W</span><br><span class="line">        <span class="keyword">elif</span> kwargs[<span class="string">&#x27;weight_mode&#x27;</span>] == <span class="string">&#x27;heat_kernel&#x27;</span>:</span><br><span class="line">            G = np.zeros((n_samples*(k+<span class="number">1</span>), <span class="number">3</span>))</span><br><span class="line">            id_now = <span class="number">0</span></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n_classes):</span><br><span class="line">                class_idx = np.column_stack(np.where(y == label[i]))[:, <span class="number">0</span>]</span><br><span class="line">                <span class="comment"># compute pairwise cosine distances for instances in class i</span></span><br><span class="line">                D = pairwise_distances(X[class_idx, :])</span><br><span class="line">                D **= <span class="number">2</span></span><br><span class="line">                <span class="comment"># sort the distance matrix D in ascending order for instances in class i</span></span><br><span class="line">                dump = np.sort(D, axis=<span class="number">1</span>)</span><br><span class="line">                idx = np.argsort(D, axis=<span class="number">1</span>)</span><br><span class="line">                idx_new = idx[:, <span class="number">0</span>:k+<span class="number">1</span>]</span><br><span class="line">                dump_new = dump[:, <span class="number">0</span>:k+<span class="number">1</span>]</span><br><span class="line">                t = kwargs[<span class="string">&#x27;t&#x27;</span>]</span><br><span class="line">                <span class="comment"># compute pairwise heat kernel distances for instances in class i</span></span><br><span class="line">                dump_heat_kernel = np.exp(-dump_new/(<span class="number">2</span>*t*t))</span><br><span class="line">                n_smp_class = <span class="built_in">len</span>(class_idx)*(k+<span class="number">1</span>)</span><br><span class="line">                G[id_now:n_smp_class+id_now, <span class="number">0</span>] = np.tile(class_idx, (k+<span class="number">1</span>, <span class="number">1</span>)).reshape(-<span class="number">1</span>)</span><br><span class="line">                G[id_now:n_smp_class+id_now, <span class="number">1</span>] = np.ravel(class_idx[idx_new[:]], order=<span class="string">&#x27;F&#x27;</span>)</span><br><span class="line">                G[id_now:n_smp_class+id_now, <span class="number">2</span>] = np.ravel(dump_heat_kernel, order=<span class="string">&#x27;F&#x27;</span>)</span><br><span class="line">                id_now += n_smp_class</span><br><span class="line">            <span class="comment"># build the sparse affinity matrix W</span></span><br><span class="line">            W = csc_matrix((G[:, <span class="number">2</span>], (G[:, <span class="number">0</span>], G[:, <span class="number">1</span>])), shape=(n_samples, n_samples))</span><br><span class="line">            bigger = np.transpose(W) &gt; W</span><br><span class="line">            W = W - W.multiply(bigger) + np.transpose(W).multiply(bigger)</span><br><span class="line">            <span class="keyword">return</span> W</span><br><span class="line">        <span class="keyword">elif</span> kwargs[<span class="string">&#x27;weight_mode&#x27;</span>] == <span class="string">&#x27;cosine&#x27;</span>:</span><br><span class="line">            <span class="comment"># normalize the data first</span></span><br><span class="line">            X_normalized = np.power(np.<span class="built_in">sum</span>(X*X, axis=<span class="number">1</span>), <span class="number">0.5</span>)</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n_samples):</span><br><span class="line">                X[i, :] = X[i, :]/<span class="built_in">max</span>(<span class="number">1e-12</span>, X_normalized[i])</span><br><span class="line">            G = np.zeros((n_samples*(k+<span class="number">1</span>), <span class="number">3</span>))</span><br><span class="line">            id_now = <span class="number">0</span></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n_classes):</span><br><span class="line">                class_idx = np.column_stack(np.where(y == label[i]))[:, <span class="number">0</span>]</span><br><span class="line">                <span class="comment"># compute pairwise cosine distances for instances in class i</span></span><br><span class="line">                D_cosine = np.dot(X[class_idx, :], np.transpose(X[class_idx, :]))</span><br><span class="line">                <span class="comment"># sort the distance matrix D in descending order for instances in class i</span></span><br><span class="line">                dump = np.sort(-D_cosine, axis=<span class="number">1</span>)</span><br><span class="line">                idx = np.argsort(-D_cosine, axis=<span class="number">1</span>)</span><br><span class="line">                idx_new = idx[:, <span class="number">0</span>:k+<span class="number">1</span>]</span><br><span class="line">                dump_new = -dump[:, <span class="number">0</span>:k+<span class="number">1</span>]</span><br><span class="line">                n_smp_class = <span class="built_in">len</span>(class_idx)*(k+<span class="number">1</span>)</span><br><span class="line">                G[id_now:n_smp_class+id_now, <span class="number">0</span>] = np.tile(class_idx, (k+<span class="number">1</span>, <span class="number">1</span>)).reshape(-<span class="number">1</span>)</span><br><span class="line">                G[id_now:n_smp_class+id_now, <span class="number">1</span>] = np.ravel(class_idx[idx_new[:]], order=<span class="string">&#x27;F&#x27;</span>)</span><br><span class="line">                G[id_now:n_smp_class+id_now, <span class="number">2</span>] = np.ravel(dump_new, order=<span class="string">&#x27;F&#x27;</span>)</span><br><span class="line">                id_now += n_smp_class</span><br><span class="line">            <span class="comment"># build the sparse affinity matrix W</span></span><br><span class="line">            W = csc_matrix((G[:, <span class="number">2</span>], (G[:, <span class="number">0</span>], G[:, <span class="number">1</span>])), shape=(n_samples, n_samples))</span><br><span class="line">            bigger = np.transpose(W) &gt; W</span><br><span class="line">            W = W - W.multiply(bigger) + np.transpose(W).multiply(bigger)</span><br><span class="line">            <span class="keyword">return</span> W</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split <span class="comment">#数据集划分</span></span><br><span class="line">data = pd.read_excel(<span class="string">r&#x27;path&#x27;</span>)</span><br><span class="line">data = data.drop(data[data[<span class="string">&#x27;PM10&#x27;</span>]==<span class="string">&#x27;—&#x27;</span>].index) <span class="comment">#去除缺失数据</span></span><br><span class="line">data.head()</span><br><span class="line">x = data.iloc[:,<span class="number">7</span>:]</span><br><span class="line">y = data[<span class="string">&#x27;AQI&#x27;</span>]</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=<span class="number">0.2</span>, random_state=<span class="number">40</span>)</span><br><span class="line">kwargs_W = &#123;<span class="string">&quot;metric&quot;</span>:<span class="string">&quot;euclidean&quot;</span>,<span class="string">&quot;neighbor_mode&quot;</span>:<span class="string">&quot;knn&quot;</span>,<span class="string">&quot;weight_mode&quot;</span>:<span class="string">&quot;heat_kernel&quot;</span>,<span class="string">&quot;k&quot;</span>:<span class="number">5</span>,<span class="string">&#x27;t&#x27;</span>:<span class="number">1</span>&#125;</span><br><span class="line">W = construct_W(x, **kwargs_W)</span><br><span class="line">score = lap_score(x.values, W=W)</span><br><span class="line">idx = feature_ranking(score)</span><br><span class="line">score, idx</span><br></pre></td></tr></table></figure>
</ummary></details>

<p>代码运行结果：</p>
<img data-src="https://s2.loli.net/2023/07/07/8SjH1DRrckdMVAO.png" alt="202307071609030" style="zoom:100%;">

<h3 id="1-2-2-Fisher-Score"><a href="#1-2-2-Fisher-Score" class="headerlink" title="1.2.2 Fisher Score"></a>1.2.2 Fisher Score</h3><p>Fisher Score是一种监督特征选取算法，通过挑选特征使得同一样本中特征值相似，不同类别特征值不相似。计算公式如下：<br>$$<br>f_i&#x3D;\frac{\sum_{j&#x3D;1}^{c}n_j(\mu_{ij}-\mu_i)^2}{\sum_{j&#x3D;1}^{c}n_j\sigma_{ij}^{2}}<br>$$<br>其中$n_j$代表类别$j$的数量、$\mu_{ij}$代表$j$类样本特征$f_i$的平均值、$\mu_j$代笔特征$f_i$的平均值</p>
<h2 id="1-3-总结"><a href="#1-3-总结" class="headerlink" title="1.3 总结"></a>1.3 总结</h2><img data-src="https://s2.loli.net/2023/07/08/KDNG3vM5raeypkE.png" alt="特征选择算法" style="zoom:50%;" align="center">

<img data-src="https://s2.loli.net/2023/07/08/zgo1Ic8DRCQUuZa.png" alt="特征选择算法" style="zoom:50%;">

<h2 id="二、数据降维"><a href="#二、数据降维" class="headerlink" title="二、数据降维"></a>二、数据降维</h2><p>为什么要进行数据降维度?数据维度又是什么?</p>
<blockquote>
<p>数据降维:将高维空间数据投影到低维空间,目的在于将数据的特征维度降低,作用和特征筛选的作用相类似但是两种之间有一定区别(两者都属于<strong>降低数据维度方法</strong>):<br>数据降维和特征筛选:两者都是对数据的维度进行减少,但是特征筛选:侧重在于从$D$维特征从选择$d$维特征,数据特征个数发生减少;数据降维:从高维向低维投影;通俗易懂描述为:<strong>前者为阉割</strong>(数量变化),<strong>后者为压缩</strong>(“形状”变化)<br>数据降维分类<a href="https://members.loria.fr/moberger/Enseignement/AVR/Exposes/TR_Dimensiereductie.pdf">^4</a>：</p>
</blockquote>
<img data-src="https://s2.loli.net/2023/07/05/pS7VKD9gRAMbzCx.png" alt="数据降维技术分类" style="zoom:100%;">

<h3 id="2-1-Principal-Components-Analysis（PCA）主成分分析"><a href="#2-1-Principal-Components-Analysis（PCA）主成分分析" class="headerlink" title="2.1 Principal Components Analysis（PCA）主成分分析"></a>2.1 Principal Components Analysis（PCA）主成分分析</h3><p>PCA是一种<strong>线性降维技术</strong>，即通过将数据嵌入到较低维的线性子空间中来进行降维。</p>
<blockquote>
<p>PCA通过寻找低维数据对高维数据进行描述</p>
</blockquote>
<p>数学原理如下：对于原始数据$X$通过找到<strong>映射</strong>$M$将原始数据从较高维度的$D$维转化为较低维度的$d$维即:$Y&#x3D;XM$($Y$代表转化后的低维数据),那么问题关键在于对于<strong>映射</strong>$M$的寻找.<br>PCA算法流程:<br>1、对输入数据$X&#x3D;{x_1…x_n}$进行中心化:$x_i&#x3D;x_i- \frac{1}{m}\sum_{j&#x3D;1}^{n}x_j $<br>2、对中心化的数据计算协方差矩阵:$X^TX$<br>3、计算协方差矩阵特征值,并且得到最大的$n^{‘}$($n^{‘}$代表要降低到的维度)个特征值所对应特征向量:$M&#x3D;(w_1..w_{n^{‘}})$<br>4、对样本进行转换:$Y&#x3D;XM$</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</span><br><span class="line">x = np.random.randn(<span class="number">10</span>,<span class="number">5</span>)</span><br><span class="line">pca = PCA(<span class="number">3</span>)</span><br><span class="line">pca.fit(x)</span><br><span class="line">pca.transform(x) <span class="comment">#转化</span></span><br><span class="line">pca.explained_variance_ratio_ <span class="comment">#贡献</span></span><br></pre></td></tr></table></figure>




<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>[1] LI J, CHENG K, WANG S, 等. Feature Selection:A Data Perspective[J&#x2F;OL]. ACM Computing Surveys, 2018, 50(6): 1-45. <a href="https://doi.org/10.1145/3136625">https://doi.org/10.1145/3136625</a>.<br>[2]CHANDRASHEKAR G, SAHIN F. A survey on feature selection methods[J&#x2F;OL]. Computers &amp; Electrical Engineering, 2014, 40(1): 16-28. DOI:<a href="https://doi.org/10.1016/j.compeleceng.2013.11.024">10.1016&#x2F;j.compeleceng.2013.11.024</a>.<br>[3]<a href="https://rasbt.github.io/mlxtend/user_guide/feature_selection/SequentialFeatureSelector/">https://rasbt.github.io/mlxtend/user_guide/feature_selection/SequentialFeatureSelector/</a><br>[4]Van Der Maaten, Laurens, Eric Postma, and Jaap Van den Herik.”Dimensionality reduction:a comparative.” <em>J Mach Learn Res</em> 10.66-71 (2009).<br>[5]HE X, CAI D, NIYOGI P.Laplacian Score for Feature Selection[C&#x2F;OL]&#x2F;&#x2F;Advances in Neural Information Processing Systems: 卷 18. MIT Press, 2005[2023-07-07].</p>
<h2 id="推荐阅读"><a href="#推荐阅读" class="headerlink" title="推荐阅读"></a>推荐阅读</h2><p>⭐⭐⭐LI J, CHENG K, WANG S, 等. Feature Selection: A Data Perspective[J&#x2F;OL]. ACM Computing Surveys, 2018, 50(6): 1-45. <a href="https://doi.org/10.1145/3136625">https://doi.org/10.1145/3136625</a>.<br>⭐⭐Van Der Maaten, Laurens, Eric Postma, and Jaap Van den Herik.”Dimensionality reduction:a comparative.” <em>J Mach Learn Res</em> 10.66-71 (2009).</p>
]]></content>
      <categories>
        <category>数据预处理</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>优化算法</tag>
        <tag>数据预处理</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习可解释性--LIME</title>
    <url>/posts/cd6f0230/</url>
    <content><![CDATA[<center>“Why Should I Trust You?” Explaining the Predictions of Any Classifier</center>

<center>trusting a prediction or trusting a model</center>

<p>如果⼀个机器学习模型运⾏良好，为什么我们仅仅信任该模型⽽忽略为什么做出特定的决策呢？<br>诸如分类准确性之类的单⼀指标⽆法完整地描述⼤多数实际任务。当涉及到预测模型时，需要作出权衡：<strong>你是只想知道预测是什么？</strong>例如，客户流失的概率或某种药物对病⼈的疗效。<strong>还是想知道为什么做出这样的预测？</strong>这种情况下可能为了可解释性付出预测性能下降的代价。在某些情况下，你不必关⼼为什么要做出这样的预测，只要知道模型在测试数据集的预测性能良好就⾜够了。但是在其他情况下，了解 “为什么” 可以帮助你更多地了解问题、数据以及模型可能失败的原因。有些模型可能不需要解释，因为它们是在低风险的环境中使⽤的，这意味着错误不会造成严重后果 (例如，电影推荐系统)，或者该⽅法已经被⼴泛研究和评估 (例如，光学字符识别 OCR)。对可解释性的需求来⾃问题形式化的不完整性，这意味着对于某些问题或任务，仅仅获得预测结果是不够的。该模型还必须解释是怎么获得这个预测的，因为正确的预测只部分地解决了你的原始问题。</p>
<span id="more"></span>

<h1 id="机器学习可解释性"><a href="#机器学习可解释性" class="headerlink" title="机器学习可解释性"></a>机器学习可解释性</h1><p>需要建立一个解释器来解释黑盒模型，并且这个解释器必须满足以下特征：<br><strong>可解释性</strong><br>要求解释器的模型与特征都必须是可解释的，像决策树、线性模型都是很适合拿来解释的模型；而可解释的模型必须搭配可解释的特征，才是真正的可解释性，让不了解机器学习的人也能通过解释器理解模型。<br><strong>局部保真度</strong><br>既然我们已经使用了可解释的模型与特征，就不可能期望简单的可解释模型在效果上等同于复杂模型（比如原始CNN分类器）。所以解释器不需要在全局上达到复杂模型的效果，但至少在局部上效果要很接近，而此处的局部代表我们想观察的那个样本的周围。<br><strong>与模型无关</strong><br>这里所指的是与复杂模型无关，换句话说无论多复杂的模型，像是SVM或神经网络，该解释器都可以工作。<br>除了传统的特征重要性排序外，ICE、PDP、SDT、LIME、SHAP都是揭开机器学习模型黑箱的有力工具。</p>
<ul>
<li>特征重要性计算依据某个特征进行决策树分裂时，分裂前后的信息增益（基尼系数）；</li>
<li>ICE和PDP考察某项特征的不同取值对模型输出值的影响；</li>
<li>SDT用单棵决策树解释其它更复杂的机器学习模型；</li>
<li>LIME的核心思想是对于每条样本，寻找一个更容易解释的代理模型解释原模型；</li>
<li>SHAP的概念源于博弈论，核心思想是计算特征对模型输出的边际贡献；</li>
</ul>
<hr>
<h1 id="机器学习可解释性–LIME"><a href="#机器学习可解释性–LIME" class="headerlink" title="机器学习可解释性–LIME"></a>机器学习可解释性–LIME</h1><p>[toc]</p>
<h2 id="机器学习的解释性"><a href="#机器学习的解释性" class="headerlink" title="机器学习的解释性"></a>机器学习的解释性</h2><p>Trusting a prediction, i.e. whether a user trusts an individual prediction sufficiently to take some action based on it, and  trusting a model, i.e. whether the user trusts a model to behave in reasonable ways if deployed.” (Ribeiro 等, 2016, p. 1135)</p>
<blockquote>
<p>信任一个预测，即用户是否充分信任一个个体的预测，并在此基础上采取行动；信任一个模型，即用户是否信任一个模型在部署后的行为是否合</p>
</blockquote>
<p>LIME, an algorithm that can explain the predictions of any classifier or regressor in a faithful way, by approximating it locally with an interpretable model.” (Ribeiro 等, 2016, p. 1135)</p>
<blockquote>
<p>LIME是一种算法，通过用可解释的模型对其进行<strong>局部近似</strong>，可以忠实地解释任何分类器或回归器的预测。</p>
</blockquote>
<p>所谓机器学习可解释性就是：我们建立了一个模型预测得到了一个结果，但是你这个<strong>结果真的可以让人信服吗</strong>（从实际生产角度考虑，不从模型的准确率的角度考虑）？这个<strong>模型的影响因素</strong>又是什么呢？如下图<a href="https://doi.org/10.1145/2939672.2939778">^2</a>：</p>
<img data-src="https://s2.loli.net/2023/06/04/Pm6VexD4KinwA2l.png" alt="机器学习可解释性--LIME[^2]" style="zoom:60%;">

<p>我们通过模型预测流感（flu），通过LIME得到的解释是：喷嚏（sneeze）、头疼（headche）对流感预测是“支持”，而没有疲劳（no fatigue）是“反对”。我们将模型结果交给决策者（human make decision）让她对模型结果做判断。<strong>机器学习</strong>只能作为我们辅助决策的工具，对于实际决策还是依靠<strong>人类</strong>。而我们的决策的依据就是<strong>解释</strong>（explainer），我们根据给出的解释来判别模型是否可作为我们的决策依据。</p>
<h2 id="LIME原理"><a href="#LIME原理" class="headerlink" title="LIME原理"></a>LIME原理</h2><p><strong>LIME（Local Interpretable Model-agnostic Explanations）</strong>。该模型是一个<strong>局部</strong>可解释模型，并且是一个与模型自身的无关的可解释方法。使用训练的<strong>局部代理模型</strong>来对单个样本进行解释。假设对于需要解释的黑盒模型，取关注的实例样本(<strong>可解释输入$x$</strong>)，在其附近进行扰动生成新的样本点(<strong>生成扰动样本</strong>)，并得到黑盒模型的预测值，使用新的数据集（<strong>扰动样本</strong>）训练可解释的模型（如线性回归、决策树），得到对黑盒模型良好的局部近似<a href="https://blog.csdn.net/iqdutao/article/details/108397239">^1</a>。</p>
<p>“The overall goal of LIME is to identify an interpretable model over the interpretable representation that is locally faithful to the classifier.” (Ribeiro 等, 2016, p. 1137)</p>
<blockquote>
<p>LIME的总体目标是基于局部可解释性模型 在局部忠实于分类器的可解释表示上识别一个可解释模型。</p>
</blockquote>
<p>LIME特点如下：</p>
<ul>
<li>Local：局部保证度，即我们希望解释真实反映的分类器</li>
</ul>
<h2 id="LIME计算"><a href="#LIME计算" class="headerlink" title="LIME计算"></a>LIME计算</h2><p>假设$f$作为我们需要解释的模型，那么我们定义解释模型$g \in G$ ，$G$作为<strong>解释族函数</strong>（一系列可能的解释模型（线性模型、决策树模型等）），因为并不是每一个$g\in G$都可能是简单到可以解释的，因此定义$\Omega(g)$作为<strong>复杂形测度</strong>，$\pi_{x}$作为实例$z$到$x$之间的邻近度量，从而定义$x$周围的局部性。最后定义$L(f,g,\pi_{x})$作为不忠实$g$在$\pi_{x}$定义的局部中逼近$f$，为了保证<strong>可解释性</strong>和<strong>局部忠实性</strong>计算公式：</p>
<p>$$<br>\xi(s)&#x3D; argmin_{g\in G} L(f,g,\pi_{x})+\Omega(g)<br>$$</p>
<h3 id="局部探索"><a href="#局部探索" class="headerlink" title="局部探索"></a>局部探索</h3><p>前面提及到了LIME是一种局部探索可解释模型，那么其局部探索功能如何实现呢？在论文中的 <code>3.3 Sampling for Local Exploration</code>作者给出解释如下：<br>如公式1所示我们需要最小化$L(f,g,\pi_{x})$，我们在$x$周围生成<strong>扰动样本</strong>（perturbed sample）设生成的扰动样本为$Z$，那么我们可以根据我们的$f$对我们所生成的扰动样本进行处理即：$f(Z)$，我们对扰动样本进行加权（距离$x$近的赋予较大权重，反之则较小权重）</p>
<blockquote>
<p>“where we sample instances both in the vicinity of $x$ (which have a high weight due to $\pi_{x}$) and far away from x (low weight from $\pi_{x}$).” ([Ribeiro 等, 2016, p. 1137]<br>其中，我们分别在$x$ (由于$\pi_{x}$而具有很高的权重)附近和远离$x$ (来自$\pi_{x}$的低权重)的地方采样实例。</p>
</blockquote>
<p>作者在论文中提到即使原始模型（$f$）很难全局进行解释，但是LIME能够在局部进行合理解释</p>
<blockquote>
<p>ven though the original model may be too complex to explain globally, LIME presents an explanation that is locally faithful</p>
</blockquote>
<h3 id="稀疏线性解释"><a href="#稀疏线性解释" class="headerlink" title="稀疏线性解释"></a>稀疏线性解释</h3><p>论文作者设置解释族函数$G$为线性模型（$g(Z)&#x3D;w_{g}z$），设置$L$为</p>
<p>$$<br>\underset{z,z^{‘}\in Z}{\sum}\pi_{x}(z)(f(z)-g(z^{‘}))^{2}<br>$$</p>
<p>$\pi_{x}$为$exp(\frac{-D(x,z)^{2}}{\sigma^{2}})$，上面提及到的扰动样本$Z$，对于扰动样本设计线性函数去对扰动样本进行区分（已分类算法为例）那么我们所赋予的不同的权重$w_{g}$就是不同的样本中不同特征的影响。</p>
<h3 id="LIME步骤"><a href="#LIME步骤" class="headerlink" title="LIME步骤"></a>LIME步骤</h3><ul>
<li>对整个数据进行训练，模型可以是Lightgbm，XGBoost等复杂的模型（本身不可解释）;</li>
<li>选择我们想要解释的变量$x$;</li>
<li>对数据集中的数据进行可解释的N次扰动，生成扰动样本;</li>
<li>对这些新的样本求出权重，这个权重是这些数据点与我们要解释的数据之间的距离;</li>
<li>根据上面新的数据集，拟合一个简单的模型$g$，比如Lasso Regression得到模型的权重;</li>
<li>通过简单模型$g$来对原复杂模型在$x$点附近进行解释;</li>
</ul>
<h2 id="LIME直观解释"><a href="#LIME直观解释" class="headerlink" title="LIME直观解释"></a>LIME直观解释</h2><h3 id="1、分类算法"><a href="#1、分类算法" class="headerlink" title="1、分类算法"></a>1、分类算法</h3><img data-src="https://s2.loli.net/2023/06/05/RqaWkvG7wHSo6jF.png" alt="202306052009422" style="zoom:50%;">

<p>如上图不同的颜色块代表不同的类别（蓝色和粉色），很难通过线性模型进行近似。因此输入样本（加重红色×）在其周围生成不同的扰动样本（×和·，其大小代表距离），我们可以对所生成的扰动样本构建线性函数进行区分。具体步骤如下图所示：</p>
<img data-src="https://s2.loli.net/2023/06/05/W2h9NPIY1sDdwi6.png" alt="202306052008339" style="zoom:60%;">

<h3 id="2、图像识别"><a href="#2、图像识别" class="headerlink" title="2、图像识别"></a>2、图像识别</h3><p>在Local Interpretable Model-Agnostic Explanations (LIME): An Introduction<a href="https://www.oreilly.com/content/introduction-to-local-interpretable-model-agnostic-explanations-lime/">^3</a>中作者对图像识别做出了解释。对一只树蛙进行分类：</p>
<img data-src="https://s2.loli.net/2023/06/05/srhUdaWXZK1jwgT.png" alt="202306052008339" style="zoom:60%;">

<p>将一些可解释的成分 “关闭”（在这种情况下，使它们变成灰色）来生成一组扰乱实例的数据。对于每个被扰乱的实例，我们根据模型得到树蛙在图像中的概率。然后我们在这个数据集上学习一个简单的（线性）模型，这个模型是局部加权的–也就是说，我们更关心在与原始图像更相似的扰动实例中犯错误。最后，我们将具有最高正向权重的超级像素作为解释，而将其他的东西涂成灰色。</p>
<img data-src="https://s2.loli.net/2023/06/05/xTNHmRpBsW52Lna.png" alt="202306052008339" style="zoom:60%;">

<p>我们在任意图像上解释谷歌的 Inception 神经网络。在这种情况下，如下图 所示，分类器将“树蛙”预测为最有可能的类别，其次是概率较低的“台球桌”和“气球”。解释表明分类器主要关注青蛙的脸作为对预测类别的解释。它还阐明了为什么“台球桌”的概率不为零：青蛙的手和眼睛与台球很相似，尤其是在绿色背景下。同样，爱心也很像一个红色的气球。</p>
<img data-src="https://s2.loli.net/2023/06/05/bE4x2AP1dktK8VZ.png" alt="202306052008339" style="zoom:60%;">

<h2 id="LIME优缺点"><a href="#LIME优缺点" class="headerlink" title="LIME优缺点"></a>LIME优缺点</h2><p>1、LIME算法有很强的通用性，效果好。LIME除了能够对图像的分类结果进行解释外，还可以应用到自然语言处理的相关任务中，如主题分类、词性标注等。因为LIME本身的出发点就是模型无关的，具有广泛的适用性。</p>
<p>2、LIME算法速度慢，LIME在采样完成后，每张采样出来的图片都要通过原模型预测一次结果，所以在速度上没有明显优势。</p>
<p>3、LIME算法拓展方向，本文的作者在18年新提出了Anchors的方法，指的是复杂模型在局部所呈现出来的很强的规则性的规律，注意和LIME的区别，LIME是在局部建立一个可理解的线性可分模型，而Anchors的目的是建立一套更精细的规则系统。在和文本相关的任务上有不错的表现。有待我们继续研究。</p>
<p>优点：</p>
<ul>
<li>表格型数据、文本和图片均适用；</li>
<li>解释对人友好，容易明白；</li>
<li>给出一个忠诚性度量，判断可解释模型是否可靠；</li>
<li>LIME可以使用原模型所用不到的一些特征数据，比如文本中一个词是否出现。</li>
</ul>
<p>缺点：</p>
<ul>
<li>表格型数据中，相邻点很难定义，需要尝试不同的kernel来看LIME给出的可解释是否合理；</li>
<li>扰动时，样本服从高斯分布，忽视了特征之间的相关性；</li>
<li>稳定性不够好，重复同样的操作，扰动生成的样本不同，给出的解释可能会差别很大。</li>
</ul>
<h2 id="推荐阅读"><a href="#推荐阅读" class="headerlink" title="推荐阅读"></a>推荐阅读</h2><p><a href="http://hjiezero.github.io/posts/32fedbdb/">http://hjiezero.github.io/posts/32fedbdb/</a></p>
<h1 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h1><p>Python库<a href="https://github.com/marcotcr/lime">LIME</a>提供了实现模型解释的接口，按数据类型分为表格(LimeTabularExplainer), 图像(LimeImageExplainer) 和文本(LimeTextExplainer)<br>创建LimeTabularExplainer 类<a href="https://e0hyl.github.io/BLOG-OF-E0/LIMEandSHAP/">^4</a></p>
<ul>
<li>必要参数是 <code>training_data</code>,因为采样时会依照训练数据的分布，类型是 <code>numpy</code> 二维数组；</li>
<li><code>feature_names</code> 和 <code>class_names</code> 分别是特征和类别的名字（字符串）；</li>
<li>由于类别数据和数值数据的处理方式不同，需要把对应的特征编号传入参数 <code>categorical_features</code> 中；</li>
<li>mode 参数代表任务的类型，可选分类（<code>classification</code>）或回归（<code>regression</code>）。</li>
</ul>
<p>解释单个样本：调用该类的 <code>explain_instance</code> 方法，返回 <code>Explanation</code> 对象</p>
<ul>
<li>必要参数 <code>data_row</code>：待解释的样本（一维 <code>numpy</code>数组）；</li>
<li>必要参数 <code>predict_fn</code>，用于预测的函数（对分类任务而言，输入 <code>numpy</code>数组，输出预测概率值，例如 <code>sklearn</code>的分类器的 <code>predict_proba</code>）;</li>
<li><code>num_features</code>表示解释结果中最多能呈现的特征数量；</li>
<li><code>distance_metric</code>默认的是 euclidean。<br>返回的 Explanation 有</li>
<li><code>as_map</code>方法：返回字典类型，标签 -&gt; 元组（特征id，权重）的列表</li>
<li><code>local_pred</code>属性：字典类型，类别标签 -&gt; 概率</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">from lime.lime_tabular import LimeTabularExplainer</span><br><span class="line">lime_explainer = LimeTabularExplainer(</span><br><span class="line">    x_train, #输入训练数据</span><br><span class="line">    feature_names=list(data_bs.columns[4:]), #特征</span><br><span class="line">    mode =&#x27;regression&#x27;, #处理类型</span><br><span class="line">    class_names=[&#x27;AQI&#x27;] #标签)</span><br><span class="line"></span><br><span class="line">exp = lime_explainer.explain_instance(</span><br><span class="line">    data_row=x_test[0], #处理数据：选择不同数据得到图不一样</span><br><span class="line">    predict_fn=mlp.predict #预测函数)</span><br><span class="line"></span><br><span class="line">#exp.show_in_notebook(show_table=True)</span><br><span class="line">exp.as_pyplot_figure()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p>结果如下：</p>
<img data-src="https://s2.loli.net/2023/06/07/LPuY56g8dC1otTB.png" alt="202306071525633" style="zoom:60%;">

<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p>1.<a href="https://blog.csdn.net/iqdutao/article/details/1083972">https://blog.csdn.net/iqdutao/article/details/1083972</a></p>
<p>2.<a href="https://doi.org/10.1145/2939672.2939778">https://doi.org/10.1145/2939672.2939778</a></p>
<p>3.<a href="https://www.oreilly.com/content/introduction-to-local-interpretable-model-agnostic-explanations-lime/">https://www.oreilly.com/content/introduction-to-local-interpretable-model-agnostic-explanations-lime/</a></p>
<p>4.<a href="https://e0hyl.github.io/BLOG-OF-E0/LIMEandSHAP/">https://e0hyl.github.io/BLOG-OF-E0/LIMEandSHAP/</a></p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>文献笔记</tag>
        <tag>机器学习可解释性</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习可解释性--SHAP</title>
    <url>/posts/5bf8429/</url>
    <content><![CDATA[<center>A Unified Approach to Interpreting Model Predictions</center>

<center>trusting a prediction or trusting a model</center>

<p>如果⼀个机器学习模型运⾏良好，为什么我们仅仅信任该模型⽽忽略为什么做出特定的决策呢？<br>诸如分类准确性之类的单⼀指标⽆法完整地描述⼤多数实际任务。当涉及到预测模型时，需要作出权衡：<strong>你是只想知道预测是什么？</strong>例如，客户流失的概率或某种药物对病⼈的疗效。<strong>还是想知道为什么做出这样的预测？</strong>这种情况下可能为了可解释性付出预测性能下降的代价。在某些情况下，你不必关⼼为什么要做出这样的预测，只要知道模型在测试数据集的预测性能良好就⾜够了。但是在其他情况下，了解 “为什么” 可以帮助你更多地了解问题、数据以及模型可能失败的原因。有些模型可能不需要解释，因为它们是在低风险的环境中使⽤的，这意味着错误不会造成严重后果 (例如，电影推荐系统)，或者该⽅法已经被⼴泛研究和评估 (例如，光学字符识别 OCR)。对可解释性的需求来⾃问题形式化的不完整性，这意味着对于某些问题或任务，仅仅获得预测结果是不够的。该模型还必须解释是怎么获得这个预测的，因为正确的预测只部分地解决了你的原始问题。</p>
<span id="more"></span>

<h1 id="机器学习可解释性"><a href="#机器学习可解释性" class="headerlink" title="机器学习可解释性"></a>机器学习可解释性</h1><p>需要建立一个解释器来解释黑盒模型，并且这个解释器必须满足以下特征：<br><strong>可解释性</strong><br>要求解释器的模型与特征都必须是可解释的，像决策树、线性模型都是很适合拿来解释的模型；而可解释的模型必须搭配可解释的特征，才是真正的可解释性，让不了解机器学习的人也能通过解释器理解模型。<br><strong>局部保真度</strong><br>既然我们已经使用了可解释的模型与特征，就不可能期望简单的可解释模型在效果上等同于复杂模型（比如原始CNN分类器）。所以解释器不需要在全局上达到复杂模型的效果，但至少在局部上效果要很接近，而此处的局部代表我们想观察的那个样本的周围。<br><strong>与模型无关</strong><br>这里所指的是与复杂模型无关，换句话说无论多复杂的模型，像是SVM或神经网络，该解释器都可以工作。<br>除了传统的特征重要性排序外，ICE、PDP、SDT、LIME、SHAP都是揭开机器学习模型黑箱的有力工具。</p>
<ul>
<li>特征重要性计算依据某个特征进行决策树分裂时，分裂前后的信息增益（基尼系数）；</li>
<li>ICE和PDP考察某项特征的不同取值对模型输出值的影响；</li>
<li>SDT用单棵决策树解释其它更复杂的机器学习模型；</li>
<li>LIME的核心思想是对于每条样本，寻找一个更容易解释的代理模型解释原模型；</li>
<li>SHAP的概念源于博弈论，核心思想是计算特征对模型输出的边际贡献；</li>
</ul>
<hr>
<p>1、<a href="https://hjezero.github.io/posts/cd6f0230/">机器学习可解释性–LIME</a><br>2、<a href="https://hjezero.github.io/posts/5bf8429/">机器学习可解释性–SHAP</a></p>
<h1 id="机器学习可解释性–SHAP"><a href="#机器学习可解释性–SHAP" class="headerlink" title="机器学习可解释性–SHAP"></a>机器学习可解释性–SHAP</h1><p>[toc]</p>
<h2 id="1、Shapley-Value"><a href="#1、Shapley-Value" class="headerlink" title="1、Shapley Value"></a>1、Shapley Value</h2><p><strong>沙普利值</strong><a href="https://github.com/MingchaoZhu/InterpretableMLBook">^1</a>（Shapley value），是通过考虑各个代理（agent）做出的贡献，来公平地分配合作收益。代理的沙普利值是对于一个合作项目所期望的贡献量的平均值。计算公式：<br>设$I&#x3D;{1,2…n}$为$n$个人的集合那么$i$的贡献为：</p>
<p>$$<br>\varphi_{i}&#x3D;\sum_{s\in S_{i}}w(|s|)[v(s)-v(s\i)]<br>$$</p>
<p>其中$S_{i}$是$I$中包含成员$i$的所有子集形成的集合，$w(|s|)$是加权因子，$s\i$表示集合$s$中去掉元素$i$后的集合。<br>$v(s)-v(s\i)$成员i在联盟中的贡献，即成员i的边际贡献；$w(|s|)$权重$w(|s|&#x3D;\frac{(|s|-1)!(n-|s|)!}{n!})$</p>
<blockquote>
<p>WiKi：</p>
<p>玩家联盟合作，并从合作中获得一定的整体收益。由于一些参与者可能比其他参与者对联盟贡献更多，或者可能拥有不同的议价能力（例如威胁要摧毁全部盈余），因此在任何特定博弈中，参与者之间产生的盈余的最终分配应该是什么？或者换句话说：每个参与者对整体合作有多重要，他或她可以合理预期的回报是什么？Shapley 值为这个问题提供了一个可能的答案。</p>
<p>1、<a href="https://en.wikipedia.org/wiki/Shapley_value">https://en.wikipedia.org/wiki/Shapley_value</a><br>《可解释的机器学习》<br>开源地址：<br>2、<a href="https://github.com/MingchaoZhu/InterpretableMLBook">https://github.com/MingchaoZhu/InterpretableMLBook</a><br>在线阅读：<br>3、<a href="https://hjezero.github.io/posts/32fedbdb/">https://hjezero.github.io/posts/32fedbdb/</a></p>
</blockquote>
<p>借助《可解释的机器学习》中例子，假设准备购买一间公寓，其中你所感兴趣的是：1、有无公园；2、公寓面积；3、公寓所位于的楼层；4、是否能养猫。而后对公寓价格预测。</p>
<img data-src="https://s2.loli.net/2023/06/07/cgknwhxlI9uJT4H.png" alt="202306071608311" style="zoom:80%;">
>⼀套 50 平⽅⽶的公寓，附近有公园以及禁⽌猫⼊内，预计价格为 300,000 欧元

<p>那么1、公寓面积；2、公寓楼层；3、养猫？4、公园？这4个特征对<strong>预测价格</strong>300000欧元的<strong>贡献</strong>是怎么样的呢？</p>
<blockquote>
<p>比如说<a href="https://www.zhihu.com/question/23180647">^2</a>：一个程序C&#x3D;500行代码需要编写，今天产品经理找了三个程序猿来完成，按照完成量发奖金：<br>条件一：$V_{1}&#x3D;100$，$V_{2}&#x3D;125$，$V_{3}&#x3D;50$<br>解释：1号屌丝程序猿独立能写100行，2号大神程序猿独立能写125行，3号美女程序猿能写50行<br>条件二：$V_{12}&#x3D;270$，$V_{23}&#x3D;350$，$V_{13}&#x3D;375$<br>解释：1,2号合作能写270行，2,3号合作能写350行，1,3号合作能写375行<br>条件三：$V_{123}&#x3D;500$<br>3个人共同能完成500行<br>那么根据3组条件，合计6种组合分别如下：<br>A.1号程序猿邀请2号程序猿加入他组成S联盟，1,2号邀请3号加入共同编写。<br>B.1号邀请3号加入成为S小组，2号加入S小组<br>C.2号邀请1号加入成为S小组，3号加入S小组<br>D.2号邀请3号加入成为S小组，1号加入S小组<br>E.3号邀请1号加入成为S小组，2号加入S小组<br>F.3号邀请2号加入成为S小组，1号加入S小组<br>计算边际贡献：</p>
</blockquote>
<blockquote>
<img data-src="https://pica.zhimg.com/450be1b5a2d5196399f6665086a728d2_r.jpg?source=1940ef5c" alt="202306071608311" style="zoom:80%;">
</blockquote>
<blockquote>
<p>根据公式1得到：<br>1的shapley value：<br>$\frac{1}{6}(100+100+145+150+325+150)&#x3D;\frac{970}{6}$</p>
<p>依次类推得到其他人的shapely value</p>
</blockquote>
<h2 id="2、A-Unified-Approach-to-Interpreting-Model-Predictions"><a href="#2、A-Unified-Approach-to-Interpreting-Model-Predictions" class="headerlink" title="2、A Unified Approach to Interpreting Model Predictions"></a>2、<a href="https://arxiv.org/pdf/1705.07874.pdf">A Unified Approach to Interpreting Model Predictions</a></h2><p>SHAP Value与LIME的方法类似，都是通过定义简单模型去对复杂模型进行解释。</p>
<blockquote>
<p>“we must use a simpler explanation model, which we define as any interpretable approximation of the original model.” (Lundberg 和 Lee, 2017, p. 2)</p>
<p>我们必须使用一个更简单的解释模型，我们将其定义为对原模型的任何可解释的近似。</p>
</blockquote>
<p>回顾LIME模型，利用简单函数$g$去在$x$“周围”去对复杂函数$f$进行局部近似。在本论文作者将部分<strong>解释模型函数</strong>称为<em>Additive feature attribution methods</em>其形式形如：</p>
<p>$$<br>g(z)&#x3D;\phi_{0}+\sum_{i&#x3D;1}^{M}\phi_{i}z_{i}<br>$$</p>
<p>其中：$M$所有的简单输入特征的个数；$\phi_{i}$每一种特征的<strong>贡献</strong>（对于的shapely值）；通过计算所有特征的贡献去近似复杂函数$f(x)$</p>
<blockquote>
<p>论文中部分<strong>解释模型函数</strong>有：</p>
<p>1、LIME模型对<strong>贡献</strong>计算：</p>
<p>$$<br>\xi(s)&#x3D; argmin_{g\in G} L(f,g,\pi_{x})+\Omega(g)<br>$$</p>
<p>2、DeepLIFT（面向深度学习的可解释方法）模型对<strong>贡献</strong>计算：</p>
<p>$$<br>\sum_{i&#x3D;1}^{n}C_{\Delta x_{i}\Delta o}&#x3D;\Delta o<br>$$</p>
<p>贡献为：$C_{\Delta x_{i}\Delta o}$，其中$o&#x3D;f(x)$为模型的输出，其中$r$为参考的样本输入，$\Delta o&#x3D;f(x)-f(r)$。<br>3、经典Shapley Value计算：</p>
<ul>
<li>Shapley regression values</li>
</ul>
<blockquote>
<p>feature importances for linear models in the presence of multicollinearity.<br>此模型要求计算模型所有的特征，认为每一个特征在模型中都起到了作用。计算公式如下：</p>
</blockquote>
<img data-src="https://s2.loli.net/2023/06/08/LFq8k1WHQsghB4P.png" alt="202306081444642" style="zoom:60%;">

<blockquote>
<p>就是上面列子中提及到的计算方法</p>
</blockquote>
<ul>
<li>Shapley sampling values</li>
<li>Quantitative Input Influence</li>
</ul>
</blockquote>
<p>对于<em>additive feature attribution methods</em> 存在如下三点<a href="https://christophm.github.io/interpretable-ml-book/shap.html">性质</a>：<br>1、Local accuracy<br>2、Missingness<br>3、Consistency</p>
<h2 id="Kernel-SHAP-Linear-LIME-Shapley-values"><a href="#Kernel-SHAP-Linear-LIME-Shapley-values" class="headerlink" title="Kernel SHAP(Linear LIME + Shapley values)"></a><em>Kernel SHAP(Linear LIME + Shapley values)</em></h2><h3 id="Kernel-SHAP计算步骤："><a href="#Kernel-SHAP计算步骤：" class="headerlink" title="Kernel SHAP计算步骤："></a>Kernel SHAP计算步骤：</h3><p><strong>※<em>1. 初始化一些数据, z’, 作为Simplified Features（随机生成（0,1,1,0）,(1,0,1,1)等）</em></strong></p>
<blockquote>
<p>其中：$z’_{k}\in{(0,1)}^{M}$（0：缺失特征；1：初始化中存在的特征）其中$M$是我们的维数（理解为样本特征数目），$k\in{(1,…K)}$代表生成数据个数</p>
</blockquote>
<p><strong>※<em>2. 将上面的Simplified Features转换到原始数据空间, 并计算对应的预测值, f(h(z’))</em></strong></p>
<blockquote>
<p>比如说上面卖房子例子，有四个特征所以$M&#x3D;4$那么假设初始化为$z’_{1}&#x3D;(0,1,1,0)$那么也就是存在第二和第三个特征的联盟，那么对于第一个和第四个则通过$h$函数进行转换</p>
<img data-src="https://s2.loli.net/2023/06/08/g3nWKHrlfB5Z79T.png" alt="202306082023949" style="zoom:60%;">
</blockquote>
<p><strong>※<em>3. 对每一个z’计算对应的权重</em></strong></p>
<blockquote>
<p><strong>Kernel SHAP的权重函数</strong>$\pi$为：$\pi_{x^{‘}}(z^{‘})&#x3D;\frac{M-1}{(M;choose;|z^{‘}|)(M-|z^{‘}|)}$，其中$M$为维数（所有特征的个数），$|z^{‘}|$代表样本中1的个数，$M;choose;|z^{‘}|$代表$C_{M}^{z^{‘}}|z^{‘}|$。容易得到：若有很多1或很多0则取较高的权重，若0和1数量相近则取较低的权重。</p>
<p><strong>LIME则是通过距离设置权重</strong></p>
</blockquote>
<p><strong>※<em>4. 拟合线性模型</em></strong><br><strong>※<em>5. 计算出每一个特征的Shapley Value, 也就是线性模型的系数</em></strong></p>
<h3 id="Kernel-SHAP计算"><a href="#Kernel-SHAP计算" class="headerlink" title="Kernel SHAP计算"></a>Kernel SHAP计算</h3><p>损失函数：</p>
<p>$$<br>L(f,g,\pi_x^{‘})&#x3D;\sum_{z^{‘} \in Z}[f(h_{x}^{-1}(z^{‘}))-g(z^{‘})]^{2}\pi_{x}^{‘}(z^{‘})<br>$$</p>
<h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#shap进行解释</span><br><span class="line"># 新建一个解释器</span><br><span class="line"># 这里传入两个变量, 1. 模型; 2. 训练数据</span><br><span class="line">explainer = shap.KernelExplainer(predict, x)</span><br><span class="line"># 对特征重要度进行解释</span><br><span class="line">#输出array数组</span><br><span class="line">shap_values1 = explainer.shap_values(np.array(x_test))</span><br><span class="line"># 进行绘图</span><br><span class="line">shap.summary_plot(shap_values=shap_values1,</span><br><span class="line">                 features=np.array(x_test),</span><br><span class="line">                 feature_names= list(data_bs.columns[4:]),</span><br><span class="line">                 plot_type=&#x27;bar&#x27;)</span><br></pre></td></tr></table></figure>
<p>运行结果：<br><img data-src="https://s2.loli.net/2023/06/09/k5NbE3TvQuFtP92.png" alt="202306091557500" style="zoom:100%;"></p>
<p>更多：<a href="https://zhuanlan.zhihu.com/p/441302127">https://zhuanlan.zhihu.com/p/441302127</a></p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p>1、<a href="https://github.com/MingchaoZhu/InterpretableMLBook">https://github.com/MingchaoZhu/InterpretableMLBook</a></p>
<p>2、<a href="https://www.zhihu.com/question/23180647">https://www.zhihu.com/question/23180647</a></p>
<p>3、[关于Shapley Value（夏普利值）的公式 - 知乎 (zhihu.com)](<a href="https://zhuanlan.zhihu.com/p/483891565#:~:text=Shapley">https://zhuanlan.zhihu.com/p/483891565#:~:text=Shapley</a> Value公式如下： 记 I%3D { 1%2C2%2C…%2Cn}为n个合作人的集合 varphi_i (upsilon),- upsilon (s backslash  { i })]})</p>
<p>4、<a href="https://e0hyl.github.io/BLOG-OF-E0/LIMEandSHAP/">https://e0hyl.github.io/BLOG-OF-E0/LIMEandSHAP/</a></p>
<p>5、<a href="https://mathpretty.com/10699.html">https://mathpretty.com/10699.html</a></p>
<h1 id="推荐阅读"><a href="#推荐阅读" class="headerlink" title="推荐阅读"></a>推荐阅读</h1><p>1、<a href="https://christophm.github.io/interpretable-ml-book">https://christophm.github.io/interpretable-ml-book</a></p>
<p>[^3]: [关于Shapley Value（夏普利值）的公式 - 知乎 (zhihu.com)](<a href="https://zhuanlan.zhihu.com/p/483891565#:~:text=Shapley">https://zhuanlan.zhihu.com/p/483891565#:~:text=Shapley</a> Value公式如下： 记 I%3D { 1%2C2%2C…%2Cn}为n个合作人的集合 varphi_i (upsilon),- upsilon (s backslash  { i })]})</p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>文献笔记</tag>
        <tag>机器学习可解释性</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习评价标准</title>
    <url>/posts/2592384004/</url>
    <content><![CDATA[<p>介绍机器学习评估标准</p>
<span id="more"></span>
<h1 id="机器学习评估指标"><a href="#机器学习评估指标" class="headerlink" title="机器学习评估指标"></a>机器学习评估指标</h1><h2 id="一、分类算法"><a href="#一、分类算法" class="headerlink" title="一、分类算法"></a>一、分类算法</h2><p><strong>混淆矩阵</strong></p>
<blockquote>
<p>混淆矩阵是监督学习中的一种可视化工具，主要用于比较分类结果和实例的真实信息。矩阵中的每一行代表实例的 <strong>预测类别</strong> ,每一列代表实例的 <strong>真实类别</strong> 。</p>
<img data-src="https://s2.loli.net/2023/06/05/uN2gU7d1AbzjX8v.png" alt="202306051339971" style="zoom:60%;">
</blockquote>
<p>混淆矩阵的指标</p>
<blockquote>
<p>1、TP：将正类预测为正类数</p>
<p>2、FN：将正类预测为负类数</p>
<p>3、FP：将负类预测为正类数</p>
<p>4、TN：将负类预测为负类数</p>
</blockquote>
<h3 id="1、精确率"><a href="#1、精确率" class="headerlink" title="1、精确率"></a>1、精确率</h3><p>分类正确的正样本个数占分类器判定为正样本的样本个数的比例（预测分类为1，相应的预测对的概率）—<strong>错报</strong></p>
<h3 id="2、召回率"><a href="#2、召回率" class="headerlink" title="2、召回率"></a>2、召回率</h3><p>分类正确的正样本个数占真正的正样本个数的比例（真实分类为1，相应的预测对的概率）—<strong>漏报</strong></p>
<blockquote>
<p>比如说：我们要从一个<strong>盒子</strong>里面挑选出10个球，其中盒子球的构成为红球：95，白球：5。那么抽到白球的准确率，召回率？</p>
<p>第一次：10个都是红色—-那么准确率：0 召回率：0</p>
<p>第二次：6个红色，4个白色—那么准确率：4&#x2F;10 召回率：4&#x2F;5</p>
</blockquote>
<p><strong>计算公式</strong></p>
<p>$$<br>准确率&#x3D;\frac{所有预测正确的样本}{总样本}&#x3D;\frac{TP+TN}{TP+FN+FP+TN}<br>$$</p>
<p>$$<br>召回率&#x3D;\frac{将正类预测为正类}{原本正类}&#x3D;\frac{TP}{TP+FN}<br>$$</p>
<p>$$<br>精确率&#x3D;\frac{将正类预测为正类}{预测的正类}&#x3D;\frac{TP}{TP+FP}<br>$$</p>
<p>取舍问题：在不同的场合对于<strong>精确率和召回率</strong>要求不同。</p>
<p>例如：对于股票预测，更多的应该是关注精准率，假设关注股票上升的情况，高精准率意味着TP值高（正确地预测到股票会升），这个时候可以帮助人们调整投资，增加收入，如果这一指标低，就以为FP值高（错误地认为股票会升），也就是说股票其实是降的，而预测成升了，这将会使用户亏钱。而召回率低只是意味着在股票上升的情况中，对几个股票上升的情况没有被预测到，这对于投资者来说也是可以接受的，毕竟没有亏钱，因此低召回率对用户影响不是很大。</p>
<p>例如：对于疾病预测领域，更多的应该关注召回率，因为高召回率意味着能够更多地将得病的病人预测出来，这个对于患病者非常重要。而精准率低意味着错误地预测病人患病，而这个时候只要被预测患病的人再去检查一下即可，实际上是可以接受的，因此低精准率对用户影响不大。</p>
<h3 id="3、F1-score"><a href="#3、F1-score" class="headerlink" title="3、F1-score"></a>3、F1-score</h3><p>是一种量测算法的精确度常用的指标 ，经常用来判断算法的精确度。目前在辨识、侦测相关的算法中经常会分别提到 精确率 （precision）和 召回率 （recall），F-score能同时考虑这两个数值，平衡地反映这个算法的精确度。</p>
<blockquote>
<p>维基百科：<a href="https://zh.wikipedia.org/wiki/F-score">https://zh.wikipedia.org/wiki/F-score</a></p>
</blockquote>
<p><strong>计算公式：</strong></p>
<p>$$<br>F_1&#x3D;\frac{2TP}{2TP+FN+FP}<br>$$</p>
<p>设想一下一个比较极端的情况，如正样本90个，负样本10个，我们直接将所有样本分类为正样本，得到准确率为90%。单从数值上而言结果是可以接受的，但是这样就违背了我们进行分类的初衷，应该赛选出正样本的同时，尽可能少的让负样本进入。那么我们就引入TPR、FPR、TNR对其进行限制</p>
<h3 id="4、ROC曲线和AUC值"><a href="#4、ROC曲线和AUC值" class="headerlink" title="4、ROC曲线和AUC值"></a>4、ROC曲线和AUC值</h3><h4 id="4-1-TPR、FPR、TNR"><a href="#4-1-TPR、FPR、TNR" class="headerlink" title="4.1 TPR、FPR、TNR"></a>4.1 TPR、FPR、TNR</h4><p><strong>真正类率</strong>，刻画的是被分类器正确分类的正实例占所有正实例的比例。<strong>即：正确判断为正的占全部正的比例</strong></p>
<p>$$<br>TPR&#x3D;\frac{TP}{TP+FN}<br>$$</p>
<p><strong>负正类率</strong>，计算的是被分类器错认为正类的负实例占所有负实例的比例。<strong>即：将负错误判断为正的占全部负的比例</strong></p>
<p>$$<br>FPR&#x3D;\frac{FP}{FP+TN}<br>$$</p>
<p><strong>真负类率</strong>，刻画的是被分类器正确分类的负实例占所有负实例的比例。<strong>即：正确分类为负占全部负的比例</strong></p>
<p>$$<br>TNR&#x3D;1-FPR&#x3D;\frac{TN}{FP+TN}<br>$$</p>
<p>那么通过分析容易知道，我们希望TPR的值越大越好，相反FPR的值越小越好。知道3个指标之后我们开始了解什么是ROC曲线，设想在一个分类问题（比如手写字体识别）中我们可能很难100%的判断就一定属于某个数值，但是要是给定属于某个数字的概率，比如说属于1的概率为95%，2的概率为90%……那么我们很可能做出判断这个手写字就是1，为什么呢？因为他的概率大？但是数字2的概率也有90%为什么不选择数字2呢？在实际生活中这种情况经常有，我们很难100%判断某个数字但是我们可以规定，比如说：概率大于90%那么就认为是1，反之记作0这样的话上面的例子就解释得通了。这个90%常常记作<strong>阈值</strong>，那么不同阈值和我们ROC曲线又有什么关系呢？不妨通过下面这个例子进行了解：</p>
<blockquote>
<p>分类问题：判断是不是🚲？</p>
<table>
<thead>
<tr>
<th align="center">序号</th>
<th>类别</th>
<th>概率</th>
</tr>
</thead>
<tbody><tr>
<td align="center">1</td>
<td>🚳</td>
<td>0.3</td>
</tr>
<tr>
<td align="center">2</td>
<td>🚲</td>
<td>0.3</td>
</tr>
<tr>
<td align="center">3</td>
<td>🚲</td>
<td>0.6</td>
</tr>
<tr>
<td align="center">4</td>
<td>🚲</td>
<td>0.8</td>
</tr>
<tr>
<td align="center">5</td>
<td>🚲</td>
<td>0.9</td>
</tr>
<tr>
<td align="center">6</td>
<td>🚳</td>
<td>0.1</td>
</tr>
<tr>
<td align="center">7</td>
<td>🚳</td>
<td>0.2</td>
</tr>
<tr>
<td align="center">8</td>
<td>🚳</td>
<td>0.3</td>
</tr>
</tbody></table>
<p>那么可以假设不同<strong>阈值</strong>，进而计算不同TPR和FPR的值。比如说:</p>
<p>阈值取[0,0.1]的时候，发现概率都大于0.1那么我们认为全部都是🚲，所以就有</p>
</blockquote>
<blockquote>
<p>$$<br>TPR&#x3D;\frac{TP}{TP+FN}&#x3D;\frac{4}{4+0}<br>$$</p>
<p>$$<br>FPR&#x3D;\frac{FP}{FP+TN}&#x3D;\frac{4}{4+0}<br>$$</p>
<p>这样的话我们就可以在ROC曲线上标记一个点，通过不断的移动阈值我们就可以得到一个ROC曲线<br>因此我们可以得到：</p>
<img data-src="https://s2.loli.net/2023/06/04/SfBCdjHLeuXponv.png" alt="202306041906707" style="zoom:60%;">
</blockquote>
<p>这样的话我们就可以得到一条ROC曲线，但是问题有来了要是我们得到另外一条ROC曲线，也就是说我们现在有两条ROC曲线，那么我们应该怎么判断呢？这就是接下来要说的AUC值，问题又来了怎么知道AUC的值呢？—–&gt;计算面积阿伟。没错就是计算面积，我们可以通过计算不同ROC曲线与FPR的面积进而得到不同AUC的值，从而判断哪条ROC曲线更加的好！</p>
<p>那么问题来了上述分析都是针对二分类问题，实际生活中并没有那么多非黑即白的事情，更多的是<strong>多分类的问题</strong>，什么是多分类？维基百科给出的定义：多元分类是将实例分配到多个（多于两个）类别中的其中一个（将实例分配到两个类别中的其中一个被称为二分类）。显然，分类算法可以分为二分类和多分类两种，而多分类算法可以通过将其转化为多个二分类来实现。简单从字面理解很容易，比如说给出大量的交通图片，交给计算机去将这些图片进行分类，划分什么是🚗🚆✈等等，那么对于多分类问题其评价指标如何？上述分析方法是否依旧行得通？对于多元分类我们可以将多分类化成二分类问题，比如说下图：</p>
<img data-src="https://s2.loli.net/2023/06/04/IvMa1ZB8m39gJ4R.png" alt="202306041907421" style="zoom:100%;">

<h2 id="二、回归算法评价指标"><a href="#二、回归算法评价指标" class="headerlink" title="二、回归算法评价指标"></a>二、回归算法评价指标</h2><h3 id="1、RMSE均方根误差"><a href="#1、RMSE均方根误差" class="headerlink" title="1、RMSE均方根误差"></a>1、RMSE均方根误差</h3><p>$$<br>RMSE(X,h)&#x3D;\sqrt[2]{\frac{1}{m}\displaystyle\sum^{m}_{i&#x3D;1}(h(x_i)-y_i)^2}<br>$$</p>
<h3 id="2、MSE均方误差"><a href="#2、MSE均方误差" class="headerlink" title="2、MSE均方误差"></a>2、MSE均方误差</h3><p>$$<br>MSE(X,h)&#x3D;\frac{1}{m}\displaystyle\sum^{m}_{i&#x3D;1}(h(x_i)-y_i)^2<br>$$</p>
<h3 id="3、MAE平均绝对误差"><a href="#3、MAE平均绝对误差" class="headerlink" title="3、MAE平均绝对误差"></a>3、MAE平均绝对误差</h3><p>$$<br>MAE(X,h)&#x3D;\frac{1}{m}\displaystyle\sum^{m}_{i&#x3D;1}(|h(x_i|)-y_i|<br>$$</p>
<h3 id="4、R-squared"><a href="#4、R-squared" class="headerlink" title="4、R-squared"></a>4、R-squared</h3><p>R Squared又叫可决系数(coefficient of determination)也叫拟合优度,反映的是自变量x对因变量y的变动的解释的程度.越接近于1,说明模型拟合得越好。可以这么理解：将TSS理解为全部按平均值预测，RSS理解为按模型预测，这就相当于去比较你模型预测和全部按平均值预测的比例，这个比例越小，则模型越精确。当然该指标存在负数的情况，即模型预测还不如全部按平均值预测<br>缺点：当数据分布方差比较大时，预测不准时，R^2依然比较大，此时改评价指标就不太好</p>
<p>$$<br>R^2&#x3D;(y,\tilde{y})&#x3D;1-\frac{\displaystyle\sum_{i&#x3D;0}^{n}({y_i-\tilde{y_i}})^2}{\displaystyle\sum_{i&#x3D;0}^{n}({y_i-\tilde{y_i}})^2}&#x3D;\frac{ESS}{TSS}&#x3D;1-\frac{RSS}{TSS}<br>$$</p>
<p><strong>参考</strong></p>
<blockquote>
<p><a href="https://blog.csdn.net/manduner/article/details/91040867">https://blog.csdn.net/manduner/article/details/91040867</a></p>
<p><a href="https://www.jianshu.com/p/2ca96fce7e81">https://www.jianshu.com/p/2ca96fce7e81</a></p>
<p><a href="https://www.bilibili.com/video/BV1wz4y197LU/?spm_id_from=333.337.search-card.all.click&vd_source=881c4826193cfb648b5cdd0bad9f19f0">【小萌五分钟】机器学习 | 模型评估: ROC曲线与AUC值_哔哩哔哩_bilibili</a><br><a href="https://blog.csdn.net/weixin_44441131/article/details/109037673">https://blog.csdn.net/weixin_44441131/article/details/109037673</a></p>
<p><a href="https://www.jianshu.com/p/e74eb43960a1">https://www.jianshu.com/p/e74eb43960a1</a></p>
</blockquote>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>评价标准</tag>
      </tags>
  </entry>
  <entry>
    <title>深度学习优化--正则化和优化算法</title>
    <url>/posts/b8446bf4/</url>
    <content><![CDATA[<p>本文关于深度学习模型优化主要围绕下面两个方面展开：</p>
<p>1、优化问题：深度神经网络的优化十分困难。集中在下面三个原因：1、神经网络的损失函数是一个非凸函数，找到全局最优解通常比较困难。2、深度神经网络的参数多，训练数据大。3、深度神经网络存在梯度消失或爆炸问题，导致基于梯度的优化方法经常失效。 </p>
<p>2、泛化问题：由于深度神经网络的复杂度比较高，并且拟合能力很强，很容易在训练集上产生过拟合。因此在训练深度神经网络时，同时也需要通过一定 的正则化方法来改进网络的泛化能力。</p>
<p>所有代码都是基于pytorch</p>
<span id="more"></span>

<p>[TOC]</p>
<h1 id="一、优化算法"><a href="#一、优化算法" class="headerlink" title="一、优化算法"></a>一、优化算法</h1><h2 id="1-1-问题提出"><a href="#1-1-问题提出" class="headerlink" title="1.1 问题提出"></a>1.1 问题提出</h2><p>我们常使用的<strong>随机梯度下降算法（stochastic gradient descent，SGD）</strong>原理是什么呢？直观上理解就是：山坡滚球，球以最快速度掉到山脚。但是有没有这种可能：球掉了一个“坑道（局部最低）”。在数学上对“坑道”的定义为<strong>鞍点</strong>即在此点梯度为0。回顾梯度下降公式：<br>$$<br>\widehat \theta&#x3D;\theta-\alpha \frac{\partial J(\theta)}{\partial \theta}<br>$$<br>其中$J(\theta)$为损失函数；$\alpha$为学习率；$\theta$为优化参数。鞍点处梯度为0那么也就意味着我们需要调整的参数$\theta $将会一直被困在“原地”无法进行更新。鞍点示意图如下：</p>
<p>从上图可以发现，我们通过随机梯度下降只是到达函数的<strong>局部最优</strong></p>
<h2 id="2、优化算法提出"><a href="#2、优化算法提出" class="headerlink" title="2、优化算法提出"></a>2、优化算法提出</h2><h3 id="2-1-梯度下降算法"><a href="#2-1-梯度下降算法" class="headerlink" title="2.1 梯度下降算法"></a>2.1 梯度下降算法</h3><p>在我们平时编程过程中经常所使用的是随机梯度下降算法，但是一般的梯度下降算法分为三类：1、批量梯度下降算法；2、随机梯度下降算法；3、小批量梯度下降算法。</p>
<p><strong>1、随机梯度下降算法</strong></p>
<p>首先SGD算法运算流程如下：</p>
<img data-src="https://s2.loli.net/2023/05/29/oa2PnzsWRwXNif7.png" alt="image-20230529201451914.png" style="zoom:67%;">

<p><strong>2、小批量梯度下降算法</strong></p>
<p>在训练深度神经网络时，面对较大规模的数据集如果直接对整个训练集的梯度进行计算，显然这是不合适的对资源的消耗太大、计算也十分冗余。因此，在训练深度神经网络时，经常使用<strong>小批量梯度下降法（Mini-Batch Gradient Descent）</strong></p>
<img data-src="https://s2.loli.net/2023/05/29/m8y7ueMvD4BdZR5.png" alt="image-20230529195427115.png" style="zoom:67%;">

<p>通过对梯度下降算法观察发现存在两个参数可以进行优化：1、学习率$\alpha $</p>
<h2 id="2-2-学习率调整"><a href="#2-2-学习率调整" class="headerlink" title="2.2 学习率调整"></a>2.2 学习率调整</h2><h3 id="2-2-1-AdaGrad算法"><a href="#2-2-1-AdaGrad算法" class="headerlink" title="2.2.1 AdaGrad算法"></a>2.2.1 AdaGrad算法</h3><h3 id="2-2-2-RMSprop算法"><a href="#2-2-2-RMSprop算法" class="headerlink" title="2.2.2 RMSprop算法"></a>2.2.2 RMSprop算法</h3><h3 id="2-2-3-Adam算法"><a href="#2-2-3-Adam算法" class="headerlink" title="2.2.3 Adam算法"></a>2.2.3 Adam算法</h3><h1 id="n、参考文献"><a href="#n、参考文献" class="headerlink" title="n、参考文献"></a>n、参考文献</h1><p>1、<a href="https://www.deeplearningbook.org/">https://www.deeplearningbook.org/</a></p>
]]></content>
      <categories>
        <category>文献笔记</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>图像识别</tag>
      </tags>
  </entry>
  <entry>
    <title>贝叶斯优化</title>
    <url>/posts/d129933e/</url>
    <content><![CDATA[<p>本文通过结合如下论文以及blog：</p>
<blockquote>
<p>1、贝叶斯优化研究综述：<a href="https://doi.org/10.13328/j.cnki.jos.005607">https://doi.org/10.13328/j.cnki.jos.005607</a>.<br>2、高斯回归可视化：<a href="https://jgoertler.com/visual-exploration-gaussian-processes/">https://jgoertler.com/visual-exploration-gaussian-processes/</a><br>3、贝叶斯优化：<a href="http://arxiv.org/abs/1012.2599">http://arxiv.org/abs/1012.2599</a></p>
</blockquote>
<p>对贝叶斯优化进行较为全面的介绍，以及部分代码复现</p>
<span id="more"></span>

<h1 id="贝叶斯优化"><a href="#贝叶斯优化" class="headerlink" title="贝叶斯优化"></a>贝叶斯优化</h1><p>[toc]</p>
<h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>问题一：如果存在函数$y&#x3D;x^2$那么对于这个函数很容易就可以得到他的最小值$x&#x3D;0$时取到最小值，但是如果只告诉我们存在函数$y&#x3D;f(x)$（$f(x)$具体的表达式未知），我们如何找到他的最小值呢？</p>
<p>问题二：对于机器学习、深度学习模型都是由许多参数所决定的（比如说：深度学习中学习率、网络深度等），假如我们通过计算模型的$R^2$来选择我们的参数，那么如何选择参数的值使得$R^2$最大呢？</p>
<p><strong>Grid Search？Random Search？Bayesian optimization？</strong></p>
<blockquote>
<p>超参数优化</p>
<p>百度百科：</p>
<p><a href="https://baike.baidu.com/item/%E8%B6%85%E5%8F%82%E6%95%B0/3101858">https://baike.baidu.com/item/%E8%B6%85%E5%8F%82%E6%95%B0/3101858</a></p>
<p>Wiki：</p>
<p><a href="https://en.wikipedia.org/wiki/Hyperparameter_optimization">https://en.wikipedia.org/wiki/Hyperparameter_optimization</a></p>
</blockquote>
<p>本文主要对<strong>Bayesian optimization</strong>进行解释。<strong>贝叶斯优化</strong>通过有限的步骤进行全局优化。定义我们的待优化函数：</p>
<p>$$<br>x^{*}&#x3D;\underset{x\in X}{argmin}f(x)<br>$$</p>
<blockquote>
<p>上式子中：$x$代表<strong>决策向量</strong>（直观理解为：深度学习中的学习率、网络深度等），$X$代表<strong>决策空间</strong>（直观理解为：以学习率为例，假设我们能从学习率集合$\alpha&#x3D;(0.01,0.02,0.03)$[&lt;–这就是决策空间] 选择最佳学习率[&lt;–这就是我们决策向量]，$f$则代表目标函数（比如上面提到的$R^2$或者机器学习模型$f$））</p>
</blockquote>
<p>许多机器学习中的优化问题都是<strong>黑盒优化</strong>问题，我们函数是一个黑盒函数<a href="http://krasserm.github.io/2018/03/21/bayesian-optimization/">^1</a>。如何通过<strong>贝叶斯优化</strong>实现**(1)<strong>式子呢？贝叶斯优化的两板斧：（1）surrogate model（</strong>代理模型<strong>）；（2）acquisition function（</strong>采集函数**）。贝叶斯优化框架如下<a href="https://doi.org/10.13328/j.cnki.jos.005607.">^3</a>：</p>
<img data-src="https://s2.loli.net/2023/06/10/cFwQxP2Doyfldtn.png" alt="202306101452451" style="zoom:80%;">

<p>贝叶斯优化框架应用在一维函数$f(x)&#x3D;(x-0.3)^2+0.2sin(20x)$上3次迭代的示例：</p>
<img data-src="https://s2.loli.net/2023/06/10/RE2hpHuvJ5wWGnB.png" alt="图一:贝叶斯优化示例" style="zoom:70%;">

<h2 id="代理模型-surrogate-models"><a href="#代理模型-surrogate-models" class="headerlink" title="代理模型(surrogate models)"></a>代理模型(surrogate models)</h2><p>上面提及到机器学习是一个黑盒子(black box)，即我们只知道input和output，所以很难确直接定存在什么样的函数关系<a href="https://zhuanlan.zhihu.com/p/53826787">^2</a>。既然你的<strong>函数关系</strong>确定不了，那么我们就可以直接找到一个模型对你的函数进行<strong>替代</strong>（代理），这就是贝叶斯优化第一板斧：<strong>代理模型</strong>。（使用概率模型代理原始评估代价高昂的复杂目标函数）</p>
<p>这里主要解释<strong>高斯过程（Gaussian processes，GP）</strong><a href="https://jgoertler.com/visual-exploration-gaussian-processes/">^4</a></p>
<blockquote>
<p>其他代理模型，感兴趣的可以阅读这篇<a href="https://doi.org/10.13328/j.cnki.jos.005607">论文</a></p>
<p>WiKi：</p>
<p><a href="https://zh.wikipedia.org/wiki/%E9%AB%98%E6%96%AF%E8%BF%87%E7%A8%8B">高斯过程 - 维基百科，自由的百科全书 (wikipedia.org)</a></p>
<p>百度百科：</p>
<p><a href="https://baike.baidu.com/item/%E9%AB%98%E6%96%AF%E8%BF%87%E7%A8%8B/4535435?structureClickId=4535435&structureId=56eafc20675c3e28256b410f&structureItemId=a46e4d355312e203fffb9c11">高斯过程_百度百科 (baidu.com)</a></p>
<p>高斯过程：<strong>就是一系列关于连续域（时间或空间）的随机变量的联合，而且针对每一个时间或是空间点上的随机变量都是服从高斯分布的</strong></p>
</blockquote>
<hr>
<blockquote>
<p>解释高斯过程前了解<strong>高斯分布</strong>学过概率论的应该都了解，高斯分布其实就是<strong>正态分布</strong>平时所学的大多为一元正态分布，推广到$n$维的高斯分布：</p>
<p>$$<br>X&#x3D;\begin{bmatrix}X_1\X_2\…\X_n\end{bmatrix}∼N(\mu,\sum)<br>$$</p>
<p>其中$\mu$代表均值，$\sum$代表协方差。</p>
</blockquote>
<hr>
<p>高斯过程的数学原理<a href="http://arxiv.org/abs/1012.2599">^5</a>：</p>
<p>$$<br>f(x)∼GP(m(x),k(x,x^{‘}))<br>$$</p>
<p>其中$m(x)$代表<strong>均值</strong>（为了方便令$m(x)&#x3D;0$），$k$代表<strong>核函数</strong>。常用核函数：</p>
<p>$$<br>k(x_i,x_j)&#x3D;\sum&#x3D;cov(x_i,x_j)&#x3D;exp(-\frac{1}{2}||x_i-x_j||^2)<br>$$</p>
<blockquote>
<p>其他核函数</p>
<img data-src="https://pic4.zhimg.com/80/v2-85fb84d30a68bc03e301ed67d868c38b_720w.webp" alt="202306131551131" style="zoom:90%;">
</blockquote>
<p>在高斯过程中，核函数往往就决定了分布的形状，于此同时也就决定我们需要预测函数所具有的特性，对于不同两点$x_i$和$x_j$两点距离近则值接近1反之则接近0。那么可以得到核矩阵为：</p>
<p>$$<br>K&#x3D;\begin{bmatrix}k(x_1,x_1)&amp;…&amp;k(x_1,x_t)\…&amp;…&amp;…\k(x_t,x_1)&amp;…&amp;k(x_1,x_t) \end{bmatrix}<br>$$</p>
<p>以<strong>回归任务</strong>为例<a href="https://jgoertler.com/visual-exploration-gaussian-processes/">^4</a>，高斯过程定义了潜在函数的<strong>概率分布</strong>，由于这是一个多元高斯分布，这些函数也呈正态分布。通常假设$μ&#x3D; 0$，在没有观察到任何训练数据的情况下。在贝叶斯推断的框架下，将其称之为先验分布 $f(x)∼N(\mu_f,K_f)$。在没观察到任何训练样本，该分布会围绕 $μ_f&#x3D;0$展开（可定义此时先验分布为$f(x)∼N(0,K_f)$）。先验分布的维数和测试点的数目 $N&#x3D;∣X∣$一致。我们将用核函数来建立协方差矩阵，维数为$N×N$。</p>
<img data-src="https://s2.loli.net/2023/06/13/jN7wmdgoxSEu2zb.png" alt="202306131635876" style="zoom:70%;">

<blockquote>
<p>以RBF为核函数生成5组样本</p>
</blockquote>
<p>当补充训练样本时得到：</p>
<img data-src="https://s2.loli.net/2023/06/13/EwdIPmZXJ4Uu3QF.png" alt="202306131650854" style="zoom:70%;">

<p>输入样本点，数学原理如下：假设观察到样本点为$(x,y)$那么$y$与先验分布$f(x)$的联合高斯分布为：</p>
<p>$$<br>\begin{bmatrix}f(x)\y \end{bmatrix}∼N(\begin{bmatrix}0\0 \end{bmatrix},\begin{bmatrix}K_{ff}&amp;&amp;K_{fy}\K^{T}<em>{fy}&amp;&amp;K</em>{yy} \end{bmatrix})<br>$$</p>
<p>那么此时可以根据联合分布得到$P(y|f)$的分布为：</p>
<p>$$<br>P(y|f)&#x3D;N(\mu(x),\sigma^2(x))<br>$$</p>
<p>其中：$\mu(x)&#x3D;K^{T}<em>{fy}K</em>{ff}^{-1}f(x)$，$\sigma^2(x)&#x3D;K_{yy}-K^{T}<em>{fy}K</em>{ff}^{-1}K_{fy}$</p>
<p>从<strong>回归</strong>的角度对<strong>高斯过程</strong>进行理解：假设我们需要拟合函数为：</p>
<p>$$<br>y&#x3D;sin(2.5x)+sin(x)+0.05x^2+1<br>$$</p>
<p>我们通过设置$x$范围生成输入数据，那么可以得到输出数据$y$那么GP拟合如下：</p>
<img data-src="https://s2.loli.net/2023/06/13/SeJKV91xCI2g4Qj.png" alt="202306131551131" style="zoom:90%;">

<p>上图也很容易理解，在$x&lt;10$以前我们输入了数据那么置信区间范围较小，而$x&gt;10$之后由于没有输入数据置信区间范围较大</p>
<blockquote>
<img data-src="https://s2.loli.net/2023/06/10/k8b4stXOKJfA2Ya.png" alt="图一" style="zoom:60%;">

<p>具有三个观测值的简单一维高斯过程。实线黑线是给定数据的目标函数的GP代理均值预测，阴影区域表示均值加减方差。</p>
<p>Simple 1D Gaussian process with three observations. The solid black line is the GP surrogate mean prediction of the objective function given the data, and the shaded area shows the mean plus and minus the variance. The superimposed Gaussians correspond to the GP mean and standard deviation ($μ(·) $and $σ(·)) $of prediction at the points, $x_{1:3}$.</p>
</blockquote>
<h2 id="采集函数（Acquisition-Functions）"><a href="#采集函数（Acquisition-Functions）" class="headerlink" title="采集函数（Acquisition Functions）"></a>采集函数（Acquisition Functions）</h2><p>在<a href="http://arxiv.org/abs/1012.2599">论文</a>中作者对于<strong>采集函数</strong>的描述为：</p>
<p>The role of the acquisition function is to guide the search for the optimum.</p>
<blockquote>
<p>个人理解为：上一节介绍了GP过程中引入新的数据点其联合分布，那么的话我们可以直接引入$n$个点直接将全部$x$进行覆盖，但是这样的话Bayesian optimization就失去其意义了，如何通过最少的点去实现$x^{*}&#x3D;\underset{x\in X}{argmin}f(x)$</p>
</blockquote>
<blockquote>
<p>Acquisition functions are defined such that high acquisition corresponds to potentially high values of the objective function.</p>
<p>采集函数被定义为目标函数的潜在高值</p>
</blockquote>
<p>可以对采集函数理解为：<strong>去找到一个合适的点</strong>。常用的采集函数：</p>
<h3 id="1、probability-of-improvement（PI）"><a href="#1、probability-of-improvement（PI）" class="headerlink" title="1、probability of improvement（PI）"></a>1、probability of improvement（PI）</h3><p>PI去尝试最大化现有的概率$f(x^+)$，其中$x^+&#x3D;\underset{x\in X}{argmax}f(x)$其公式为：</p>
<p>$$<br>PI_{t}(x)&#x3D;P(f_{t}(x) \geq f_{t}(x^+)+\xi)&#x3D;\phi(\frac{\mu_{t}(x)-f_{t}(x^+)-\xi}{\sigma_{t}(x)})<br>$$</p>
<p>其中$\phi(.)$为正则化，$\xi（\xi \geq 0）$为权重。PI策略通过PI提升最大化来选择新一轮的超参组合：</p>
<p>$$<br>x_{t+1}&#x3D;argmax_{x}(PI_{t}(x))<br>$$</p>
<p>其中$x_{t+1}$代表新一轮超参组合。</p>
<h3 id="2、expected-improvement（EI）"><a href="#2、expected-improvement（EI）" class="headerlink" title="2、expected improvement（EI）"></a>2、expected improvement（EI）</h3><p>PI策略选择提升<strong>概率最大</strong>的候选点，这一策略值考虑了提升的概率而没有考虑<strong>提升量</strong>的大小，EI针对此提出：$EI(x)&#x3D;E[max(f_{t+1}(x)-f(x^+),0)]$那么EI函数为：</p>
<p>$$<br>f(n)&#x3D; \begin{cases}<br>(\mu(x)-f(x^+)\phi(Z)+\sigma(x)\phi(Z) &amp; \text {if $\phi(x)&gt;0$} \<br>0 &amp; \text{if $\phi(x)&#x3D;0$ }<br>\end{cases}<br>$$</p>
<p>其中$Z&#x3D;\frac{\mu(x)-f(x^+)}{\sigma(x)}$</p>
<blockquote>
<p>具体公式推导见论文（第13页）：<a href="http://arxiv.org/abs/1012.2599">http://arxiv.org/abs/1012.2599</a></p>
</blockquote>
<h3 id="3、Confidence-bound-criteria（置信边界策略）"><a href="#3、Confidence-bound-criteria（置信边界策略）" class="headerlink" title="3、Confidence bound criteria（置信边界策略）"></a>3、Confidence bound criteria（置信边界策略）</h3><p><strong>1. LCB</strong>(置信下界策略，计算目标函数最小值)</p>
<p>$$<br>LCB(x)&#x3D;\mu(x)-\kappa \phi(x)<br>$$</p>
<p><strong>2. UCB</strong>（置信上界策略，计算目标函数最大值）</p>
<p>$$<br>UCB(x)&#x3D;\mu(x)+\kappa \phi(x)<br>$$</p>
<blockquote>
<p>LCB、UCB中的$\kappa$ is left to the user</p>
</blockquote>
<p><strong>3. GP-UCB</strong></p>
<p>$$<br>GP-UCB&#x3D;\mu(x)+\sqrt{v\tau_{t}}\phi(x)<br>$$</p>
<p>GP-UCB很简单的一种采集策略，以随机变量的置信上界最大化为原则选择下一轮的超参组合</p>
<p><strong>4.其它</strong></p>
<p>见论文：<a href="https://doi.org/10.13328/j.cnki.jos.005607">https://doi.org/10.13328/j.cnki.jos.005607</a></p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><h3 id="1、常用代理函数"><a href="#1、常用代理函数" class="headerlink" title="1、常用代理函数"></a>1、常用代理函数</h3><img data-src="https://s2.loli.net/2023/06/13/qRv9E61jIO2hYsn.png" alt="202306132058850" style="zoom:100%;">

<h3 id="2、常用采集函数"><a href="#2、常用采集函数" class="headerlink" title="2、常用采集函数"></a>2、常用采集函数</h3><img data-src="https://s2.loli.net/2023/06/13/tfUN829GmbqKka3.png" alt="202306132059038" style="zoom:100%;">

<h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><p>代码参考：<a href="https://github.com/bayesian-optimization/BayesianOptimization">https://github.com/bayesian-optimization/BayesianOptimization</a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> bayes_opt <span class="keyword">import</span> BayesianOptimization <span class="comment">#调用第三方库</span></span><br><span class="line"><span class="keyword">from</span> skelarn.svm <span class="keyword">import</span> SVR</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> r2_score</span><br><span class="line"></span><br><span class="line"><span class="comment">#交叉验证贝叶斯优化</span></span><br><span class="line">X_train, X_test, Y_train, Y_test= train_test_split(x,y,test_size=<span class="number">0.2</span>,random_state=<span class="number">100</span>)</span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVR</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">svr_cv</span>(<span class="params">C, epsilon, gamma</span>):</span><br><span class="line">    kf = KFold(n_splits=<span class="number">5</span>, shuffle=<span class="literal">True</span>, random_state=<span class="number">100</span>)</span><br><span class="line">    svr = SVR(C=C, epsilon=epsilon, gamma=gamma)</span><br><span class="line">    <span class="keyword">for</span> i, (train_index, test_index) <span class="keyword">in</span> <span class="built_in">enumerate</span>(kf.split(X_train, Y_train.values)):</span><br><span class="line">        svr.fit(X_train[train_index], Y_train.values[train_index])</span><br><span class="line">        pred = svr.predict(X_train[test_index])</span><br><span class="line">        <span class="keyword">return</span> r2_score(pred, Y_train.values[test_index])</span><br><span class="line"></span><br><span class="line">svr_bo = BayesianOptimization(svr_cv,&#123;<span class="string">&#x27;C&#x27;</span>:(<span class="number">1</span>,<span class="number">16</span>), <span class="string">&#x27;epsilon&#x27;</span>:(<span class="number">0</span>,<span class="number">1</span>), <span class="string">&#x27;gamma&#x27;</span>:(<span class="number">0</span>,<span class="number">1</span>)&#125;)</span><br><span class="line"><span class="comment">#输入测试的函数，以及变量的范围</span></span><br><span class="line">svr_bo.maximize()</span><br></pre></td></tr></table></figure>

<img data-src="https://s2.loli.net/2023/06/10/YTiR82Xsz4SHKxh.png" alt="202306101944250" style="zoom:60%;">

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">svr_bo.<span class="built_in">max</span> <span class="comment">#得到最佳参数</span></span><br><span class="line"><span class="comment">#&#123;&#x27;target&#x27;: 0.9875895309185105,</span></span><br><span class="line"><span class="comment"># &#x27;params&#x27;: &#123;&#x27;C&#x27;: 14.595794386042416,</span></span><br><span class="line"><span class="comment">#  &#x27;epsilon&#x27;: 0.09480102745231553,</span></span><br><span class="line"><span class="comment">#  &#x27;gamma&#x27;: 0.09251046201638335&#125;&#125;</span></span><br></pre></td></tr></table></figure>

<p>通过最佳参数进行测试：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">svr1 =  SVR(C=<span class="number">14.595794386042416</span>, epsilon=<span class="number">0.09480102745231553</span>, gamma=<span class="number">0.09251046201638335</span>)</span><br><span class="line">svr1.fit(X_train, Y_train)</span><br><span class="line">r2_score(Y_test.values, svr1.predict(X_test))</span><br><span class="line"><span class="comment">#0.9945825852230629</span></span><br></pre></td></tr></table></figure>

<p>高斯拟合代码：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">n = <span class="number">100</span></span><br><span class="line">x_min = -<span class="number">10</span></span><br><span class="line">x_max = <span class="number">10</span></span><br><span class="line">X = np.sort(np.random.uniform(size=n))*(x_max- x_min) + x_min</span><br><span class="line">X = X.reshape(-<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">eta = np.random.normal(loc=<span class="number">0.0</span>, scale= <span class="number">0.5</span>, size= n)</span><br><span class="line"></span><br><span class="line">y_clean = np.sin(X * <span class="number">2.5</span>) + np.sin(X * <span class="number">1.0</span>)  + np.multiply(X, X) * <span class="number">0.05</span> + <span class="number">1</span></span><br><span class="line">y_clean = y_clean.ravel()</span><br><span class="line">y = y_clean+ eta</span><br><span class="line"><span class="keyword">from</span> sklearn.gaussian_process <span class="keyword">import</span> GaussianProcessRegressor</span><br><span class="line"><span class="keyword">from</span> sklearn.gaussian_process.kernels <span class="keyword">import</span> RBF</span><br><span class="line"> </span><br><span class="line">kernel = RBF(</span><br><span class="line">    length_scale=<span class="number">1</span>, </span><br><span class="line">    length_scale_bounds=(<span class="number">1e-2</span>, <span class="number">1e3</span>))</span><br><span class="line"> </span><br><span class="line">gpr = GaussianProcessRegressor( kernel,</span><br><span class="line">                               alpha=<span class="number">0.1</span>,</span><br><span class="line">                               n_restarts_optimizer=<span class="number">5</span>,</span><br><span class="line">                               normalize_y=<span class="literal">True</span>)</span><br><span class="line">gpr.fit(X,y )</span><br><span class="line"><span class="comment">#print(&quot;LML:&quot;, gpr.log_marginal_likelihood())</span></span><br><span class="line"><span class="comment">#print(gpr.get_params())</span></span><br><span class="line">x = np.linspace(x_min - <span class="number">2.0</span>, x_max + <span class="number">7.5</span>, n * <span class="number">2</span>).reshape(-<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">y_pred, y_pred_std = gpr.predict(x, return_std=<span class="literal">True</span>)</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">plt.figure(figsize=(<span class="number">15</span>, <span class="number">8</span>))</span><br><span class="line">plt.plot(x, y_pred,linewidth = <span class="number">3</span>, label=<span class="string">&quot;GP mean&quot;</span>)</span><br><span class="line">plt.plot(X, y_clean, linewidth = <span class="number">3</span>,  label=<span class="string">&quot;Original y&quot;</span>)</span><br><span class="line">plt.plot(X, y,linewidth = <span class="number">3</span>, label=<span class="string">&quot;Noisy y&quot;</span>)</span><br><span class="line">plt.scatter(X, np.zeros_like(X), marker=<span class="string">&#x27;x&#x27;</span>)</span><br><span class="line">plt.fill_between(x.ravel(),</span><br><span class="line">                 y_pred - y_pred_std,</span><br><span class="line">                 y_pred + y_pred_std,</span><br><span class="line">                label=<span class="string">&quot;95% confidence interval&quot;</span>,</span><br><span class="line">                interpolate=<span class="literal">True</span>,</span><br><span class="line">                facecolor=<span class="string">&#x27;blue&#x27;</span>,</span><br><span class="line">                alpha=<span class="number">0.5</span>)</span><br><span class="line">plt.xlim(<span class="number">5</span>, <span class="number">15</span>)</span><br><span class="line">plt.legend()</span><br></pre></td></tr></table></figure>

<h2 id="推荐"><a href="#推荐" class="headerlink" title="推荐"></a>推荐</h2><p>1、Gaussian Processes for Machine Learning：<a href="https://gaussianprocess.org/gpml/chapters/RW.pdf">https://gaussianprocess.org/gpml/chapters/RW.pdf</a></p>
<p>2、贝叶斯优化论文：<a href="http://arxiv.org/abs/1012.2599">http://arxiv.org/abs/1012.2599</a></p>
<p>3、贝叶斯优化博客：<a href="https://banxian-w.com/article/2023/3/27/2539.html">https://banxian-w.com/article/2023/3/27/2539.html</a></p>
<p>4、可视化高斯过程：<a href="https://jgoertler.com/visual-exploration-gaussian-processes/#MargCond">https://jgoertler.com/visual-exploration-gaussian-processes/#MargCond</a></p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p>1、<a href="http://krasserm.github.io/2018/03/21/bayesian-optimization/">http://krasserm.github.io/2018/03/21/bayesian-optimization/</a></p>
<p>2、<a href="https://zhuanlan.zhihu.com/p/53826787">https://zhuanlan.zhihu.com/p/53826787</a></p>
<p>3、崔佳旭, 杨博. 贝叶斯优化方法和应用综述[J&#x2F;OL]. 软件学报, 2018, 29(10): 3068-3090. <a href="https://doi.org/10.13328/j.cnki.jos.005607">https://doi.org/10.13328/j.cnki.jos.005607</a>.</p>
<p>4、<a href="https://jgoertler.com/visual-exploration-gaussian-processes/">https://jgoertler.com/visual-exploration-gaussian-processes/</a></p>
<p>5、<a href="http://arxiv.org/abs/1012.2599">http://arxiv.org/abs/1012.2599</a></p>
<p>6、<a href="https://www.cvmart.net/community/detail/3502">https://www.cvmart.net/community/detail/3502</a></p>
<p>7、<a href="https://gaussianprocess.org/gpml/chapters/RW.pdf">https://gaussianprocess.org/gpml/chapters/RW.pdf</a></p>
]]></content>
      <categories>
        <category>优化算法</category>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>优化算法</tag>
      </tags>
  </entry>
  <entry>
    <title>贝叶斯分类器</title>
    <url>/posts/2045861030/</url>
    <content><![CDATA[<h1 id="朴素贝叶斯分类器"><a href="#朴素贝叶斯分类器" class="headerlink" title="朴素贝叶斯分类器"></a>朴素贝叶斯分类器</h1><span id="more"></span>

<h2 id="数学理论"><a href="#数学理论" class="headerlink" title="数学理论"></a>数学理论</h2><ul>
<li><strong>先验概率</strong></li>
</ul>
<p>根据以往经验和分析得到的概率</p>
<ul>
<li><strong>条件概率（后验概率）</strong></li>
</ul>
<p>在事件B发生的条件下A在发生的概率</p>
<p>$$<br>P(A|B)&#x3D;\frac{P(AB)}{p(A)}<br>$$</p>
<ul>
<li><strong>朴素贝叶斯定理</strong><br>直观理解：我们假设B是我们的特征标签A是我们的分类标签。那么公式直观上的理解就是：我们在具有B这么多的特征之后一个样本属于A的概率有多大</li>
</ul>
<p>$$<br>P(A|B)&#x3D;\frac{P(B_1|A)P(B_2|A)P(B_3|A)…P(B_n|A)P(A)}{P(B)}\\text{公式中}P(B_i|A)\text{代表在训练集中}B_i特征下属于A的概率<br>$$</p>
<blockquote>
<p>此时问题来了：如果我们的特征是非数字数据比如说：绿色、蓝色等那么我们很容易就可以计算得到概率的计算，但是如果是具体数字呢？那么应该怎么计算呢？</p>
</blockquote>
<ul>
<li><strong>高斯朴素贝叶斯</strong></li>
</ul>
<blockquote>
<p>高斯分布：正态分布</p>
</blockquote>
<p>$$<br>P(A|B)&#x3D;\frac{1}{\sqrt{2\pi\sigma_{B}^{2}}}e^{-\frac{(A-\mu)^2}{2\sigma_{B}^{2}}}\\mu:均值 \sigma:方差<br>$$</p>
<blockquote>
<p><a href="https://zhuanlan.zhihu.com/p/38361524">正态分布判别</a></p>
</blockquote>
<h2 id="朴素贝叶斯"><a href="#朴素贝叶斯" class="headerlink" title="朴素贝叶斯"></a>朴素贝叶斯</h2><p>我们先看关于他的解释：朴素贝叶斯是一种建分类器的简单方法。该分类器模型会给问题实例分配用特征值表示的类标签，类标签取自有限集合。它不是训练这种分类器的单一算法，而是一系列基于相同原理的算法：所有朴素贝叶斯分类器都假定样本每个特征与其他特征都不相关。</p>
<blockquote>
<p><a href="https://zh.wikipedia.org/wiki/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8">https://zh.wikipedia.org/wiki/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8</a></p>
</blockquote>
<p>从定义上看起来觉得很麻烦，其实朴素贝叶斯算法的原理十分简单。我们以如下例子为例：</p>
<blockquote>
<p>假设训练集如下</p>
<table>
<thead>
<tr>
<th>性别</th>
<th>身高（英尺）</th>
<th>体重（磅）</th>
<th>脚的尺寸（英寸）</th>
</tr>
</thead>
<tbody><tr>
<td>男</td>
<td>6</td>
<td>180</td>
<td>12</td>
</tr>
<tr>
<td>男</td>
<td>5.92 (5’11”)</td>
<td>190</td>
<td>11</td>
</tr>
<tr>
<td>男</td>
<td>5.58 (5’7”)</td>
<td>170</td>
<td>12</td>
</tr>
<tr>
<td>男</td>
<td>5.92 (5’11”)</td>
<td>165</td>
<td>10</td>
</tr>
<tr>
<td>女</td>
<td>5</td>
<td>100</td>
<td>6</td>
</tr>
<tr>
<td>女</td>
<td>5.5 (5’6”)</td>
<td>150</td>
<td>8</td>
</tr>
<tr>
<td>女</td>
<td>5.42 (5’5”)</td>
<td>130</td>
<td>7</td>
</tr>
<tr>
<td>女</td>
<td>5.75 (5’9”)</td>
<td>150</td>
<td>9</td>
</tr>
</tbody></table>
<p>我们对训练集计算得到：</p>
<table>
<thead>
<tr>
<th>性别</th>
<th>均值（身高）</th>
<th>方差（体重）</th>
<th>均值（体重）</th>
<th>方差（体重）</th>
<th>均值（脚的尺寸）</th>
<th>方差（脚的尺寸）</th>
</tr>
</thead>
<tbody><tr>
<td>男</td>
<td>5.855</td>
<td>3.5033e-02</td>
<td>176.25</td>
<td>1.2292e+02</td>
<td>11.25</td>
<td>9.1667e-01</td>
</tr>
<tr>
<td>女</td>
<td>5.4175</td>
<td>9.7225e-02</td>
<td>132.5</td>
<td>5.5833e+02</td>
<td>7.5</td>
<td>1.6667e+00</td>
</tr>
</tbody></table>
<p>那么在给定如下样本进行判别：</p>
<ul>
<li><p>身高：6 体重：130 脚的尺寸：8<br>如何计算呢？很简单！！！比如说我们计算$P(身高|男性)$我们只需要将身高6代入到我们的<strong>高斯贝叶斯公式</strong>里面就可以得到我们的概率。我们依次计算体重、脚的尺寸就可以得到一系列的概率，而后我们代入公式：</p>
<p>$$<br>P(男性)&#x3D;\frac{P(男性)P(身高|男性)….}{P(A)}\P(A)&#x3D;P(男)*P(身高|男性)….+P(女性)*P(身高|女性)….\P(男)&#x3D;0.5&#x3D;P(女)<br>$$</p>
</li>
</ul>
<p>而后判别男和女的概率大小进而判别是男性还是女性！</p>
</blockquote>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>贝叶斯分类器</tag>
      </tags>
  </entry>
  <entry>
    <title>逻辑斯蒂回归</title>
    <url>/posts/317190628/</url>
    <content><![CDATA[<p>介绍逻辑斯蒂回归基本原理</p>
<span id="more"></span>
<h1 id="Logistic回归-逻辑斯蒂回归"><a href="#Logistic回归-逻辑斯蒂回归" class="headerlink" title="Logistic回归(逻辑斯蒂回归)"></a>Logistic回归(逻辑斯蒂回归)</h1><p>[toc]</p>
<blockquote>
<p>首先明确一点Logistic回归虽然叫回归，但是<strong>一般用于二分类问题</strong>，也就是说还是分类算法的一种！</p>
</blockquote>
<h2 id="数学理论准备"><a href="#数学理论准备" class="headerlink" title="数学理论准备"></a>数学理论准备</h2><ul>
<li><strong>线性回归</strong><br>是利用称为线性回归方程的最小二乘函数对一个或多个自变量和因变量之间关系进行建模的一种回归分析</li>
<li><strong>Sigmoid函数</strong><br>因其函数图像形状像字母S得名。其形状曲线至少有2个焦点，也叫“二焦点曲线函数”。S型函数是有界、可微的实函数，在实数范围内均有取值，且导数恒为非负 ，有且只有一个拐点。</li>
</ul>
<h2 id="逻辑斯蒂回归"><a href="#逻辑斯蒂回归" class="headerlink" title="逻辑斯蒂回归"></a>逻辑斯蒂回归</h2><p>我们先看百度百科关于他的解释：logistic回归是一种广义线性回归（generalized linear model），因此与多重线性回归分析有很多相同之处。<strong>它们的模型形式基本上相同</strong>，都具有 wx+b，其中w和b是待求参数，其区别在于他们的因变量的不同，多重线性回归直接将wx+b作为因变量，即y &#x3D;wx+b，而logistic回归则通过<strong>函数L</strong>将wx+b对应一个隐状态p，p &#x3D;L(wx+b)，然后根据p 与1-p的大小决定因变量的值。如果L是logistic函数，就是logistic回归。</p>
<blockquote>
<p><a href="https://baike.baidu.com/item/logistic%E5%9B%9E%E5%BD%92/2981575">logistic回归_百度百科 (baidu.com)</a></p>
</blockquote>
<p><strong>什么意思</strong>？听我给你解释！首先我们目的是利用这个算法做什么？分类！没错既然是分类算法，那么我必须就有一个进行分类的标准！比如说：KNN(K近邻算法)我们就是直接计算测试集与训练集之间的距离而后进行划分。贝叶斯分类器：就是直接通过计算概率而后进行分类。也就是说：分类算法模型的大致流程如下：</p>
<pre class="mermaid">graph LR
A[数据]--->B{模型}--->c[结果]</pre>

<blockquote>
<p><strong>结果</strong>：通过模型我们得到的结果应该大致在一个区间内部（比如：0-1），这样我们就可以建立一个分类的标准，比如：&gt;0.5的时候分类结果为1，＜0.5的时候分类结果为0</p>
</blockquote>
<p>我们知道如果仅仅只使用$wx+b$我们是得不到上述<strong>结果</strong>，因此我们借助<strong>函数L</strong>，对$wx+b$的结果进行处理。</p>
<h2 id="逻辑斯蒂函数"><a href="#逻辑斯蒂函数" class="headerlink" title="逻辑斯蒂函数"></a>逻辑斯蒂函数</h2><p>通过上述对逻辑斯蒂回归定义的了解我们得知，仅仅只借助$wx+b$是得不到我们需要的结果，我们还需要一个函数对$wx+b$进行处理，此处所用的函数就是<strong>逻辑斯蒂函数：</strong></p>
<p>$$<br>y&#x3D;\frac{1}{1+e^{-z}}\z&#x3D;wx+b,那么将z代入上式得到：\y&#x3D;\frac{1}{1+e^{-(wx+b)}}<br>$$</p>
<p>我们上式变形可以得到：</p>
<p>$$<br>y&#x3D;\frac{1}{1+e^{-(wx+b)}}则：\frac{1}{y}-1&#x3D;e^{-(wx+b)}同时取对数:ln(\frac{y}{1-y})&#x3D;wx+b<br>$$</p>
<p>此时我们只需要求解参数w和b就可以进行分类了</p>
<h2 id="参数求解"><a href="#参数求解" class="headerlink" title="参数求解"></a>参数求解</h2><p>此处介绍两种求解方法：梯度下降算法、牛顿迭代算法</p>
<h3 id="梯度上升算法"><a href="#梯度上升算法" class="headerlink" title="梯度上升算法"></a>梯度上升算法</h3><p>$$<br>\theta&#x3D;\theta-\eta\nabla_θ J(θ)\\eta:学习率，\nabla:求导<br>$$</p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>逻辑斯蒂回归</tag>
      </tags>
  </entry>
</search>
